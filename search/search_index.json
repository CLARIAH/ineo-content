{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Ineo Content At this stage this repository is a proposal! Introduction This repository stores the rich content for the Ineo Resource finder. We denote the various curated texts pertaining to resources as 'rich content'. A select team of CLARIAH's communication department is working on these texts. This repository enables easy collaboration, also with developers and data providers, and provides a sustainable storage solution. Rich content can be contrasted to the automatically harvested metadata that results from the CLARIAH Tool Discovery track or the FAIR Datasets track and is not supposed to overlap. For automatically harvested metadata, the primary authorship lies with the tool/data producer. For rich user content, it lies with the people of the communication department (WP1). The rich content in this repository is stored in Markdown format , a simple plain text format for text markup. We follow a specific structure to the data can be easily ingested into Ineo, as well as any other systems who want to make use of it. The content can be previewed directly on https://clariah.github.io/ineo-content . We used a simple static site generator ( MkDocs ) to achieve this. That site, however, is not intended for end-users to be used directly. On the preview site we also provide access to a simple headless CMS to facilitate working with the Markdown files in the git repository, without requiring real technical knowledge of either. Directory Structure Directories: src/ - Contains Markdown texts tools/ - Contains Markdown texts describing tools. data/ - Contains Markdown texts describing data workflows/ - Contains Markdown texts describing workflows standards/ - Contains Markdown texts describing standards media/ - Stores images or other associated media files, Right now all media files are in this single folder, without subfolders, this limitation is only due to the admin CMS. Images or downloadable documents/spreadsheets/presentations that are referenced from one of the markdown files may be included in this subdirectory as well. You can reference images from the markdown files, just use /media/ as URL prefix when doing so. You can also reference media elsewhere on the web, always sure to use https:// in that case. This should probably be kept to a minimum as there is no guarantee such media will persist in that location over time. Videos are not suitable for direct inclusion in the git repository as they are too big, they need to be hosted elsewhere. Ensure the media files you upload here are suitable for use on the web, pay attention to file-size, format and resolution. Within these directories, each resource is described by a single Markdown file, with optional YAML frontmatter . The title of the markdown file (a level-one heading, e.g. # ) corresponds with the title of the resource as shown in the frontend. The extension is always .md . The paragraph(s) immediately succeeding the level-one heading are taken to be the description, for tools (unless it concerns a tool suite), this generally be omitted as the description should be automatically drawn from the metadata. It is recommended to keep filename to a simple subset of lowercase ASCII characters, without spaces or any punctuation aside from dashes. The markdown file contains sections marked with level two headings ( ## ), each should correspond to agreed-upon tabs shown in the Ineo frontend, we currently distinguish the following: Overview Learn Mentions The content editors and frontend developers can introduce new ones when needed. Any subsections (level-three headings and beyond) can be used freely. YAML frontmatter The markdown files can be enriched with YAML frontmatter to convey some extra information. This is NOT intended for extensive tool/data metadata, as those come directly from different data provisioning pipelines and are not kept in this repository. The only metadata we always duplicate is the title of the resource, which typically also determines the filename (in a 'slug' form in lowercase only and with spaces replaced by hyphens). --- title: The title of the resource --- For tools , we use either the identifier or group field to link to https://tools.clariah.nl . The identifier links to an exact resource, such as frog for https://tools.clariah.nl/frog , i.e. it is identical to what appears in the URL. The other one, group , links to a group or tool suite because the level of granularity offered on https://tools.clariah.nl may be too fine-grained compared to what is desirable for Ineo or other portals. It should be exact name of the group as it appears on the tool discovery overview. The name s of the groups an be easily spotted by looking at the table of contents there; the groups are the ones with sub-items. --- identifier: tool title: Tool --- If both identifier and group are set, the identifier is used to associate metadata of one specific tool in the group/suite with the group as a whole. This allows you to still describe a whole tool suite in Ineo, but picks one of the tools in the suite as being its representative and have its metadata prominently features. Note that neither identifier nor group uniquely identify a markdown file, they may be reused from multiple files and are merely intended to link to https://tools.clariah.nl . The following Ineo-specific metadata can be added: (TODO: add specification of part of Ineo's YAML syntax that are reusable. This is something for the Ineo developers to specify.) Ineo currently allows for a carousel widget showing several images in sequence (see for example https://www.ineo.tools/resources/media-suite ). The data definition for such a widget would go in the YAML frontmatter, I would propose something like: --- carousel: - /media/mediasuite-cover1.png - https://vimeo.com/503507411?embedded=true&source=vimeo_logo&owner=115309374 - /media/mediasuite-cover2.png - /media/mediasuite-cover3.png - /media/mediasuite-cover4.png --- Note these are specifically for Ineo and won't be previewed in the editor. Whether you images interspersed in the flowing text (easier) or the carousel is up to the maintainer team to decide. Note that for data, no way of linking to underlying data registry via identifiers has been defined yet (take this up with Menzo). Template The following template serves as an example for tools: --- identifier: tool-identifier-from-tools-clariah-nl title: Name of the Tool carousel: - /media/tool-screenshot.png --- # Name of the Tool (a short description can go here) ## Overview (all text you want on the overview tab goes here) ## Learn (all text you want on the learn tab goes here) ## Mentions (all text you want on the mentions tab goes here) Editing Content Management System (CMS) A headless CMS ( DecapCMS previously known as NetlifyCMS) is available to make it easier for content editors to do their editing. Within the CMS, both the Markdown content and the YAML frontmatter can be edited in a WYSIWYG fashion without knowledge of either Markdown or YAML. A GitHub account is required to edit. The CMS can be accessed here The CMS is a nice convenience but it also enforced some limitations, it has put the following constraints on this specification: All media files are stored in a single directory /media/ and does not allow further divisions in subdirectories. Online IDE An alternative to using the CMS is to use GitHub's online Visual Studio Code IDE. Each page on https://clariah.github.io/ineo-content has an edit this page link that automatically takes you to the web-based IDE. This offers a bit more freedom than the CMS. Local editing Being a simple git repository with MarkDown files, you are not constrained to either of the above options but can simply clone this git repository locally and work with your text editor of choice, such as vim, emacs, VSCode, Sublime, or something specifically geared for comfortable Markdown editing such as Zettlr . Version Control As a version control system, git will ensure all versions of the content are tracked throughout their history. You can use git tools to revert to previous versions if needed. It also offers an exact provenance trail of who edited what and when, offering maximum transparency. Contributions and Maintainers Anybody can contribute directly to the content by simply doing pull requests. The maintainers of this repository review and accept or decline those pull requests. This procedure enables the maintainers to easily collaborate with tool and data developers. The maintainers, from CLARIAH's communication department, are: Sebastiaan Fluitsma Emma Verbree Liselore Tissen","title":"Ineo Content"},{"location":"#ineo-content","text":"At this stage this repository is a proposal!","title":"Ineo Content"},{"location":"#introduction","text":"This repository stores the rich content for the Ineo Resource finder. We denote the various curated texts pertaining to resources as 'rich content'. A select team of CLARIAH's communication department is working on these texts. This repository enables easy collaboration, also with developers and data providers, and provides a sustainable storage solution. Rich content can be contrasted to the automatically harvested metadata that results from the CLARIAH Tool Discovery track or the FAIR Datasets track and is not supposed to overlap. For automatically harvested metadata, the primary authorship lies with the tool/data producer. For rich user content, it lies with the people of the communication department (WP1). The rich content in this repository is stored in Markdown format , a simple plain text format for text markup. We follow a specific structure to the data can be easily ingested into Ineo, as well as any other systems who want to make use of it. The content can be previewed directly on https://clariah.github.io/ineo-content . We used a simple static site generator ( MkDocs ) to achieve this. That site, however, is not intended for end-users to be used directly. On the preview site we also provide access to a simple headless CMS to facilitate working with the Markdown files in the git repository, without requiring real technical knowledge of either.","title":"Introduction"},{"location":"#directory-structure","text":"Directories: src/ - Contains Markdown texts tools/ - Contains Markdown texts describing tools. data/ - Contains Markdown texts describing data workflows/ - Contains Markdown texts describing workflows standards/ - Contains Markdown texts describing standards media/ - Stores images or other associated media files, Right now all media files are in this single folder, without subfolders, this limitation is only due to the admin CMS. Images or downloadable documents/spreadsheets/presentations that are referenced from one of the markdown files may be included in this subdirectory as well. You can reference images from the markdown files, just use /media/ as URL prefix when doing so. You can also reference media elsewhere on the web, always sure to use https:// in that case. This should probably be kept to a minimum as there is no guarantee such media will persist in that location over time. Videos are not suitable for direct inclusion in the git repository as they are too big, they need to be hosted elsewhere. Ensure the media files you upload here are suitable for use on the web, pay attention to file-size, format and resolution. Within these directories, each resource is described by a single Markdown file, with optional YAML frontmatter . The title of the markdown file (a level-one heading, e.g. # ) corresponds with the title of the resource as shown in the frontend. The extension is always .md . The paragraph(s) immediately succeeding the level-one heading are taken to be the description, for tools (unless it concerns a tool suite), this generally be omitted as the description should be automatically drawn from the metadata. It is recommended to keep filename to a simple subset of lowercase ASCII characters, without spaces or any punctuation aside from dashes. The markdown file contains sections marked with level two headings ( ## ), each should correspond to agreed-upon tabs shown in the Ineo frontend, we currently distinguish the following: Overview Learn Mentions The content editors and frontend developers can introduce new ones when needed. Any subsections (level-three headings and beyond) can be used freely.","title":"Directory Structure"},{"location":"#yaml-frontmatter","text":"The markdown files can be enriched with YAML frontmatter to convey some extra information. This is NOT intended for extensive tool/data metadata, as those come directly from different data provisioning pipelines and are not kept in this repository. The only metadata we always duplicate is the title of the resource, which typically also determines the filename (in a 'slug' form in lowercase only and with spaces replaced by hyphens). --- title: The title of the resource --- For tools , we use either the identifier or group field to link to https://tools.clariah.nl . The identifier links to an exact resource, such as frog for https://tools.clariah.nl/frog , i.e. it is identical to what appears in the URL. The other one, group , links to a group or tool suite because the level of granularity offered on https://tools.clariah.nl may be too fine-grained compared to what is desirable for Ineo or other portals. It should be exact name of the group as it appears on the tool discovery overview. The name s of the groups an be easily spotted by looking at the table of contents there; the groups are the ones with sub-items. --- identifier: tool title: Tool --- If both identifier and group are set, the identifier is used to associate metadata of one specific tool in the group/suite with the group as a whole. This allows you to still describe a whole tool suite in Ineo, but picks one of the tools in the suite as being its representative and have its metadata prominently features. Note that neither identifier nor group uniquely identify a markdown file, they may be reused from multiple files and are merely intended to link to https://tools.clariah.nl . The following Ineo-specific metadata can be added: (TODO: add specification of part of Ineo's YAML syntax that are reusable. This is something for the Ineo developers to specify.) Ineo currently allows for a carousel widget showing several images in sequence (see for example https://www.ineo.tools/resources/media-suite ). The data definition for such a widget would go in the YAML frontmatter, I would propose something like: --- carousel: - /media/mediasuite-cover1.png - https://vimeo.com/503507411?embedded=true&source=vimeo_logo&owner=115309374 - /media/mediasuite-cover2.png - /media/mediasuite-cover3.png - /media/mediasuite-cover4.png --- Note these are specifically for Ineo and won't be previewed in the editor. Whether you images interspersed in the flowing text (easier) or the carousel is up to the maintainer team to decide. Note that for data, no way of linking to underlying data registry via identifiers has been defined yet (take this up with Menzo).","title":"YAML frontmatter"},{"location":"#template","text":"The following template serves as an example for tools: --- identifier: tool-identifier-from-tools-clariah-nl title: Name of the Tool carousel: - /media/tool-screenshot.png --- # Name of the Tool (a short description can go here) ## Overview (all text you want on the overview tab goes here) ## Learn (all text you want on the learn tab goes here) ## Mentions (all text you want on the mentions tab goes here)","title":"Template"},{"location":"#editing","text":"","title":"Editing"},{"location":"#content-management-system-cms","text":"A headless CMS ( DecapCMS previously known as NetlifyCMS) is available to make it easier for content editors to do their editing. Within the CMS, both the Markdown content and the YAML frontmatter can be edited in a WYSIWYG fashion without knowledge of either Markdown or YAML. A GitHub account is required to edit. The CMS can be accessed here The CMS is a nice convenience but it also enforced some limitations, it has put the following constraints on this specification: All media files are stored in a single directory /media/ and does not allow further divisions in subdirectories.","title":"Content Management System (CMS)"},{"location":"#online-ide","text":"An alternative to using the CMS is to use GitHub's online Visual Studio Code IDE. Each page on https://clariah.github.io/ineo-content has an edit this page link that automatically takes you to the web-based IDE. This offers a bit more freedom than the CMS.","title":"Online IDE"},{"location":"#local-editing","text":"Being a simple git repository with MarkDown files, you are not constrained to either of the above options but can simply clone this git repository locally and work with your text editor of choice, such as vim, emacs, VSCode, Sublime, or something specifically geared for comfortable Markdown editing such as Zettlr .","title":"Local editing"},{"location":"#version-control","text":"As a version control system, git will ensure all versions of the content are tracked throughout their history. You can use git tools to revert to previous versions if needed. It also offers an exact provenance trail of who edited what and when, offering maximum transparency.","title":"Version Control"},{"location":"#contributions-and-maintainers","text":"Anybody can contribute directly to the content by simply doing pull requests. The maintainers of this repository review and accept or decline those pull requests. This procedure enables the maintainers to easily collaborate with tool and data developers. The maintainers, from CLARIAH's communication department, are: Sebastiaan Fluitsma Emma Verbree Liselore Tissen","title":"Contributions and Maintainers"},{"location":"tools/alud/","text":"alud alud is a Go package for deriving Universal Dependencies from Dutch sentences parsed with Alpino. Overview Universal Dependencies (UD) is a framework for consistent annotation of grammar (parts of speech, morphological features, and syntactic dependencies) across different human languages. alud is a package for deriving Universal Dependencies parsed with Alpino, a collection of tools and programs for parsing Dutch sentences into dependency structures. Usually, the input is XML in the alpino_ds format. The output is in the CoNLL-U format, or the Universal Dependencies can be embedded into the alpino_ds format (version 1.10), making them available for XPath queries. It is also possible to embed a user provided file in the CoNLL-U format, and embed this into the alpino_ds format. Learn To see, what the results would look like, have a look this visualisation . For more information, please refer to the ReadMe Installation The package comes with two example programs, alud and alud-dact. To install the demo program alud-dact, you need to have Oracle Berkeley DB XML installed. With alud-dact you can process dact files. For more information, please refer to the ReadMe","title":"alud"},{"location":"tools/alud/#alud","text":"alud is a Go package for deriving Universal Dependencies from Dutch sentences parsed with Alpino.","title":"alud"},{"location":"tools/alud/#overview","text":"Universal Dependencies (UD) is a framework for consistent annotation of grammar (parts of speech, morphological features, and syntactic dependencies) across different human languages. alud is a package for deriving Universal Dependencies parsed with Alpino, a collection of tools and programs for parsing Dutch sentences into dependency structures. Usually, the input is XML in the alpino_ds format. The output is in the CoNLL-U format, or the Universal Dependencies can be embedded into the alpino_ds format (version 1.10), making them available for XPath queries. It is also possible to embed a user provided file in the CoNLL-U format, and embed this into the alpino_ds format.","title":"Overview"},{"location":"tools/alud/#learn","text":"To see, what the results would look like, have a look this visualisation . For more information, please refer to the ReadMe","title":"Learn"},{"location":"tools/alud/#installation","text":"The package comes with two example programs, alud and alud-dact. To install the demo program alud-dact, you need to have Oracle Berkeley DB XML installed. With alud-dact you can process dact files. For more information, please refer to the ReadMe","title":"Installation"},{"location":"tools/asr_nl/","text":"Automatic Speech Recognition for Dutch CLARIAH offers a webservice for automatic speech recognition to provide the transcriptions of recordings spoken in Dutch. Overview The webservice is hosted by the Radboud University and requires an institute login to be used. The user can upload one file per project. Bulk processing is possible after contacting the developers. The webservice offers multiple ASR models to choose from, including one for daily conversations, oral history interviews, parliamentary discussions and Belgian Dutch. The code of the webservice is shared on GitHub . Privacy notice All data you upload to the service and data obtained using the service will remain yours and is accessible only by you and our technical staff. Your data will not be shared with third parties and not be used for any purpose other than the service's operation. You can remove your projects at any time and are encouraged to do so, which will remove your data from our servers permanently. We can not guarantee any long-term storage of your data so you are recommended to download the results and store it yourself immediately; projects on the server will be automatically deleted after 30 days. Despite our security precautions, we do discourage use of this service for highly confidential material as there is no encryption on the storage. Last, we also collect some statistics on the frequency of use of the service, when shared this will always be anonymised. Learn How to use In order to use the ASR webservice, the user needs an institute login. Having logged in, the user can create a project. Old projects are listed on the same page. Once the user has created a project, they can upload a file from their disk. Before doing so, they need to specify the file type (e.g. .wav or .mp3). The user is then prompted to choose the ASR model they want to use. A simple click on the Start button will start the processing of the file. The user may safely close their browser or shut down their computer during this process, the system will keep running on the server and is available when they return another time. The output consists of a textual transcription, including timestamps and speaker diarisation, in plain text as well as XML, and a logging file. User support The current version of the ASR webservice is hosted and maintained by the Centre for Language and Speech Technology of the Radboud University. If you have any suggestions, questions, or general feedback you can contact Henk van den Heuvel . Mentions Webpages Webservice: https://webservices.cls.ru.nl/asr_nl/ Source code: https://github.com/opensource-spraakherkenning-nl/asr_nl","title":"asr_nl"},{"location":"tools/asr_nl/#automatic-speech-recognition-for-dutch","text":"CLARIAH offers a webservice for automatic speech recognition to provide the transcriptions of recordings spoken in Dutch.","title":"Automatic Speech Recognition for Dutch"},{"location":"tools/asr_nl/#overview","text":"The webservice is hosted by the Radboud University and requires an institute login to be used. The user can upload one file per project. Bulk processing is possible after contacting the developers. The webservice offers multiple ASR models to choose from, including one for daily conversations, oral history interviews, parliamentary discussions and Belgian Dutch. The code of the webservice is shared on GitHub .","title":"Overview"},{"location":"tools/asr_nl/#privacy-notice","text":"All data you upload to the service and data obtained using the service will remain yours and is accessible only by you and our technical staff. Your data will not be shared with third parties and not be used for any purpose other than the service's operation. You can remove your projects at any time and are encouraged to do so, which will remove your data from our servers permanently. We can not guarantee any long-term storage of your data so you are recommended to download the results and store it yourself immediately; projects on the server will be automatically deleted after 30 days. Despite our security precautions, we do discourage use of this service for highly confidential material as there is no encryption on the storage. Last, we also collect some statistics on the frequency of use of the service, when shared this will always be anonymised.","title":"Privacy notice"},{"location":"tools/asr_nl/#learn","text":"","title":"Learn"},{"location":"tools/asr_nl/#how-to-use","text":"In order to use the ASR webservice, the user needs an institute login. Having logged in, the user can create a project. Old projects are listed on the same page. Once the user has created a project, they can upload a file from their disk. Before doing so, they need to specify the file type (e.g. .wav or .mp3). The user is then prompted to choose the ASR model they want to use. A simple click on the Start button will start the processing of the file. The user may safely close their browser or shut down their computer during this process, the system will keep running on the server and is available when they return another time. The output consists of a textual transcription, including timestamps and speaker diarisation, in plain text as well as XML, and a logging file.","title":"How to use"},{"location":"tools/asr_nl/#user-support","text":"The current version of the ASR webservice is hosted and maintained by the Centre for Language and Speech Technology of the Radboud University. If you have any suggestions, questions, or general feedback you can contact Henk van den Heuvel .","title":"User support"},{"location":"tools/asr_nl/#mentions","text":"","title":"Mentions"},{"location":"tools/asr_nl/#webpages","text":"Webservice: https://webservices.cls.ru.nl/asr_nl/ Source code: https://github.com/opensource-spraakherkenning-nl/asr_nl","title":"Webpages"},{"location":"tools/cow/","text":"CoW CoW (CSV on the Web) is a conversion tool that transposes tabular datasets in CSV to Linked Data in RDF-format. Overview CoW is a comprehensive and high-performance tool for batch conversion of multiple datasets expressed in CSV to RDF (Linked Data). In a first step, CoW generates a JSON schema file from the input CSV, expressed using an extended version of the W3C standard CSVW, which can be manually adjusted by the user to accommodate their needs: selecting specific columns, creating new virtual columns, combining the values of different columns to mint URIs, etc. In a scond step, CoW uses the instructions in this JSON schema file to build a RDF file correspondingly. CoW uses the W3C standard CSVW for rich semantic table specifications, and nanopublications as an output RDF model. CoW uses a command line interface (CLI) for Python scripts. Instead of using the command line tool there is also the webservice cattle, providing the same functionality that CoW provides without having to install it. However, note that there's a limit to the size of the CSVs you can upload to cattle, conversion could take longer and cattle is no longer being maintained will eventually be taken offline. L\ufeffearn Instruction webpages The ReadMe-section shows which commands to use if you want to use CoW in the most straightforward way. Read a more elaborate explanation of CoW\u2019s usage . The CoW wiki explains how to augment the JSON-schema (with several examples) produced by CoW in the first step of convertion. The developers have organized workshops to show the usage of CoW. Take a look at the workshops slides. Instruction videos General instruction CoW Basic Conversion . In this tutorial (in Dutch) you will see the most basic conversion of a .csv file into triples (RDF) using CoW. For more information on CoW goto https://github.com/clariah/cow/wiki or raise a question via https://github.com/clariah/cow/issues (and click the 'new issue' button). CoW Change Base Uri . In this tutorial (in Dutch) on CoW, it is explained how to create triples using your own domain as the base URI. For more information on CoW goto https://github.com/clariah/cow/wiki or raise a question via https://github.com/clariah/cow/issues (and click the 'new issue' button). Introduction Cliopatria TripleStore . In this tutorial it is shown how to setup a Triplestore on your machine. For more information or support visit: https://cliopatria.swi-prolog.org/swish/pldoc/doc/home/swipl/src/ClioPatria/ClioPatria/web/help/Download.txt Tutorial Tutorial for Cow part 1 and part 2 . Workshops and courses For master and PhD-students within economic and social history, we organize a yearly course Data management for historians. This course lets students work according to the FAIR data principles, instructs them on the basics of quantitative data management, and lets students make and query their own Linked Data. The course consists of 8 weeks, during which experts in the field discuss: The principles of FAIR data Data bases and scripts Pipelines and data cleaning Data visualization The quantitative research cycle The principles of Linked Data Querying Linked Data Download the Data management for historians full course description as pdf . Apply via the Application Form . For questions and more information, contact course coordinator Dr. Rick Mourits . M\ufeffentions Articles (incl. conference papers, presentations and demo\u2019s) Ashkan Ashkpour. \u201cTheory and Practice of Historical Census Data Harmonization. The Dutch Historical Census Use Case: A flexible, structured and accountable approach using Linked Data. Erasmus University (2019). PhD Thesis. Albert Mero\u00f1o-Pe\u00f1uela, Marnix van Berchum, Bram van den Hout. \u201cThe Oldest Song Score in the Newest Notation: The Hurrian Hymn to Nikkal as Linked Data\u201d. Digital Humanities Conference (DH2019), Utrecht, July 9-12 (2019) Albert Mero\u00f1o-Pe\u00f1uela, Ashkan Ashkpour, Richard Zijdeman, Rinke Hoekstra. \u201cMaking Social Science More Reproducible by Encapsulating Access to Linked Data\u201d. In: European Social Science History Conference (ESSHC 2018), 4-7 April, Belfast, North Ireland, UK (2018) Albert Mero\u00f1o-Pe\u00f1uela, Ashkan Ashkpour, Valentijn Gilissen, Jan Jonker, Tom Vreugdenhil, Peter Doorn. \u201cImproving Access to the Dutch Historical Censuses with Linked Open Data\u201d. Research Data Journal for the Humanities and Social Sciences (in print) (2018) Ashkan Ashkpour, Albert Mero\u00f1o-Pe\u00f1uela. \u201cLOCS AND KEYS: Linked Open Classification System and Opening up Knowledge\u201d. 12th European Social Science History Conference. Belfast United Kingdom (2018). Rinke Hoekstra, Albert Mero\u00f1o-Pe\u00f1uela, Auke Rijpma, Richard Zijdeman, Ashkan Ashkpour, Kathrin Dentler, Ivo Zandhuis, Laurens Rietveld. \u201cThe dataLegend Ecosystem for Historical Statistics\u201d. Journal of Web Semantics: Science, Services and Agents on the World Wide Web, volume 50, pp. 49-61 (2018) Tobias Kuhn, Albert Mero\u00f1o-Pe\u00f1uela, Alexander Malic, Jorrit H. Poelen, Allen H. Hurlbert, Emilio Centeno Ortiz, Laura I. Furlong, N\u00faria Queralt-Rosinach, Christine Chichester, Juan M. Banda, Egon Willighagen, Friederike Ehrhart, Chris Evelo, Tareq B. Malas, Michel Dumontier. \u201cNanopublications: A Growing Resource of Provenance-Centric Scientific Linked Data\u201d. IEEE eScience Conference 2018, 29 October \u2013 1 November, Amsterdam, The Netherlands (2018) Ruben Schalk, Auke Rijpma & Richard Zijdeman, \u2018Clariah Datalegend: Linked Economic and Social History Datasets in the Cloud\u2019, World Economic History Conference, Boston (August 2018) Ashkan Ashkpour. LICR Classification System for Religions. (2017). https://datasets.socialhistory.org/dataverse/LICR Ashkan Ashkpour. http://www.licr.io An interface linking to different tools and systems in CLARIAH WP4. (2017). Ashkan Ashkpour. QBer Demonstration, CLARIAH Tech Dag, Utrecht 7 oktober 2016. http://www.clariah.nl/evenementen/tech-dag-2016 . Ashkpour, A., Mandemakers, K., & Boonstra, O. W. A. (2016). Source Oriented Harmonization of Aggregate Historical Census Data: a flexible and accountable approach in RDF. Historical Social Research. 41(4) pp. 296-307. Ashkan Ashkpour and Albert Mero\u00f1o Pe\u00f1uela. Linked Classifications Systems and Religion. Conference: SSHA, Montreal, Canada, November 2-5, 2017. Antal van den Bosch, Lex Heerma van Voss, Karina van Dalen-Oskam and Richard L. Zijdeman, 2017. Panel Digital Humanities. [online] Available at: https://www.eventbrite.nl/e/tickets-knaw-humanities-cluster-opening-29740908859 Francesca Ceolan, Dimitris Alivanistos, Kathrin Dentler, and Auke Rijpma. 2017. Artificial Intelligence and Simulation of Behaviour Convention. The benefits of Linked Data for the Social Sciences. Analyzing economic drivers and network effects of international migration based on semantically integrated data. Forthcoming. Rinke Hoekstra QBer \u2013 Connect your data to the cloud Rinke Hoekstra and Mero\u00f1o-Pe\u00f1uela. QBer \u2013 Crowd Based Coding and Harmonization using Linked Data Albert Mero\u00f1o Pe\u00f1uela, Ashkan Ashkpour, Actionable Data Links: Tools for Reproducibility in Social Science and History. Conference: SSHA, Montreal, Canada, November 2-5, 2017. Rinke Hoekstra & Stefan Slobach Knowledge Representation on the Web Victor de Boer, Albert Mero\u00f1o-Pe\u00f1uela, Niels Ockeloen. \u201cLinked Data for Digital History. Lessons Learned from Three Case Studies\u201d. Mirella Romer Recio, M. Jes\u00fas Colmenero (eds.). Historiograf\u00eda digital: proyectos para almacenar y construir la Historia. Anejos de la Revista de Historiograf\u00eda 4. Universidad Carlos III de Madrid (2016). (PDF) Rinke Hoekstra, Albert Mero\u00f1o-Pe\u00f1uela, Kathrin Dentler, Auke Rijpma, Richard Zijdeman & Ivo Zandhuis An Ecosystem for Linked Humanities Data Albert Mero\u00f1o-Pe\u00f1uela, Ashkan Ashkpour. \u201cHistorical Quantitative Reasoning on the Web\u201d. European Social Science History Conference (ESSHC 2016), Valencia, Spain (2016). (PDF) Albert Mero\u00f1o-Pe\u00f1uela, Rinke Hoekstra. \u201cgrlc Makes GitHub Taste Like Linked Data APIs\u201d. SALAD 2016 \u2014 Services and Applications over Linked Data APIs and Data. International workshop, ESWC 2016, May 29th, Heraklion, Crete, Greece (2016). (PDF) Best SALAD2016 paper award Rinke Hoekstra, Albert Mero\u00f1o-Pe\u00f1uela, Kathrin Dentler, Auke Rijpma, Richard Zijdeman, Ivo Zandhuis. \u201cAn Ecosystem for Linked Humanities Data\u201d. In: Proceedings of the 1st Workshop on Humanities in the SEmantic web (WHiSE 2016). ESWC 2016, May 29th, Heraklion, Crete, Greece (2016). (PDF) Best WHiSE2016 paper award Albert Mero\u00f1o-Pe\u00f1uela. \u201cRefining Statistical Data on the Web\u201d. Vrije Universiteit Amsterdam (2016) (Amazon) (VU-DARE) PhD thesis Ashkan Ashkpour, Albert Mero\u00f1o-Pe\u00f1uela, Kees Mandemakers. \u201cThe Aggregate Dutch Historical Censuses: Harmonization and RDF\u201d. Historical Methods: A Journal of Quantitative and Interdisciplinary History, 48(4), pp. 230-245, 2015. (PDF) Albert Mero\u00f1o-Pe\u00f1uela, Ashkan Ashkpour, Andrea Scharnhorst, Christophe Gu\u00e9ret, Sally Wyatt. \u201cLinked Open Census Data\u201d. DHCommons Journal, 1st issue. (PDF) (HTML) Albert Mero\u00f1o-Pe\u00f1uela, Christophe Gu\u00e9ret, Stefan Schlobach. \u201cLinked Edit Rules: A Web Friendly Way of Checking Quality of RDF Data Cubes\u201d. Proceedings of the 3rd International Workshop on Semantic Statistics (SemStats 2015), ISWC 2015, Bethlehem, PA, USA (2015). (PDF) Bas Stringer, Albert Mero\u00f1o-Pe\u00f1uela, Antonis Loizou, Sanne Abeln, Jaap Heringa. \u201cTo SCRY Linked Data: Extending SPARQL the Easy Way\u201d. Diversity++ workshop, ISWC 2015, Bethlehem, PA, USA (2015). (PDF) Albert Mero\u00f1o-Pe\u00f1uela, Ashkan Ashkpour, Marieke van Erp, Kees Mandemakers, Leen Breure, Andrea Scharnhorst, Stefan Schlobach, Frank van Harmelen. \u201cSemantic Technologies for Historical Research: A Survey\u201d. Semantic Web \u2014 Interoperability, Usability, Applicability, 6(6), pp. 539\u2013564. IOS Press (2015). http://www.semantic-web-journal.net/content/semantic-technologies-historical-research-survey-0 (PDF) Albert Mero\u00f1o-Pe\u00f1uela, Ashkan Ashkpour, Christophe Gu\u00e9ret, Stefan Schlobach. \u201cCEDAR: The Dutch Historical Censuses as Linked Open Data\u201d. Semantic Web \u2014 Interoperability, Usability, Applicability, 8(2), pp. 297\u2013310. IOS Press (2015). http://semantic-web-journal.net/content/cedar-dutch-historical-censuses-linked-open-data-1 (PDF) Albert Mero\u00f1o-Pe\u00f1uela, Rinke Hoekstra. \u201cThe Song Remains the Same: Lossless Conversion and Streaming of MIDI to RDF and Back\u201d. In: 13th Extended Semantic Web Conference (ESWC 2016), posters and demos track. May 29th \u2014 June 2nd, Heraklion, Crete, Greece (2016). (PDF) Albert Mero\u00f1o-Pe\u00f1uela, Rinke Hoekstra. \u201cSPARQL2Git: Transparent SPARQL and Linked Data API Curation via Git\u201d. In: 14th Extended Semantic Web Conference (ESWC 2017), posters and demos track. (under review) (2017) Laurens Rietveld, Rinke Hoekstra. 2015. The YASGUI family of SPARQL clients1. Semantic Web:1-11. IOS Press. Laurens Rietveld, Wouter Beek, Stefan Schlobach, Rinke Hoekstra. 2016. Semantic Web. [PDF] from semantic-web-journal.org Auke Rijpma. WP4-demo, Clariah Toogdag 2017, Amsterdam Auke Rijpma. Presentation \u201cLinked data in the Clariah Structured Datahub\u201d, CREATE Salon, UvA Auke Rijpma. Presentatie \u201cBrede Welvaartsindicator: Een integrale indicator voor het welbevinden van Nederlanders\u201d. Auke Rijpma. Presentation \u201cLinked data in the Clariah Structured Datahub\u201d, Global social science research workshop, Pittsburgh Auke Rijpma. (met Tine de Moor) \u2013 Wat is citizen science en wat moeten we er mee?, KNAW-symposium citizen science. De betrokkenheid van burgers in het wetenschappelijke proces Auke Rijpma. Presentation \u201cWP4: structured data\u201d, CLARIAH-dag, Amersfoort Auke Rijpma. \u201cCombining multiple repositories: the case of the quantity-quality trade-off\u201d. Big Questions, Big Data Conference, International Institute of Social History, Amsterdam. Auke Rijpma. \u201cWhat can\u2019t money buy? Wellbeing and GDP since 1820\u201d. World Economic History Congress (WEHC), Kyoto. Auke Rijpma. \u201cQuantity versus Quality: Household structure, number of siblings, and educational attainment in the long nineteenth century\u201d, WEHC, Kyoto. (With Sarah Carmichael and Lotte van der Vleuten.) Auke Rijpma. \u201cThe Clariah-project and the quantity-quality trade-off : understanding household size and investment in human capital through big data\u201d, Posthumus Conference, Brussels. Auke Rijpma. \u201cFamily constraints on women\u2019s agency: measurement and persistence\u201d, Workshop Murdock and Goody Re-visited: (Pre)history and evolution of Eurasian and African family systems. Max Planck Institute for Social Anthropology, Halle. Veruska Zamborlini, Rinke Hoekstra, Marcos da Silveira, Cedric Pruski, Annette ten Teije, Frank van Harmelen. 2016. Generalizing the Detection of Clinical Guideline Interactions Enhanced with LOD. International Joint Conference on Biomedical Engineering Systems and Technologies: 360-386. Springer, Cham. Awards and grants Auke Rijpma, Kathrin Dentler, Rinke Hoekstra, Albert Mero\u00f1o-Penuela & Richard Zijdeman. TRUMP: The RDF Unified Migration Portal. Scholarship for a research master. Albert Mero\u00f1o-Pe\u00f1uela, Rinke Hoekstra. \u201cgrlc Makes GitHub Taste Like Linked Data APIs\u201d. SALAD 2016 \u2014 Services and Applications over Linked Data APIs and Data. International workshop, ESWC 2016, May 29th, Heraklion, Crete, Greece (2016). (PDF) Best SALAD2016 paper award Marieke van Erp & Richard Zijdeman. 2016. \u201cIf buildings could talk\u201d. Audience Award for demo build during the Royal Library and National Digital Heritage Hack-a-LOD. [PDF] Rinke Hoekstra, Albert Mero\u00f1o-Pe\u00f1uela, Kathrin Dentler, Auke Rijpma, Richard Zijdeman, Ivo Zandhuis. \u201cAn Ecosystem for Linked Humanities Data\u201d. In: Proceedings of the 1st Workshop on Humanities in the SEmantic web (WHiSE 2016). ESWC 2016, May 29th, Heraklion, Crete, Greece (2016). (PDF) Best WHiSE2016 paper award. Richard Zijdeman & Ashkan Ashkpour. KDP DANS, k\u20ac10. (2017). Project name: Linking past and present: Augmenting historical municipality characteristics through harmonization and linkage with contemporary data. Richard Zijdeman, Antske Fokkens, Auke Rijpma & Martijn Kleppe. CLARIAH research call, k\u20ac55. (t.b.c. April 2017). HHuCap: The History of Human Capital. Books Auke Rijpma. Cliometrics and the family: global patterns and diverging development. Berlin: Springer Verlag, forthcoming. (With Claude Diebolt, Sarah Carmichael, Selin Dilli, Charlotte St\u00f6rmer, eds.) Auke Rijpma. Agency, gender, and economic development in the world economy, 1850\u20132000: testing the Sen hypothesis. Forthcoming at Routledge/Ashgate Publishing. (With Jan Luiten van Zanden en Jan Kok, eds.) Book chapters Auke Rijpma. \u201cQuantity versus Quality: Household structure, number of siblings, and educational attainment in the long nineteenth century\u201d. Forthcoming in Van Zanden, Kok and Rijpma eds. Agency, gender, and economic development in the world economy, 1850\u20132000: testing the Sen hypothesis (with Sarah Carmichael and Lotte van der Vleuten). Auke Rijpma. \u201cMeasuring agency\u201d. Forthcoming in Van Zanden, Kok and Rijpma eds. Agency, gender, and economic development in the world economy, 1850\u20132000: testing the Sen hypothesis (with Sarah Carmichael). Auke Rijpma. \u201cWomen in global economic history\u201d. In Joerg Baten (ed.), A History of the Global Economy: From 1500 to the Present (Cambridge: Cambridge University Press). (With Selin Dilli and Sarah Carmichael.) Projects Auke Rijpma. Record linkage for Cape of Good Hope Panel. Teaching and Instruction Paul S. Lambert and Richard L. Zijdeman, 2015. Introduction to Multilevel Models. Paul S. Lambert and Richard L. Zijdeman, 2016. Introduction to Multilevel Models. Auke Rijpma. Introduction R and record linkage at LEAP-conference, South Africa Auke Rijpma. Introduction Linked Data and SPARQL @ Amsterdam ThatCamp Richard L. Zijdeman, 2016d. Augmenting Historical Data. [online] Available at: https://arthist.net/archive/13218/view=pdf Richard L. Zijdeman, 2016e. Course: Introduction into R. Richard L. Zijdeman, 2015b. Workshop: Introduction to R. [online] Available at: http://www.ehps-net.eu/sites/default/files/program_cluj_summer_school_2015.pdf Richard L. Zijdeman, 2017b. Historical Occupational Classification and Stratification Schemes: HISCO, HISCLASS & HISCAM. [online] Available at: http://iegd.csic.es/sites/default/files/content/event/2017/programa_del_ciclofbbva2017final_1.pdf Richard L. Zijdeman, 2015a. The not so U-shaped curve of female labour force participation of married women: The United States, 1860-2010. [online] Available at: https://pure.knaw.nl/portal/files/1011394/ushp_lund02.pdf Richard L. Zijdeman, 2016a. Advancing the comparability of occupations through Linked Open Data. [online] Available at: http://www.ed.lu.se/previous_seminars Richard L. Zijdeman, 2016b. Advancing the comparability of occupations through Linked Open Data. [online] Available at: http://www.ed.lu.se/previous_seminars Richard L. Zijdeman, 2016c. Advancing the comparability of occupations through Linked Open Data. [online] Available at: http://www.cedar.umu.se/digitalAssets/176/176124_agenda-ehps-net-cedar-database-workshop-16-18-feb-2016.pdf Richard L. Zijdeman, 2016f. Historical Demography: Reconstructing Life Course Dynamics. [online] Available at: http://www.slideshare.net/rlzijdeman/historical-occupational-classification-and-occupational-stratification-schemes Richard L. Zijdeman, 2016g. The not so U-shaped Curve of Female Labour Force Participation of Married Women: the United States, 1860-2010. [online] Available at: https://www.slideshare.net/rlzijdeman/labour-force-participation-of-married-women-us-18602010 Richard L. Zijdeman, 2017a. HISCO to RDF. Anticipating expansion of occupational descriptions and tooling. Invited Lecture at Stirling University. 13-01-2017. Richard L. Zijdeman, 2017c. WP4: dataLegend (project update). [online] Available at: https://www.slideshare.net/rlzijdeman/toogdag-2017 Richard L. Zijdeman, Antske Fokkens, Auke Rijpma and Martijn Kleppe, 2017. HHuCap: The History of HUman Capital. [online] Available at: https://www.clariah.nl/evenementen/toog-dag-2017#research-pilot-presentations Richard L. Zijdeman and Marieke van Erp, 2016. If Buildings Could Talk: Constructing Buildings\u2019 Biographies. [online] Available at: http://www.pilod.nl/wiki/LinkedDataSeminar-December2,2016 Richard L. Zijdeman and Rombert Stapel, 2016. Work in a globalized world. An algorithm allocating labour relations to digitized census data. [online] Available at: http://dh2016.adho.org/ * Rombert Stapel and Richard L. Zijdeman, 2016. The Influence of Educational Attainment on Self-Employment across Occupational Sectors: United States, 1850-2010. [online] Available at: https://socialhistory.org/en/events/workshop-self-employment-historical-perspective","title":"CoW"},{"location":"tools/cow/#cow","text":"CoW (CSV on the Web) is a conversion tool that transposes tabular datasets in CSV to Linked Data in RDF-format.","title":"CoW"},{"location":"tools/cow/#overview","text":"CoW is a comprehensive and high-performance tool for batch conversion of multiple datasets expressed in CSV to RDF (Linked Data). In a first step, CoW generates a JSON schema file from the input CSV, expressed using an extended version of the W3C standard CSVW, which can be manually adjusted by the user to accommodate their needs: selecting specific columns, creating new virtual columns, combining the values of different columns to mint URIs, etc. In a scond step, CoW uses the instructions in this JSON schema file to build a RDF file correspondingly. CoW uses the W3C standard CSVW for rich semantic table specifications, and nanopublications as an output RDF model. CoW uses a command line interface (CLI) for Python scripts. Instead of using the command line tool there is also the webservice cattle, providing the same functionality that CoW provides without having to install it. However, note that there's a limit to the size of the CSVs you can upload to cattle, conversion could take longer and cattle is no longer being maintained will eventually be taken offline.","title":"Overview"},{"location":"tools/cow/#learn","text":"","title":"L\ufeffearn"},{"location":"tools/cow/#instruction-webpages","text":"The ReadMe-section shows which commands to use if you want to use CoW in the most straightforward way. Read a more elaborate explanation of CoW\u2019s usage . The CoW wiki explains how to augment the JSON-schema (with several examples) produced by CoW in the first step of convertion. The developers have organized workshops to show the usage of CoW. Take a look at the workshops slides.","title":"Instruction webpages"},{"location":"tools/cow/#instruction-videos","text":"General instruction CoW Basic Conversion . In this tutorial (in Dutch) you will see the most basic conversion of a .csv file into triples (RDF) using CoW. For more information on CoW goto https://github.com/clariah/cow/wiki or raise a question via https://github.com/clariah/cow/issues (and click the 'new issue' button). CoW Change Base Uri . In this tutorial (in Dutch) on CoW, it is explained how to create triples using your own domain as the base URI. For more information on CoW goto https://github.com/clariah/cow/wiki or raise a question via https://github.com/clariah/cow/issues (and click the 'new issue' button). Introduction Cliopatria TripleStore . In this tutorial it is shown how to setup a Triplestore on your machine. For more information or support visit: https://cliopatria.swi-prolog.org/swish/pldoc/doc/home/swipl/src/ClioPatria/ClioPatria/web/help/Download.txt","title":"Instruction videos"},{"location":"tools/cow/#tutorial","text":"Tutorial for Cow part 1 and part 2 .","title":"Tutorial"},{"location":"tools/cow/#workshops-and-courses","text":"For master and PhD-students within economic and social history, we organize a yearly course Data management for historians. This course lets students work according to the FAIR data principles, instructs them on the basics of quantitative data management, and lets students make and query their own Linked Data. The course consists of 8 weeks, during which experts in the field discuss: The principles of FAIR data Data bases and scripts Pipelines and data cleaning Data visualization The quantitative research cycle The principles of Linked Data Querying Linked Data Download the Data management for historians full course description as pdf . Apply via the Application Form . For questions and more information, contact course coordinator Dr. Rick Mourits .","title":"Workshops and courses"},{"location":"tools/cow/#mentions","text":"","title":"M\ufeffentions"},{"location":"tools/cow/#articles-incl-conference-papers-presentations-and-demos","text":"Ashkan Ashkpour. \u201cTheory and Practice of Historical Census Data Harmonization. The Dutch Historical Census Use Case: A flexible, structured and accountable approach using Linked Data. Erasmus University (2019). PhD Thesis. Albert Mero\u00f1o-Pe\u00f1uela, Marnix van Berchum, Bram van den Hout. \u201cThe Oldest Song Score in the Newest Notation: The Hurrian Hymn to Nikkal as Linked Data\u201d. Digital Humanities Conference (DH2019), Utrecht, July 9-12 (2019) Albert Mero\u00f1o-Pe\u00f1uela, Ashkan Ashkpour, Richard Zijdeman, Rinke Hoekstra. \u201cMaking Social Science More Reproducible by Encapsulating Access to Linked Data\u201d. In: European Social Science History Conference (ESSHC 2018), 4-7 April, Belfast, North Ireland, UK (2018) Albert Mero\u00f1o-Pe\u00f1uela, Ashkan Ashkpour, Valentijn Gilissen, Jan Jonker, Tom Vreugdenhil, Peter Doorn. \u201cImproving Access to the Dutch Historical Censuses with Linked Open Data\u201d. Research Data Journal for the Humanities and Social Sciences (in print) (2018) Ashkan Ashkpour, Albert Mero\u00f1o-Pe\u00f1uela. \u201cLOCS AND KEYS: Linked Open Classification System and Opening up Knowledge\u201d. 12th European Social Science History Conference. Belfast United Kingdom (2018). Rinke Hoekstra, Albert Mero\u00f1o-Pe\u00f1uela, Auke Rijpma, Richard Zijdeman, Ashkan Ashkpour, Kathrin Dentler, Ivo Zandhuis, Laurens Rietveld. \u201cThe dataLegend Ecosystem for Historical Statistics\u201d. Journal of Web Semantics: Science, Services and Agents on the World Wide Web, volume 50, pp. 49-61 (2018) Tobias Kuhn, Albert Mero\u00f1o-Pe\u00f1uela, Alexander Malic, Jorrit H. Poelen, Allen H. Hurlbert, Emilio Centeno Ortiz, Laura I. Furlong, N\u00faria Queralt-Rosinach, Christine Chichester, Juan M. Banda, Egon Willighagen, Friederike Ehrhart, Chris Evelo, Tareq B. Malas, Michel Dumontier. \u201cNanopublications: A Growing Resource of Provenance-Centric Scientific Linked Data\u201d. IEEE eScience Conference 2018, 29 October \u2013 1 November, Amsterdam, The Netherlands (2018) Ruben Schalk, Auke Rijpma & Richard Zijdeman, \u2018Clariah Datalegend: Linked Economic and Social History Datasets in the Cloud\u2019, World Economic History Conference, Boston (August 2018) Ashkan Ashkpour. LICR Classification System for Religions. (2017). https://datasets.socialhistory.org/dataverse/LICR Ashkan Ashkpour. http://www.licr.io An interface linking to different tools and systems in CLARIAH WP4. (2017). Ashkan Ashkpour. QBer Demonstration, CLARIAH Tech Dag, Utrecht 7 oktober 2016. http://www.clariah.nl/evenementen/tech-dag-2016 . Ashkpour, A., Mandemakers, K., & Boonstra, O. W. A. (2016). Source Oriented Harmonization of Aggregate Historical Census Data: a flexible and accountable approach in RDF. Historical Social Research. 41(4) pp. 296-307. Ashkan Ashkpour and Albert Mero\u00f1o Pe\u00f1uela. Linked Classifications Systems and Religion. Conference: SSHA, Montreal, Canada, November 2-5, 2017. Antal van den Bosch, Lex Heerma van Voss, Karina van Dalen-Oskam and Richard L. Zijdeman, 2017. Panel Digital Humanities. [online] Available at: https://www.eventbrite.nl/e/tickets-knaw-humanities-cluster-opening-29740908859 Francesca Ceolan, Dimitris Alivanistos, Kathrin Dentler, and Auke Rijpma. 2017. Artificial Intelligence and Simulation of Behaviour Convention. The benefits of Linked Data for the Social Sciences. Analyzing economic drivers and network effects of international migration based on semantically integrated data. Forthcoming. Rinke Hoekstra QBer \u2013 Connect your data to the cloud Rinke Hoekstra and Mero\u00f1o-Pe\u00f1uela. QBer \u2013 Crowd Based Coding and Harmonization using Linked Data Albert Mero\u00f1o Pe\u00f1uela, Ashkan Ashkpour, Actionable Data Links: Tools for Reproducibility in Social Science and History. Conference: SSHA, Montreal, Canada, November 2-5, 2017. Rinke Hoekstra & Stefan Slobach Knowledge Representation on the Web Victor de Boer, Albert Mero\u00f1o-Pe\u00f1uela, Niels Ockeloen. \u201cLinked Data for Digital History. Lessons Learned from Three Case Studies\u201d. Mirella Romer Recio, M. Jes\u00fas Colmenero (eds.). Historiograf\u00eda digital: proyectos para almacenar y construir la Historia. Anejos de la Revista de Historiograf\u00eda 4. Universidad Carlos III de Madrid (2016). (PDF) Rinke Hoekstra, Albert Mero\u00f1o-Pe\u00f1uela, Kathrin Dentler, Auke Rijpma, Richard Zijdeman & Ivo Zandhuis An Ecosystem for Linked Humanities Data Albert Mero\u00f1o-Pe\u00f1uela, Ashkan Ashkpour. \u201cHistorical Quantitative Reasoning on the Web\u201d. European Social Science History Conference (ESSHC 2016), Valencia, Spain (2016). (PDF) Albert Mero\u00f1o-Pe\u00f1uela, Rinke Hoekstra. \u201cgrlc Makes GitHub Taste Like Linked Data APIs\u201d. SALAD 2016 \u2014 Services and Applications over Linked Data APIs and Data. International workshop, ESWC 2016, May 29th, Heraklion, Crete, Greece (2016). (PDF) Best SALAD2016 paper award Rinke Hoekstra, Albert Mero\u00f1o-Pe\u00f1uela, Kathrin Dentler, Auke Rijpma, Richard Zijdeman, Ivo Zandhuis. \u201cAn Ecosystem for Linked Humanities Data\u201d. In: Proceedings of the 1st Workshop on Humanities in the SEmantic web (WHiSE 2016). ESWC 2016, May 29th, Heraklion, Crete, Greece (2016). (PDF) Best WHiSE2016 paper award Albert Mero\u00f1o-Pe\u00f1uela. \u201cRefining Statistical Data on the Web\u201d. Vrije Universiteit Amsterdam (2016) (Amazon) (VU-DARE) PhD thesis Ashkan Ashkpour, Albert Mero\u00f1o-Pe\u00f1uela, Kees Mandemakers. \u201cThe Aggregate Dutch Historical Censuses: Harmonization and RDF\u201d. Historical Methods: A Journal of Quantitative and Interdisciplinary History, 48(4), pp. 230-245, 2015. (PDF) Albert Mero\u00f1o-Pe\u00f1uela, Ashkan Ashkpour, Andrea Scharnhorst, Christophe Gu\u00e9ret, Sally Wyatt. \u201cLinked Open Census Data\u201d. DHCommons Journal, 1st issue. (PDF) (HTML) Albert Mero\u00f1o-Pe\u00f1uela, Christophe Gu\u00e9ret, Stefan Schlobach. \u201cLinked Edit Rules: A Web Friendly Way of Checking Quality of RDF Data Cubes\u201d. Proceedings of the 3rd International Workshop on Semantic Statistics (SemStats 2015), ISWC 2015, Bethlehem, PA, USA (2015). (PDF) Bas Stringer, Albert Mero\u00f1o-Pe\u00f1uela, Antonis Loizou, Sanne Abeln, Jaap Heringa. \u201cTo SCRY Linked Data: Extending SPARQL the Easy Way\u201d. Diversity++ workshop, ISWC 2015, Bethlehem, PA, USA (2015). (PDF) Albert Mero\u00f1o-Pe\u00f1uela, Ashkan Ashkpour, Marieke van Erp, Kees Mandemakers, Leen Breure, Andrea Scharnhorst, Stefan Schlobach, Frank van Harmelen. \u201cSemantic Technologies for Historical Research: A Survey\u201d. Semantic Web \u2014 Interoperability, Usability, Applicability, 6(6), pp. 539\u2013564. IOS Press (2015). http://www.semantic-web-journal.net/content/semantic-technologies-historical-research-survey-0 (PDF) Albert Mero\u00f1o-Pe\u00f1uela, Ashkan Ashkpour, Christophe Gu\u00e9ret, Stefan Schlobach. \u201cCEDAR: The Dutch Historical Censuses as Linked Open Data\u201d. Semantic Web \u2014 Interoperability, Usability, Applicability, 8(2), pp. 297\u2013310. IOS Press (2015). http://semantic-web-journal.net/content/cedar-dutch-historical-censuses-linked-open-data-1 (PDF) Albert Mero\u00f1o-Pe\u00f1uela, Rinke Hoekstra. \u201cThe Song Remains the Same: Lossless Conversion and Streaming of MIDI to RDF and Back\u201d. In: 13th Extended Semantic Web Conference (ESWC 2016), posters and demos track. May 29th \u2014 June 2nd, Heraklion, Crete, Greece (2016). (PDF) Albert Mero\u00f1o-Pe\u00f1uela, Rinke Hoekstra. \u201cSPARQL2Git: Transparent SPARQL and Linked Data API Curation via Git\u201d. In: 14th Extended Semantic Web Conference (ESWC 2017), posters and demos track. (under review) (2017) Laurens Rietveld, Rinke Hoekstra. 2015. The YASGUI family of SPARQL clients1. Semantic Web:1-11. IOS Press. Laurens Rietveld, Wouter Beek, Stefan Schlobach, Rinke Hoekstra. 2016. Semantic Web. [PDF] from semantic-web-journal.org Auke Rijpma. WP4-demo, Clariah Toogdag 2017, Amsterdam Auke Rijpma. Presentation \u201cLinked data in the Clariah Structured Datahub\u201d, CREATE Salon, UvA Auke Rijpma. Presentatie \u201cBrede Welvaartsindicator: Een integrale indicator voor het welbevinden van Nederlanders\u201d. Auke Rijpma. Presentation \u201cLinked data in the Clariah Structured Datahub\u201d, Global social science research workshop, Pittsburgh Auke Rijpma. (met Tine de Moor) \u2013 Wat is citizen science en wat moeten we er mee?, KNAW-symposium citizen science. De betrokkenheid van burgers in het wetenschappelijke proces Auke Rijpma. Presentation \u201cWP4: structured data\u201d, CLARIAH-dag, Amersfoort Auke Rijpma. \u201cCombining multiple repositories: the case of the quantity-quality trade-off\u201d. Big Questions, Big Data Conference, International Institute of Social History, Amsterdam. Auke Rijpma. \u201cWhat can\u2019t money buy? Wellbeing and GDP since 1820\u201d. World Economic History Congress (WEHC), Kyoto. Auke Rijpma. \u201cQuantity versus Quality: Household structure, number of siblings, and educational attainment in the long nineteenth century\u201d, WEHC, Kyoto. (With Sarah Carmichael and Lotte van der Vleuten.) Auke Rijpma. \u201cThe Clariah-project and the quantity-quality trade-off : understanding household size and investment in human capital through big data\u201d, Posthumus Conference, Brussels. Auke Rijpma. \u201cFamily constraints on women\u2019s agency: measurement and persistence\u201d, Workshop Murdock and Goody Re-visited: (Pre)history and evolution of Eurasian and African family systems. Max Planck Institute for Social Anthropology, Halle. Veruska Zamborlini, Rinke Hoekstra, Marcos da Silveira, Cedric Pruski, Annette ten Teije, Frank van Harmelen. 2016. Generalizing the Detection of Clinical Guideline Interactions Enhanced with LOD. International Joint Conference on Biomedical Engineering Systems and Technologies: 360-386. Springer, Cham.","title":"Articles (incl. conference papers, presentations and demo\u2019s)"},{"location":"tools/cow/#awards-and-grants","text":"Auke Rijpma, Kathrin Dentler, Rinke Hoekstra, Albert Mero\u00f1o-Penuela & Richard Zijdeman. TRUMP: The RDF Unified Migration Portal. Scholarship for a research master. Albert Mero\u00f1o-Pe\u00f1uela, Rinke Hoekstra. \u201cgrlc Makes GitHub Taste Like Linked Data APIs\u201d. SALAD 2016 \u2014 Services and Applications over Linked Data APIs and Data. International workshop, ESWC 2016, May 29th, Heraklion, Crete, Greece (2016). (PDF) Best SALAD2016 paper award Marieke van Erp & Richard Zijdeman. 2016. \u201cIf buildings could talk\u201d. Audience Award for demo build during the Royal Library and National Digital Heritage Hack-a-LOD. [PDF] Rinke Hoekstra, Albert Mero\u00f1o-Pe\u00f1uela, Kathrin Dentler, Auke Rijpma, Richard Zijdeman, Ivo Zandhuis. \u201cAn Ecosystem for Linked Humanities Data\u201d. In: Proceedings of the 1st Workshop on Humanities in the SEmantic web (WHiSE 2016). ESWC 2016, May 29th, Heraklion, Crete, Greece (2016). (PDF) Best WHiSE2016 paper award. Richard Zijdeman & Ashkan Ashkpour. KDP DANS, k\u20ac10. (2017). Project name: Linking past and present: Augmenting historical municipality characteristics through harmonization and linkage with contemporary data. Richard Zijdeman, Antske Fokkens, Auke Rijpma & Martijn Kleppe. CLARIAH research call, k\u20ac55. (t.b.c. April 2017). HHuCap: The History of Human Capital.","title":"Awards and grants"},{"location":"tools/cow/#books","text":"Auke Rijpma. Cliometrics and the family: global patterns and diverging development. Berlin: Springer Verlag, forthcoming. (With Claude Diebolt, Sarah Carmichael, Selin Dilli, Charlotte St\u00f6rmer, eds.) Auke Rijpma. Agency, gender, and economic development in the world economy, 1850\u20132000: testing the Sen hypothesis. Forthcoming at Routledge/Ashgate Publishing. (With Jan Luiten van Zanden en Jan Kok, eds.)","title":"Books"},{"location":"tools/cow/#book-chapters","text":"Auke Rijpma. \u201cQuantity versus Quality: Household structure, number of siblings, and educational attainment in the long nineteenth century\u201d. Forthcoming in Van Zanden, Kok and Rijpma eds. Agency, gender, and economic development in the world economy, 1850\u20132000: testing the Sen hypothesis (with Sarah Carmichael and Lotte van der Vleuten). Auke Rijpma. \u201cMeasuring agency\u201d. Forthcoming in Van Zanden, Kok and Rijpma eds. Agency, gender, and economic development in the world economy, 1850\u20132000: testing the Sen hypothesis (with Sarah Carmichael). Auke Rijpma. \u201cWomen in global economic history\u201d. In Joerg Baten (ed.), A History of the Global Economy: From 1500 to the Present (Cambridge: Cambridge University Press). (With Selin Dilli and Sarah Carmichael.)","title":"Book chapters"},{"location":"tools/cow/#projects","text":"Auke Rijpma. Record linkage for Cape of Good Hope Panel.","title":"Projects"},{"location":"tools/cow/#teaching-and-instruction","text":"Paul S. Lambert and Richard L. Zijdeman, 2015. Introduction to Multilevel Models. Paul S. Lambert and Richard L. Zijdeman, 2016. Introduction to Multilevel Models. Auke Rijpma. Introduction R and record linkage at LEAP-conference, South Africa Auke Rijpma. Introduction Linked Data and SPARQL @ Amsterdam ThatCamp Richard L. Zijdeman, 2016d. Augmenting Historical Data. [online] Available at: https://arthist.net/archive/13218/view=pdf Richard L. Zijdeman, 2016e. Course: Introduction into R. Richard L. Zijdeman, 2015b. Workshop: Introduction to R. [online] Available at: http://www.ehps-net.eu/sites/default/files/program_cluj_summer_school_2015.pdf Richard L. Zijdeman, 2017b. Historical Occupational Classification and Stratification Schemes: HISCO, HISCLASS & HISCAM. [online] Available at: http://iegd.csic.es/sites/default/files/content/event/2017/programa_del_ciclofbbva2017final_1.pdf Richard L. Zijdeman, 2015a. The not so U-shaped curve of female labour force participation of married women: The United States, 1860-2010. [online] Available at: https://pure.knaw.nl/portal/files/1011394/ushp_lund02.pdf Richard L. Zijdeman, 2016a. Advancing the comparability of occupations through Linked Open Data. [online] Available at: http://www.ed.lu.se/previous_seminars Richard L. Zijdeman, 2016b. Advancing the comparability of occupations through Linked Open Data. [online] Available at: http://www.ed.lu.se/previous_seminars Richard L. Zijdeman, 2016c. Advancing the comparability of occupations through Linked Open Data. [online] Available at: http://www.cedar.umu.se/digitalAssets/176/176124_agenda-ehps-net-cedar-database-workshop-16-18-feb-2016.pdf Richard L. Zijdeman, 2016f. Historical Demography: Reconstructing Life Course Dynamics. [online] Available at: http://www.slideshare.net/rlzijdeman/historical-occupational-classification-and-occupational-stratification-schemes Richard L. Zijdeman, 2016g. The not so U-shaped Curve of Female Labour Force Participation of Married Women: the United States, 1860-2010. [online] Available at: https://www.slideshare.net/rlzijdeman/labour-force-participation-of-married-women-us-18602010 Richard L. Zijdeman, 2017a. HISCO to RDF. Anticipating expansion of occupational descriptions and tooling. Invited Lecture at Stirling University. 13-01-2017. Richard L. Zijdeman, 2017c. WP4: dataLegend (project update). [online] Available at: https://www.slideshare.net/rlzijdeman/toogdag-2017 Richard L. Zijdeman, Antske Fokkens, Auke Rijpma and Martijn Kleppe, 2017. HHuCap: The History of HUman Capital. [online] Available at: https://www.clariah.nl/evenementen/toog-dag-2017#research-pilot-presentations Richard L. Zijdeman and Marieke van Erp, 2016. If Buildings Could Talk: Constructing Buildings\u2019 Biographies. [online] Available at: http://www.pilod.nl/wiki/LinkedDataSeminar-December2,2016 Richard L. Zijdeman and Rombert Stapel, 2016. Work in a globalized world. An algorithm allocating labour relations to digitized census data. [online] Available at: http://dh2016.adho.org/ * Rombert Stapel and Richard L. Zijdeman, 2016. The Influence of Educational Attainment on Self-Employment across Occupational Sectors: United States, 1850-2010. [online] Available at: https://socialhistory.org/en/events/workshop-self-employment-historical-perspective","title":"Teaching and Instruction"},{"location":"tools/frog/","text":"Frog Frog is an integration of memory-based natural language processing (NLP) modules developed for Dutch. It performs automatic linguistic enrichment such as part of speech tagging, lemmatisation, named entity recognition, shallow parsing, dependency parsing and morphological analysis. Overview Frog is an integration of memory-based natural language processing (NLP) modules developed for Dutch. Frog performs tokenization, part-of-speech tagging, lemmatization and morphological segmentation of word tokens. At the sentence level Frog identifies non-embedded phrase chunks in the sentence, recognizes named entities and assigns a dependency parse graph. Frog produces output in either FoLiA XML or a simple tab-delimited column format with one token per line. Where possible, Frog makes use of multi-processor support to run subtasks in parallel. Frog offers a command-line interface (that can also run as a daemon) and a C++ library. Learn All NLP modules are based on Timbl, the Tilburg memory-based learning software package. Most modules were created in the 1990s at the ILK Research Group (Tilburg University, the Netherlands) and the CLiPS Research Centre (University of Antwerp, Belgium). Over the years they have been integrated into a single text processing tool, which is currently maintained and developed by the Language Machines Research Group and the Centre for Language and Speech Technology at Radboud University (Nijmegen, the Netherlands). Frog produces FoLiA XML, or tab-delimited column-formatted output. To learn more about what the output contains, have look at the website . Installation There are two ways to install the software: You can download Frog, manually compile and install it from source. However, due to the many dependencies and required technical expertise this is not an easy endeavor. The recommendation methods is using LaMachine. Frog is part of theLaMachine software distribution and includes all necessary dependencies. It runs on Linux, BSD and Mac OS X. It can also run as a virtual machine under other operating systems, including Windows. LaMachine makes the installation of Frog straightforward; detailed instructions for the installation of LaMachine can be found here: http://proycon.github.io/LaMachine/ . Using Frog from Python It is also possible to call Frog directly from Python using the python-frog software library. Contrary to the Frog client for Python discussed in Section [servermode], this library is a direct binding with code from Frog and does not use a client/server model. It therefore offers the tightest form of integration, and highest performance, possible. The Python-Frog library is not included with Frog itself, but is shipped separately from https://github.com/proycon/python-frog. Users who installed Frog using LaMachine, however, will already find that this software has been installed. Other users will need to compile and install it from source. First ensure Frog itself is installed, then install the dependency cython [14]. Installation of Python-Frog is then done by running: $ python setup.py install from its directory. F\ufeffor more information, have a look at the website . Mentions Articles (incl. conference papers, presentations and demo\u2019s) Projects Teaching and Instruction","title":"Frog"},{"location":"tools/frog/#frog","text":"Frog is an integration of memory-based natural language processing (NLP) modules developed for Dutch. It performs automatic linguistic enrichment such as part of speech tagging, lemmatisation, named entity recognition, shallow parsing, dependency parsing and morphological analysis.","title":"Frog"},{"location":"tools/frog/#overview","text":"Frog is an integration of memory-based natural language processing (NLP) modules developed for Dutch. Frog performs tokenization, part-of-speech tagging, lemmatization and morphological segmentation of word tokens. At the sentence level Frog identifies non-embedded phrase chunks in the sentence, recognizes named entities and assigns a dependency parse graph. Frog produces output in either FoLiA XML or a simple tab-delimited column format with one token per line. Where possible, Frog makes use of multi-processor support to run subtasks in parallel. Frog offers a command-line interface (that can also run as a daemon) and a C++ library.","title":"Overview"},{"location":"tools/frog/#learn","text":"All NLP modules are based on Timbl, the Tilburg memory-based learning software package. Most modules were created in the 1990s at the ILK Research Group (Tilburg University, the Netherlands) and the CLiPS Research Centre (University of Antwerp, Belgium). Over the years they have been integrated into a single text processing tool, which is currently maintained and developed by the Language Machines Research Group and the Centre for Language and Speech Technology at Radboud University (Nijmegen, the Netherlands). Frog produces FoLiA XML, or tab-delimited column-formatted output. To learn more about what the output contains, have look at the website .","title":"Learn"},{"location":"tools/frog/#installation","text":"There are two ways to install the software: You can download Frog, manually compile and install it from source. However, due to the many dependencies and required technical expertise this is not an easy endeavor. The recommendation methods is using LaMachine. Frog is part of theLaMachine software distribution and includes all necessary dependencies. It runs on Linux, BSD and Mac OS X. It can also run as a virtual machine under other operating systems, including Windows. LaMachine makes the installation of Frog straightforward; detailed instructions for the installation of LaMachine can be found here: http://proycon.github.io/LaMachine/ .","title":"Installation"},{"location":"tools/frog/#using-frog-from-python","text":"It is also possible to call Frog directly from Python using the python-frog software library. Contrary to the Frog client for Python discussed in Section [servermode], this library is a direct binding with code from Frog and does not use a client/server model. It therefore offers the tightest form of integration, and highest performance, possible. The Python-Frog library is not included with Frog itself, but is shipped separately from https://github.com/proycon/python-frog. Users who installed Frog using LaMachine, however, will already find that this software has been installed. Other users will need to compile and install it from source. First ensure Frog itself is installed, then install the dependency cython [14]. Installation of Python-Frog is then done by running: $ python setup.py install from its directory. F\ufeffor more information, have a look at the website .","title":"Using Frog from Python"},{"location":"tools/frog/#mentions","text":"","title":"Mentions"},{"location":"tools/frog/#articles-incl-conference-papers-presentations-and-demos","text":"","title":"Articles (incl. conference papers, presentations and demo\u2019s)"},{"location":"tools/frog/#projects","text":"","title":"Projects"},{"location":"tools/frog/#teaching-and-instruction","text":"","title":"Teaching and Instruction"},{"location":"tools/gretel/","text":"GrETEL GrETEL is a user-friendly query engine in which linguists can use an utterance as a starting point for searching a treebank for a linguistic phenomenon. Instead of a formal, technical query instruction, it takes a natural language example as input. O\ufeffverview GrETEL stands for Greedy Extraction of Trees for Empirical Linguistics. GrETEL is a user-friendly query engine in which linguists can use an utterance as a starting point for searching a treebank for a linguistic phenomenon. Instead of a formal, technical query instruction, it takes a natural language example as input. This provides a convenient way for novice and non-technical users to use treebanks with a limited knowledge of the underlying syntax and formal query languages. By allowing linguists to search for constructions similar to the example they provide, GrETEL aims to bridge the gap between descriptive-theoretical and computational linguistics. The example-based query procedure consists of multiple steps: 1) The user enters an example of the construction they are interested in. 2) The example is returned in the form of a matrix, in which the user specifies which aspects of this example are essential for the construction under investigation. This matrix is automatically converted in an XPath query, which can be manually edited or even written by the user. 3) The user chooses the corpus on which the query must be executed. 4) The query is executed on the selected corpus, and the matching constructions are presented to the user as a list of sentences, which can be downloaded. The user can also click on the sentences in order to visualize the results as syntax trees, in which the matching part of the tree is highlighted. 5) The results of the query can then be efficiently analysed through the use of a pivot table. GrETEL enables search in the LASSY-SMALL and the CGN (Spoken Dutch Corpus) Treebanks (1 million tokens each), among other corpora. GrETEL also allows users to upload their own corpus. The corpus is then automatically parsed with the Alpino parser, and can then be used for querying. L\ufeffearn Instructions and support Instructions \\ Most recently, a complete tutorial on GrETEL was given on Oct 7, 2022 by Jan Odijk (in Dutch). This tutorial was taped and included slides and exercises, which can be found here . The GrETEL webpage offers a wide range of information, including tutorials, documentation and FAQ. Slides Exercises Manuals and documentation FAQ Contact the developers Local installation \\ It is possible to install GrETEL locally. GrETEL\u2019s GitHub repository has detailed information on how to install GrETEL locally, as well as notes for users and developers. Contact the developers \\ The current version of GrETEL is developed by the Digital Humanities Lab at Utrecht University. If you have any suggestions, questions, or general feedback you are welcome to give us a ring, or send us an email. You can find contact information on Digital Humanities Lab's website or in the footer of the GrETEL webpage . M\ufeffentions Key publications: Jan Odijk, Martijn van der Klis and Sheean Spoel (2018). \u201cExtensions to the GrETEL treebank query application.\u201d In: Proceedings of the 16th International Workshop on Treebanks and Linguistic Theories. Prague, Czech Republic. pp. 46-55. Liesbeth Augustinus, Vincent Vandeghinste and Frank Van Eynde (2012). \u201cExample-based treebank querying.\u201d In: Proceedings of the 8th Conference on Language Resources and Evaluation (LREC 2012). Istanbul, Turkey. pp. 3161-3167. Source code: https://github.com/UUDigitalHumanitieslab/gretel Previous versions: http://gretel.ccl.kuleuven.be/gretel3/ http://gretel.ccl.kuleuven.be/gretel-2.0/ebs/input.php More publications and talks: http://gretel.ccl.kuleuven.be/project/publications.php Poly-GrETEL: An extension of GrETEL to be used in parallel corpora: http://gretel.ccl.kuleuven.be/poly-gretel/ GrETEL for Afrikaans: http://gretel.ccl.kuleuven.be/afribooms/","title":"GrETEL"},{"location":"tools/gretel/#gretel","text":"GrETEL is a user-friendly query engine in which linguists can use an utterance as a starting point for searching a treebank for a linguistic phenomenon. Instead of a formal, technical query instruction, it takes a natural language example as input.","title":"GrETEL"},{"location":"tools/gretel/#overview","text":"GrETEL stands for Greedy Extraction of Trees for Empirical Linguistics. GrETEL is a user-friendly query engine in which linguists can use an utterance as a starting point for searching a treebank for a linguistic phenomenon. Instead of a formal, technical query instruction, it takes a natural language example as input. This provides a convenient way for novice and non-technical users to use treebanks with a limited knowledge of the underlying syntax and formal query languages. By allowing linguists to search for constructions similar to the example they provide, GrETEL aims to bridge the gap between descriptive-theoretical and computational linguistics. The example-based query procedure consists of multiple steps: 1) The user enters an example of the construction they are interested in. 2) The example is returned in the form of a matrix, in which the user specifies which aspects of this example are essential for the construction under investigation. This matrix is automatically converted in an XPath query, which can be manually edited or even written by the user. 3) The user chooses the corpus on which the query must be executed. 4) The query is executed on the selected corpus, and the matching constructions are presented to the user as a list of sentences, which can be downloaded. The user can also click on the sentences in order to visualize the results as syntax trees, in which the matching part of the tree is highlighted. 5) The results of the query can then be efficiently analysed through the use of a pivot table. GrETEL enables search in the LASSY-SMALL and the CGN (Spoken Dutch Corpus) Treebanks (1 million tokens each), among other corpora. GrETEL also allows users to upload their own corpus. The corpus is then automatically parsed with the Alpino parser, and can then be used for querying.","title":"O\ufeffverview"},{"location":"tools/gretel/#learn","text":"","title":"L\ufeffearn"},{"location":"tools/gretel/#instructions-and-support","text":"Instructions \\ Most recently, a complete tutorial on GrETEL was given on Oct 7, 2022 by Jan Odijk (in Dutch). This tutorial was taped and included slides and exercises, which can be found here . The GrETEL webpage offers a wide range of information, including tutorials, documentation and FAQ. Slides Exercises Manuals and documentation FAQ Contact the developers Local installation \\ It is possible to install GrETEL locally. GrETEL\u2019s GitHub repository has detailed information on how to install GrETEL locally, as well as notes for users and developers. Contact the developers \\ The current version of GrETEL is developed by the Digital Humanities Lab at Utrecht University. If you have any suggestions, questions, or general feedback you are welcome to give us a ring, or send us an email. You can find contact information on Digital Humanities Lab's website or in the footer of the GrETEL webpage .","title":"Instructions and support"},{"location":"tools/gretel/#mentions","text":"","title":"M\ufeffentions"},{"location":"tools/gretel/#key-publications","text":"Jan Odijk, Martijn van der Klis and Sheean Spoel (2018). \u201cExtensions to the GrETEL treebank query application.\u201d In: Proceedings of the 16th International Workshop on Treebanks and Linguistic Theories. Prague, Czech Republic. pp. 46-55. Liesbeth Augustinus, Vincent Vandeghinste and Frank Van Eynde (2012). \u201cExample-based treebank querying.\u201d In: Proceedings of the 8th Conference on Language Resources and Evaluation (LREC 2012). Istanbul, Turkey. pp. 3161-3167. Source code: https://github.com/UUDigitalHumanitieslab/gretel Previous versions: http://gretel.ccl.kuleuven.be/gretel3/ http://gretel.ccl.kuleuven.be/gretel-2.0/ebs/input.php More publications and talks: http://gretel.ccl.kuleuven.be/project/publications.php Poly-GrETEL: An extension of GrETEL to be used in parallel corpora: http://gretel.ccl.kuleuven.be/poly-gretel/ GrETEL for Afrikaans: http://gretel.ccl.kuleuven.be/afribooms/","title":"Key publications:"},{"location":"tools/grlc/","text":"grlc grlc makes all your Linked Data accessible to the Web by automatically converting your SPARQL queries into RESTful APIs. Overview grlc is a lightweight server that takes SPARQL queries (stored in a GitHub repository, in your local filesystem, or listed in a URL), and translates them to Linked Data Web APIs. This enables universal access to Linked Data. Users are not required to know SPARQL to query their data, but instead can access a web API. grlc assumes that you have a collection of SPARQL queries as .rq files. grlc will create one API operation for each SPARQL query/.rq file in the collection. Your queries can add API parameters to each operation by using the parameter mapping syntax. This allows your query to define query variables which will be mapped to API parameters for your API operation (see here for an example). Your queries can include special decorators to add extra functionality to your API. Learn Instruction webpages The Quick Tutorial is a quick walkthrough for deploying your own Linked Data API using grlc. Mentions Articles (incl. conference papers, presentations and demo\u2019s) Albert Mero\u00f1o-Pe\u00f1uela, Rinke Hoekstra. \u201cgrlc Makes GitHub Taste Like Linked Data APIs\u201d. The Semantic Web \u2013 ESWC 2016 Satellite Events, Heraklion, Crete, Greece, May 29 \u2013 June 2, 2016, Revised Selected Papers. LNCS 9989, pp. 342-353 (2016). (PDF) Albert Mero\u00f1o-Pe\u00f1uela, Rinke Hoekstra. \u201cSPARQL2Git: Transparent SPARQL and Linked Data API Curation via Git\u201d. In: Proceedings of the 14th Extended Semantic Web Conference (ESWC 2017), Poster and Demo Track. Portoroz, Slovenia, May 28th \u2013 June 1st, 2017 (2017). (PDF) Albert Mero\u00f1o-Pe\u00f1uela, Rinke Hoekstra. \u201cAutomatic Query-centric API for Routine Access to Linked Data\u201d. In: The Semantic Web \u2013 ISWC 2017, 16th International Semantic Web Conference. Lecture Notes in Computer Science, vol 10587, pp. 334-339 (2017). (PDF) Pasquale Lisena, Albert Mero\u00f1o-Pe\u00f1uela, Tobias Kuhn, Rapha\u00ebl Troncy. \u201cEasy Web API Development with SPARQL Transformer\u201d. In: The Semantic Web \u2013 ISWC 2019, 18th International Semantic Web Conference. Lecture Notes in Computer Science, vol 11779, pp. 454-470 (2019). (PDF) Projects Teaching and Instruction","title":"grlc"},{"location":"tools/grlc/#grlc","text":"grlc makes all your Linked Data accessible to the Web by automatically converting your SPARQL queries into RESTful APIs.","title":"grlc"},{"location":"tools/grlc/#overview","text":"grlc is a lightweight server that takes SPARQL queries (stored in a GitHub repository, in your local filesystem, or listed in a URL), and translates them to Linked Data Web APIs. This enables universal access to Linked Data. Users are not required to know SPARQL to query their data, but instead can access a web API. grlc assumes that you have a collection of SPARQL queries as .rq files. grlc will create one API operation for each SPARQL query/.rq file in the collection. Your queries can add API parameters to each operation by using the parameter mapping syntax. This allows your query to define query variables which will be mapped to API parameters for your API operation (see here for an example). Your queries can include special decorators to add extra functionality to your API.","title":"Overview"},{"location":"tools/grlc/#learn","text":"","title":"Learn"},{"location":"tools/grlc/#instruction-webpages","text":"The Quick Tutorial is a quick walkthrough for deploying your own Linked Data API using grlc.","title":"Instruction webpages"},{"location":"tools/grlc/#mentions","text":"","title":"Mentions"},{"location":"tools/grlc/#articles-incl-conference-papers-presentations-and-demos","text":"Albert Mero\u00f1o-Pe\u00f1uela, Rinke Hoekstra. \u201cgrlc Makes GitHub Taste Like Linked Data APIs\u201d. The Semantic Web \u2013 ESWC 2016 Satellite Events, Heraklion, Crete, Greece, May 29 \u2013 June 2, 2016, Revised Selected Papers. LNCS 9989, pp. 342-353 (2016). (PDF) Albert Mero\u00f1o-Pe\u00f1uela, Rinke Hoekstra. \u201cSPARQL2Git: Transparent SPARQL and Linked Data API Curation via Git\u201d. In: Proceedings of the 14th Extended Semantic Web Conference (ESWC 2017), Poster and Demo Track. Portoroz, Slovenia, May 28th \u2013 June 1st, 2017 (2017). (PDF) Albert Mero\u00f1o-Pe\u00f1uela, Rinke Hoekstra. \u201cAutomatic Query-centric API for Routine Access to Linked Data\u201d. In: The Semantic Web \u2013 ISWC 2017, 16th International Semantic Web Conference. Lecture Notes in Computer Science, vol 10587, pp. 334-339 (2017). (PDF) Pasquale Lisena, Albert Mero\u00f1o-Pe\u00f1uela, Tobias Kuhn, Rapha\u00ebl Troncy. \u201cEasy Web API Development with SPARQL Transformer\u201d. In: The Semantic Web \u2013 ISWC 2019, 18th International Semantic Web Conference. Lecture Notes in Computer Science, vol 11779, pp. 454-470 (2019). (PDF)","title":"Articles (incl. conference papers, presentations and demo\u2019s)"},{"location":"tools/grlc/#projects","text":"","title":"Projects"},{"location":"tools/grlc/#teaching-and-instruction","text":"","title":"Teaching and Instruction"},{"location":"tools/hypodisc/","text":"H\ufeffypoDisc This tool aims to discover hypotheses in RDF knowledge graphs by: 1) Training a multi-hop link prediction model on a graph subset, encouraging the clustering of similar context entities. 2) Using cluster centroids as representatives, ranking paths within clusters: top-k paths become the hypotheses. O\ufeffverview The goal of discovering hypotheses is redefined as a multi-hop link prediction and embedding clustering problem. First, a multi-hop link prediction model is trained on a subset of the graph, while simultaneously encouraging the embeddings of entities whose context is similar to cluster together in the embedding space. The second step takes the centroids of the clusters, and uses them as representatives of these clusters with which to rank the set of paths that exist in the cluster, thereby swapping the original embedding by the new representative. The top-k paths form the hypotheses for that cluster. L\ufeffearn Multimodal learning This pipeline includes experimental support for multimodal features, stored as literal nodes. These are disabled by default, but can be enabled by specifying one or more of the following modalities: numerical, temporal, textual, spatial, and visual. These modalities map to their corresponding XSD datatypes, or b64Image for images encoded as string literals. Note that, for literals to be considered a member of a modality they should correctly be annotated with a datatype or language tag. Find out more about how to create and run a HDF5 file (which in includes text and images) on the github . Generating Clusters t-SNE can be used to visualize the clusters in the entity embedding space. You can enable this feature, by installing t-SNE as a submodule of this repository. Detailed information on how to use the tool can be found on the github . The README.md on the tool's github contains more information.","title":"HypoDisc"},{"location":"tools/hypodisc/#hypodisc","text":"This tool aims to discover hypotheses in RDF knowledge graphs by: 1) Training a multi-hop link prediction model on a graph subset, encouraging the clustering of similar context entities. 2) Using cluster centroids as representatives, ranking paths within clusters: top-k paths become the hypotheses.","title":"H\ufeffypoDisc"},{"location":"tools/hypodisc/#overview","text":"The goal of discovering hypotheses is redefined as a multi-hop link prediction and embedding clustering problem. First, a multi-hop link prediction model is trained on a subset of the graph, while simultaneously encouraging the embeddings of entities whose context is similar to cluster together in the embedding space. The second step takes the centroids of the clusters, and uses them as representatives of these clusters with which to rank the set of paths that exist in the cluster, thereby swapping the original embedding by the new representative. The top-k paths form the hypotheses for that cluster.","title":"O\ufeffverview"},{"location":"tools/hypodisc/#learn","text":"","title":"L\ufeffearn"},{"location":"tools/hypodisc/#multimodal-learning","text":"This pipeline includes experimental support for multimodal features, stored as literal nodes. These are disabled by default, but can be enabled by specifying one or more of the following modalities: numerical, temporal, textual, spatial, and visual. These modalities map to their corresponding XSD datatypes, or b64Image for images encoded as string literals. Note that, for literals to be considered a member of a modality they should correctly be annotated with a datatype or language tag. Find out more about how to create and run a HDF5 file (which in includes text and images) on the github .","title":"Multimodal learning"},{"location":"tools/hypodisc/#generating-clusters","text":"t-SNE can be used to visualize the clusters in the entity embedding space. You can enable this feature, by installing t-SNE as a submodule of this repository. Detailed information on how to use the tool can be found on the github . The README.md on the tool's github contains more information.","title":"Generating Clusters"},{"location":"tools/mediasuite/","text":"Media Suite The CLARIAH Media Suite is a research environment of the Dutch infrastructure for digital humanities and social science. It facilitates scholarly research with large Dutch media collections by providing advanced search and analysis tools. Overview The CLARIAH Media Suite is an application for doing research with data collections by scholars and students at universities and in higher education (e.g., film, television, and other media scholars, oral historians, and political historians). It consists of three building blocks: data, tools to work with the data, and a workspace to store your work with the data. The Media Suite is an innovative digital research environment, an experimental environment (LAB) , in which we are experimenting with new ways of working with multimedia data collections. It caters to various levels of expertise and research interests: from providing access to many audio-visual collections for exploratory research to close reading; and from more complex modes of data analysis to distant reading strategies. The transparent search and analysis tools that the Media Suite offers, combined with its APIs that can be used with Jupyter notebooks, allow for many new possibilities for research and represents the middle ground between full algorithmic literacy and being a data novice. Users from Dutch universities and research institutes can log into the Media Suite using their university credentials. Read more about access to the Media Suite . Data Via the CLARIAH infrastructure, the Media Suite provides access to data collections in Dutch archives (among others: The Netherlands Institute for Sound and Vision, EYE Film Museum collections, DANS oral history interview collections, collections from the Open Images Project). Typically, data collections are registered in a registry that allows the infastructure to either access collections directly or use some form of data harvesting to enables access. More about our data: What collections/data are available via the Media Suite? > How does the Media Suite make the data available? > Can I play/view all the sources that I find via the Media Suite? > Tools As a research environment, the CLARIAH Media Suite aims to support scholars in all the steps of their research process. At a general level, it provides tools for exploring the data and collections, creating personal selections (or corpora), adding annotations (such as tags, comments, links, and other metadata), and the possibility to export them. The Media Suite also facilitates working with data directly by using its APIs in combination with Jupyter Notebooks. Workspace Workspace The CLARIAH Media Suite offers a \u201cvirtual work space\u201d to its users. It allows researchers to store bookmarks, annotations, saved queries, personal collections, or automatic enrichments. The workspace thus provides researchers with novel ways for making transparent and managing their research process. Read more about the Media Suite > Learn Instruction and support The Media Suite Learn pages offer a wide set instruction and support material: Quick start guide Written, video and hybrid tutorials for different levels of user expertise, beginner\u2019s to advanced levels, based on subject or tool: Subject tutorials , Tool tutorials Frequently Asked Questions Glossary of terms Overview of example projects that have used Media Suite collections and tools in either teaching or research. Media Suite\u2019s Group Library in Zotero. Advice and user support Media Suite Learn team aims to support a wide array of approaches and areas of teaching and research, and are happy to offer advice on how to use the Media Suite productively in your own project. You can contact the team via mediastudies@clariah.nl . The team regularly contributes to organising research events that introduce the Media Suite to new users, and that reflect on digital methods and videographic approaches more broadly. Events include online webinars and public research seminars focussing on state-of-the-art digital scholarship, with contributions from scholars from the Netherlands and abroad. Public forum The Media Suite Public Forum uses Gitter , an open source instant messaging and chat room system. To start chatting, you would need to have either a Twitter or Github account. Media Suite related questions & answers Bug reports Discussions on new features and future directions Mentions Publications Aasman, S., Melgar Estrada, L., Slootweg, T. & Wegter, R., (2019). Tales of a Tool Encounter: Exploring Video Annotation for Doing Media History, VIEW Journal of European Television History and Culture, Special Issue on Audiovisual Data in Digital Humanities, eds. Pelle Snickars, Mark Williams and Andreas Fickers, Spring 2019. Open access, online multi-media article. Ashkpour, A., Merono-Penuela, A., & Mandemakers, K. (2015). The Aggregate Dutch Historical Censuses: Harmonization and RDF. Historical Methods: A Journal of Quantitative and Interdisciplinary History, 48(4), 230-245. (PDF) Bilgin, A., Sang, E.T.K., Smeenk, K., Hollink, L., van Ossenbruggen, J., Harbers, F. and Broersma, M. (2018). Utilizing a Transparency-driven Environment toward Trusted Automatic Genre Classification: A Case Study in Journalism History. Proceedings 14th International Conference on e-Science (e-Science) (pp. 486-496). IEEE. Bloothooft, G., Oosterlaken, R., Reynaert, M., Depuydt, K., Schoonheim, T. (2018). NAMES: Towards gold standards for personal names. DHBenelux conference 2018. Broersma, M., & Harbers, F. Eds. (2019). Dossier CLARIAH Media projects. Tijdschrift voor Mediageschiedenis/Journal for Media History. See all publications Presentations Erp, M. van. A philosophy of change, Institute for Social Work, Utrecht, 12 december 2018 Dijk, J. van. Towards Semantic enrichment of Newspapers: A Historical Ecology use case, Amsterdam, 28 december 2017 See all presentations Webpages \u2018Big Heritage Data\u2019 in Media Suite, Beeld en geluid Media Suite makes \u2018Big Heritage Data\u2019 accessible for research, DANS Video\u2019s Talkshow Mediasuite // Data Stories, Dutch Media Week","title":"Media Suite"},{"location":"tools/mediasuite/#media-suite","text":"The CLARIAH Media Suite is a research environment of the Dutch infrastructure for digital humanities and social science. It facilitates scholarly research with large Dutch media collections by providing advanced search and analysis tools.","title":"Media Suite"},{"location":"tools/mediasuite/#overview","text":"The CLARIAH Media Suite is an application for doing research with data collections by scholars and students at universities and in higher education (e.g., film, television, and other media scholars, oral historians, and political historians). It consists of three building blocks: data, tools to work with the data, and a workspace to store your work with the data. The Media Suite is an innovative digital research environment, an experimental environment (LAB) , in which we are experimenting with new ways of working with multimedia data collections. It caters to various levels of expertise and research interests: from providing access to many audio-visual collections for exploratory research to close reading; and from more complex modes of data analysis to distant reading strategies. The transparent search and analysis tools that the Media Suite offers, combined with its APIs that can be used with Jupyter notebooks, allow for many new possibilities for research and represents the middle ground between full algorithmic literacy and being a data novice. Users from Dutch universities and research institutes can log into the Media Suite using their university credentials. Read more about access to the Media Suite .","title":"Overview"},{"location":"tools/mediasuite/#data","text":"Via the CLARIAH infrastructure, the Media Suite provides access to data collections in Dutch archives (among others: The Netherlands Institute for Sound and Vision, EYE Film Museum collections, DANS oral history interview collections, collections from the Open Images Project). Typically, data collections are registered in a registry that allows the infastructure to either access collections directly or use some form of data harvesting to enables access. More about our data: What collections/data are available via the Media Suite? > How does the Media Suite make the data available? > Can I play/view all the sources that I find via the Media Suite? >","title":"Data"},{"location":"tools/mediasuite/#tools","text":"As a research environment, the CLARIAH Media Suite aims to support scholars in all the steps of their research process. At a general level, it provides tools for exploring the data and collections, creating personal selections (or corpora), adding annotations (such as tags, comments, links, and other metadata), and the possibility to export them. The Media Suite also facilitates working with data directly by using its APIs in combination with Jupyter Notebooks. Workspace","title":"Tools"},{"location":"tools/mediasuite/#workspace","text":"The CLARIAH Media Suite offers a \u201cvirtual work space\u201d to its users. It allows researchers to store bookmarks, annotations, saved queries, personal collections, or automatic enrichments. The workspace thus provides researchers with novel ways for making transparent and managing their research process. Read more about the Media Suite >","title":"Workspace"},{"location":"tools/mediasuite/#learn","text":"","title":"Learn"},{"location":"tools/mediasuite/#instruction-and-support","text":"The Media Suite Learn pages offer a wide set instruction and support material: Quick start guide Written, video and hybrid tutorials for different levels of user expertise, beginner\u2019s to advanced levels, based on subject or tool: Subject tutorials , Tool tutorials Frequently Asked Questions Glossary of terms Overview of example projects that have used Media Suite collections and tools in either teaching or research. Media Suite\u2019s Group Library in Zotero.","title":"Instruction and support"},{"location":"tools/mediasuite/#advice-and-user-support","text":"Media Suite Learn team aims to support a wide array of approaches and areas of teaching and research, and are happy to offer advice on how to use the Media Suite productively in your own project. You can contact the team via mediastudies@clariah.nl . The team regularly contributes to organising research events that introduce the Media Suite to new users, and that reflect on digital methods and videographic approaches more broadly. Events include online webinars and public research seminars focussing on state-of-the-art digital scholarship, with contributions from scholars from the Netherlands and abroad.","title":"Advice and user support"},{"location":"tools/mediasuite/#public-forum","text":"The Media Suite Public Forum uses Gitter , an open source instant messaging and chat room system. To start chatting, you would need to have either a Twitter or Github account. Media Suite related questions & answers Bug reports Discussions on new features and future directions","title":"Public forum"},{"location":"tools/mediasuite/#mentions","text":"","title":"Mentions"},{"location":"tools/mediasuite/#publications","text":"Aasman, S., Melgar Estrada, L., Slootweg, T. & Wegter, R., (2019). Tales of a Tool Encounter: Exploring Video Annotation for Doing Media History, VIEW Journal of European Television History and Culture, Special Issue on Audiovisual Data in Digital Humanities, eds. Pelle Snickars, Mark Williams and Andreas Fickers, Spring 2019. Open access, online multi-media article. Ashkpour, A., Merono-Penuela, A., & Mandemakers, K. (2015). The Aggregate Dutch Historical Censuses: Harmonization and RDF. Historical Methods: A Journal of Quantitative and Interdisciplinary History, 48(4), 230-245. (PDF) Bilgin, A., Sang, E.T.K., Smeenk, K., Hollink, L., van Ossenbruggen, J., Harbers, F. and Broersma, M. (2018). Utilizing a Transparency-driven Environment toward Trusted Automatic Genre Classification: A Case Study in Journalism History. Proceedings 14th International Conference on e-Science (e-Science) (pp. 486-496). IEEE. Bloothooft, G., Oosterlaken, R., Reynaert, M., Depuydt, K., Schoonheim, T. (2018). NAMES: Towards gold standards for personal names. DHBenelux conference 2018. Broersma, M., & Harbers, F. Eds. (2019). Dossier CLARIAH Media projects. Tijdschrift voor Mediageschiedenis/Journal for Media History. See all publications","title":"Publications"},{"location":"tools/mediasuite/#presentations","text":"Erp, M. van. A philosophy of change, Institute for Social Work, Utrecht, 12 december 2018 Dijk, J. van. Towards Semantic enrichment of Newspapers: A Historical Ecology use case, Amsterdam, 28 december 2017 See all presentations","title":"Presentations"},{"location":"tools/mediasuite/#webpages","text":"\u2018Big Heritage Data\u2019 in Media Suite, Beeld en geluid Media Suite makes \u2018Big Heritage Data\u2019 accessible for research, DANS","title":"Webpages"},{"location":"tools/mediasuite/#videos","text":"Talkshow Mediasuite // Data Stories, Dutch Media Week","title":"Video\u2019s"},{"location":"tools/paqu/","text":"PaQu- Parse and Query The PaQu web service makes it possible to search in syntactically annotated corpora in Dutch. You can parse your own Dutch text corpus or use one of two corpora provided by the developers. O\ufeffverview PaQu uses the Alpino parser to make treebanks of your own text corpus, and to search in these treebanks using an interface based on the LASSY Word Relations Search interface ( http://dev.clarin.nl/node/1966 ). Two treebanks are already available in the application: Lassy Klein (1M words, manually checked syntactic analysis) and Lassy Groot (700M words, syntactic analysis automatically assigned by Alpino). PaQu offers two ways to search through the syntactically annotated texts. The first option is to use the search bar to look for word pairs, optionally complemented by their syntactic relationship. The second search option is to use the query language XPath.","title":"PaQu"},{"location":"tools/paqu/#paqu-parse-and-query","text":"The PaQu web service makes it possible to search in syntactically annotated corpora in Dutch. You can parse your own Dutch text corpus or use one of two corpora provided by the developers.","title":"PaQu- Parse and Query"},{"location":"tools/paqu/#overview","text":"PaQu uses the Alpino parser to make treebanks of your own text corpus, and to search in these treebanks using an interface based on the LASSY Word Relations Search interface ( http://dev.clarin.nl/node/1966 ). Two treebanks are already available in the application: Lassy Klein (1M words, manually checked syntactic analysis) and Lassy Groot (700M words, syntactic analysis automatically assigned by Alpino). PaQu offers two ways to search through the syntactically annotated texts. The first option is to use the search bar to look for word pairs, optionally complemented by their syntactic relationship. The second search option is to use the query language XPath.","title":"O\ufeffverview"},{"location":"tools/stam/","text":"STAM Stand-off Text Annotation Model (STAM) is a data model for stand-off-text annotation where any information on a text is represented as an annotation. Overview STAM is a minimalist data model for stand-off text annotation. Any information on a text is represented an annotation, which can be any kind of remarks, classifications, or tags on specific portions of the text or the entire resource. Annotations can also point to other annotations (higher-order annotations). STAM does not define specific vocabularies and accepts plain text as its base resource. It is independent of complex data models like RDF, W3C Web Annotations, TEI, or FoLiA. STAM aims to be a functional and practical solution, allowing users to use use vocabularies that are formalised elsewhere. STAM is primarily intended as a model for data representation, and less so as a format for data interchange. It is designed in such as way that an efficient implementation (both speed & memory) is feasible. Goals/characteristics of STAM are: Simplicity - the data model must be easy to understand for a user/developer to use and only contain what is needed, not more. We provide a minimal foundation upon which other projects can build more complex solutions. These are deliberately kept out of STAM itself. The notion that everything is an annotation is at the core of STAM and one of the things that keeps it simple. Separation from semantics - The data model does not commit to any vocabulary or annotation paradigm. It must be flexible enough to express whatever annotation paradigm a researcher wants to use, yet provide the facilities to be specific enough for practical purposes. The model basically allows for any kind of directed or undirected graph. Standalone - No dependency on other data models (e.g. RDF) aside from Unicode and JSON for serialisation, no dependency on any software services. Practical - Rather than provide a theoretical framework, we primarily aim to provide a practical specification and actual low-level tooling you can get to work with right away. Performant - The data model is set up in such a way that it allows for efficient/performant implementations, with regard to processing requirements but especially memory consumption. The model should be suitable for big data (millions of annotations). We sit at a point where we deem to have an optimal trade-off between simplicity and performance. Import & Export - Reads/writes a simple JSON format. But also designed with export to more complex formats in mind (such as W3C Web Annotations / RDF) and imports from common formats such as TSV and CONLL. Note that although STAM puts no constraints on annotation paradigms and vocabularies, higher data models may. The name STAM, an acronym for \" Stand-off Text Annotation Model \", is Dutch, Swedish, Afrikaans and Frisian for \" trunk \" (as in the trunk of a tree), the name itself depicts a solid foundation upon which more elaborate solutions can be built. If you want to learn more, please have a look at the specification on project's github page and at the implementations mentioned below: Implementations There are currently two implementations for STAM: stam-rust - A STAM library written in Rust, aims to be a full STAM implementation with high performance and memory-based storage model. stam-python - A STAM library for Python. This is not an independent implementation but it is a Python binding to the above Rust library. Furthermore, there is also the following implementation that builds upon the primary STAM library: stam-tools - A set of command-line tools to work with STAM Learn STAM Specification The STAM specification lays out the data model of STAM in formal terms, and is a complete source to understand what STAM is all about: STAM Specification Python Tutorial: Standoff Text Annotation for Pythonistas To get hands-on experience with STAM from Python, please consult this tutorial, which comes in the form of a Jupyter Notebook you can run interactively: STAM Tutorial: Standoff Text Annotation for Pythonistas The full Python API is documented here: API Reference Rust library The core library for STAM is implemented in Rust. It is also used by the Python binding. Advanced programmers may also use it directly to build efficient applications that deal with stand-off annotation on text: stam-rust: STAM library for Rust API Reference Extensions STAM is kept simple and only the bare minimum is defined. Other functionality is included in extensions. Extensions do one or more of the following: they extend the data model, specify new serialisations, specify mappings/crosswalks to other paradigms/formats, specify additional functionality. The following are currently defined: STAM-Vocab - Allows expressing and validating against user-defined vocabularies. STAM-Webannotations - Models W3C Web Annotations using STAM and vice versa. STAM-Textvalidation - Adds an extra redundancy layer that helps protecting data integrity and aids readability of serialisations STAM-CSV - Defines an alternative serialisation format using CSV. STAM-Baseoffset - allows splitting large monolithic text resources into multiple smaller text resources, whilst still retaining the ability the reference offsets as if they refer to the original/monolithic resource. Implementations SHOULD explicitly state which extensions they support. For more information, have a look at the README .","title":"STAM"},{"location":"tools/stam/#stam","text":"Stand-off Text Annotation Model (STAM) is a data model for stand-off-text annotation where any information on a text is represented as an annotation.","title":"STAM"},{"location":"tools/stam/#overview","text":"STAM is a minimalist data model for stand-off text annotation. Any information on a text is represented an annotation, which can be any kind of remarks, classifications, or tags on specific portions of the text or the entire resource. Annotations can also point to other annotations (higher-order annotations). STAM does not define specific vocabularies and accepts plain text as its base resource. It is independent of complex data models like RDF, W3C Web Annotations, TEI, or FoLiA. STAM aims to be a functional and practical solution, allowing users to use use vocabularies that are formalised elsewhere. STAM is primarily intended as a model for data representation, and less so as a format for data interchange. It is designed in such as way that an efficient implementation (both speed & memory) is feasible.","title":"Overview"},{"location":"tools/stam/#goalscharacteristics-of-stam-are","text":"Simplicity - the data model must be easy to understand for a user/developer to use and only contain what is needed, not more. We provide a minimal foundation upon which other projects can build more complex solutions. These are deliberately kept out of STAM itself. The notion that everything is an annotation is at the core of STAM and one of the things that keeps it simple. Separation from semantics - The data model does not commit to any vocabulary or annotation paradigm. It must be flexible enough to express whatever annotation paradigm a researcher wants to use, yet provide the facilities to be specific enough for practical purposes. The model basically allows for any kind of directed or undirected graph. Standalone - No dependency on other data models (e.g. RDF) aside from Unicode and JSON for serialisation, no dependency on any software services. Practical - Rather than provide a theoretical framework, we primarily aim to provide a practical specification and actual low-level tooling you can get to work with right away. Performant - The data model is set up in such a way that it allows for efficient/performant implementations, with regard to processing requirements but especially memory consumption. The model should be suitable for big data (millions of annotations). We sit at a point where we deem to have an optimal trade-off between simplicity and performance. Import & Export - Reads/writes a simple JSON format. But also designed with export to more complex formats in mind (such as W3C Web Annotations / RDF) and imports from common formats such as TSV and CONLL. Note that although STAM puts no constraints on annotation paradigms and vocabularies, higher data models may. The name STAM, an acronym for \" Stand-off Text Annotation Model \", is Dutch, Swedish, Afrikaans and Frisian for \" trunk \" (as in the trunk of a tree), the name itself depicts a solid foundation upon which more elaborate solutions can be built. If you want to learn more, please have a look at the specification on project's github page and at the implementations mentioned below:","title":"Goals/characteristics of STAM are:"},{"location":"tools/stam/#implementations","text":"There are currently two implementations for STAM: stam-rust - A STAM library written in Rust, aims to be a full STAM implementation with high performance and memory-based storage model. stam-python - A STAM library for Python. This is not an independent implementation but it is a Python binding to the above Rust library. Furthermore, there is also the following implementation that builds upon the primary STAM library: stam-tools - A set of command-line tools to work with STAM","title":"Implementations"},{"location":"tools/stam/#learn","text":"","title":"Learn"},{"location":"tools/stam/#stam-specification","text":"The STAM specification lays out the data model of STAM in formal terms, and is a complete source to understand what STAM is all about: STAM Specification","title":"STAM Specification"},{"location":"tools/stam/#python-tutorial-standoff-text-annotation-for-pythonistas","text":"To get hands-on experience with STAM from Python, please consult this tutorial, which comes in the form of a Jupyter Notebook you can run interactively: STAM Tutorial: Standoff Text Annotation for Pythonistas The full Python API is documented here: API Reference","title":"Python Tutorial: Standoff Text Annotation for Pythonistas"},{"location":"tools/stam/#rust-library","text":"The core library for STAM is implemented in Rust. It is also used by the Python binding. Advanced programmers may also use it directly to build efficient applications that deal with stand-off annotation on text: stam-rust: STAM library for Rust API Reference","title":"Rust library"},{"location":"tools/stam/#extensions","text":"STAM is kept simple and only the bare minimum is defined. Other functionality is included in extensions. Extensions do one or more of the following: they extend the data model, specify new serialisations, specify mappings/crosswalks to other paradigms/formats, specify additional functionality. The following are currently defined: STAM-Vocab - Allows expressing and validating against user-defined vocabularies. STAM-Webannotations - Models W3C Web Annotations using STAM and vice versa. STAM-Textvalidation - Adds an extra redundancy layer that helps protecting data integrity and aids readability of serialisations STAM-CSV - Defines an alternative serialisation format using CSV. STAM-Baseoffset - allows splitting large monolithic text resources into multiple smaller text resources, whilst still retaining the ability the reference offsets as if they refer to the original/monolithic resource. Implementations SHOULD explicitly state which extensions they support. For more information, have a look at the README .","title":"Extensions"}]}