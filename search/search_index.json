{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Ineo Content Introduction This repository stores the rich content for the Ineo Resource finder. We denote the various curated texts pertaining to resources as 'rich content'. A select team of CLARIAH's communication department is working on these texts. This repository enables easy collaboration, also with developers and data providers, and provides a sustainable storage solution. Rich content can be contrasted to the automatically harvested metadata that results from the CLARIAH Tool Discovery track or the FAIR Datasets track and is not supposed to overlap. For automatically harvested metadata, the primary authorship lies with the tool/data producer. For rich user content, it lies with the people of the communication department (WP1). The rich content in this repository is stored in Markdown format , a simple plain text format for text markup. We follow a specific structure to the data can be easily ingested into Ineo, as well as any other systems who want to make use of it. The content can be previewed directly on https://clariah.github.io/ineo-content . We used a simple static site generator ( MkDocs ) to achieve this. That site, however, is not intended for end-users to be used directly. On the preview site we also provide access to a simple headless CMS to facilitate working with the Markdown files in the git repository, without requiring real technical knowledge of either. Directory Structure Directories: src/ - Contains Markdown texts tools/ - Contains Markdown texts describing tools. data/ - Contains Markdown texts describing data workflows/ - Contains Markdown texts describing workflows standards/ - Contains Markdown texts describing standards media/ - Stores images or other associated media files, Right now all media files are in this single folder, without subfolders, this limitation is only due to the admin CMS. Images or downloadable documents/spreadsheets/presentations that are referenced from one of the markdown files may be included in this subdirectory as well. You can reference images from the markdown files, just use /media/ as URL prefix when doing so. You can also reference media elsewhere on the web, always sure to use https:// in that case. This should probably be kept to a minimum as there is no guarantee such media will persist in that location over time. Videos are not suitable for direct inclusion in the git repository as they are too big, they need to be hosted elsewhere. Ensure the media files you upload here are suitable for use on the web, pay attention to file-size, format and resolution. Within these directories, each resource is described by a single Markdown file, with optional YAML frontmatter . The title of the markdown file (a level-one heading, e.g. # ) corresponds with the title of the resource as shown in the frontend. The extension is always .md . The paragraph(s) immediately succeeding the level-one heading are taken to be the description, for tools (unless it concerns a tool suite), this generally be omitted as the description should be automatically drawn from the metadata. It is recommended to keep filename to a simple subset of lowercase ASCII characters, without spaces or any punctuation aside from dashes. The markdown file contains sections marked with level two headings ( ## ), each should correspond to agreed-upon tabs shown in the Ineo frontend, we currently distinguish the following: Overview Learn Mentions The content editors and frontend developers can introduce new ones when needed. Any subsections (level-three headings and beyond) can be used freely. YAML frontmatter The markdown files can be enriched with YAML frontmatter to convey some extra information. This is NOT intended for extensive tool/data metadata, as those come directly from different data provisioning pipelines and are not kept in this repository. The only metadata we always duplicate is the title of the resource, which typically also determines the filename (in a 'slug' form in lowercase only and with spaces replaced by hyphens). --- title: The title of the resource --- For tools , we use either the identifier or group field to link to https://tools.clariah.nl . The identifier links to an exact resource, such as frog for https://tools.clariah.nl/frog , i.e. it is identical to what appears in the URL. The other one, group , links to a group or tool suite because the level of granularity offered on https://tools.clariah.nl may be too fine-grained compared to what is desirable for Ineo or other portals. It should be exact name of the group as it appears on the tool discovery overview. The name s of the group can be easily spotted by looking at the table of contents there; the groups are the ones with sub-items. --- identifier: tool-identifier-from-tools-clariah-nl title: Tool --- If both identifier and group are set, the identifier is used to associate metadata of one specific tool in the group/suite with the group as a whole. This allows you to still describe a whole tool suite in Ineo, but picks one of the tools in the suite as being its representative and have its metadata prominently features. Note that neither identifier nor group uniquely identify a markdown file, they may be reused from multiple files and are merely intended to link to https://tools.clariah.nl . It is possible that a Rich Content description is focussed on certain Software-as-a-Service whereas the codemeta is more focussed on the underlying software. In such cases it can happen that the codemeta representation lists multiple services/deployments of that software (expressed via the targetProduct property in the codemeta JSON-LD), whereas in the Rich User Content you may want to only cover a particular one. An example of this is https://tools.clariah.nl/#corpus-frontend . To constrain a rich user content page to a single service, add the following to the YAML frontmatter: --- identifier: tool-identifier-from-tools-clariah-nl title: Service name service: https://some.domain/service --- The URL to the service must exactly match the url property of the targetProduct in the metadata. This service option should be interpreted as a signal to the consolidation pipeline merging rich content and metadata that only this single service should be expressed (and none of the others, if any). Moreover, it indicates that expressing metadata of the service (the matching object under targetProduct ) has precedence over expressing metadata of the source code, in situations where there might otherwise be a tie. The following Ineo-specific metadata can be added: (TODO: add specification of part of Ineo's YAML syntax that are reusable. This is something for the Ineo developers to specify.) Ineo currently allows for a carousel widget showing several images in sequence (see for example https://www.ineo.tools/resources/media-suite ). The data definition for such a widget would go in the YAML frontmatter, I would propose something like: --- carousel: - /media/mediasuite-cover1.png - https://vimeo.com/503507411?embedded=true&source=vimeo_logo&owner=115309374 - /media/mediasuite-cover2.png - /media/mediasuite-cover3.png - /media/mediasuite-cover4.png --- Note these are specifically for Ineo and won't be previewed in the editor. Whether you images interspersed in the flowing text (easier) or the carousel is up to the maintainer team to decide. Note that for data, no way of linking to underlying data registry via identifiers has been defined yet (take this up with Menzo). Template The following template serves as an example for tools: --- identifier: tool-identifier-from-tools-clariah-nl title: Name of the Tool carousel: - /media/tool-screenshot.png --- # Name of the Tool (a short description can go here) ## Overview (all text you want on the overview tab goes here) ## Learn (all text you want on the learn tab goes here) ## Mentions (all text you want on the mentions tab goes here) Editing Content Management System (CMS) A headless CMS ( DecapCMS previously known as NetlifyCMS) is available to make it easier for content editors to do their editing. Within the CMS, both the Markdown content and the YAML frontmatter can be edited in a WYSIWYG fashion without knowledge of either Markdown or YAML. A GitHub account is required to edit. The CMS can be accessed here The CMS is a nice convenience but it also enforced some limitations, it has put the following constraints on this specification: All media files are stored in a single directory /media/ and does not allow further divisions in subdirectories. Online IDE An alternative to using the CMS is to use GitHub's online Visual Studio Code IDE. Each page on https://clariah.github.io/ineo-content has an edit this page link that automatically takes you to the web-based IDE. This offers a bit more freedom than the CMS. Local editing Being a simple git repository with MarkDown files, you are not constrained to either of the above options but can simply clone this git repository locally and work with your text editor of choice, such as vim, emacs, VSCode, Sublime, or something specifically geared for comfortable Markdown editing such as Zettlr . Version Control As a version control system, git will ensure all versions of the content are tracked throughout their history. You can use git tools to revert to previous versions if needed. It also offers an exact provenance trail of who edited what and when, offering maximum transparency. Contributions and Maintainers Anybody can contribute directly to the content by simply doing pull requests. The maintainers of this repository review and accept or decline those pull requests. This procedure enables the maintainers to easily collaborate with tool and data developers. The maintainers, from CLARIAH's communication department, are: Sebastiaan Fluitsma Janessa Vleghert Liselore Tissen","title":"Ineo Content"},{"location":"#ineo-content","text":"","title":"Ineo Content"},{"location":"#introduction","text":"This repository stores the rich content for the Ineo Resource finder. We denote the various curated texts pertaining to resources as 'rich content'. A select team of CLARIAH's communication department is working on these texts. This repository enables easy collaboration, also with developers and data providers, and provides a sustainable storage solution. Rich content can be contrasted to the automatically harvested metadata that results from the CLARIAH Tool Discovery track or the FAIR Datasets track and is not supposed to overlap. For automatically harvested metadata, the primary authorship lies with the tool/data producer. For rich user content, it lies with the people of the communication department (WP1). The rich content in this repository is stored in Markdown format , a simple plain text format for text markup. We follow a specific structure to the data can be easily ingested into Ineo, as well as any other systems who want to make use of it. The content can be previewed directly on https://clariah.github.io/ineo-content . We used a simple static site generator ( MkDocs ) to achieve this. That site, however, is not intended for end-users to be used directly. On the preview site we also provide access to a simple headless CMS to facilitate working with the Markdown files in the git repository, without requiring real technical knowledge of either.","title":"Introduction"},{"location":"#directory-structure","text":"Directories: src/ - Contains Markdown texts tools/ - Contains Markdown texts describing tools. data/ - Contains Markdown texts describing data workflows/ - Contains Markdown texts describing workflows standards/ - Contains Markdown texts describing standards media/ - Stores images or other associated media files, Right now all media files are in this single folder, without subfolders, this limitation is only due to the admin CMS. Images or downloadable documents/spreadsheets/presentations that are referenced from one of the markdown files may be included in this subdirectory as well. You can reference images from the markdown files, just use /media/ as URL prefix when doing so. You can also reference media elsewhere on the web, always sure to use https:// in that case. This should probably be kept to a minimum as there is no guarantee such media will persist in that location over time. Videos are not suitable for direct inclusion in the git repository as they are too big, they need to be hosted elsewhere. Ensure the media files you upload here are suitable for use on the web, pay attention to file-size, format and resolution. Within these directories, each resource is described by a single Markdown file, with optional YAML frontmatter . The title of the markdown file (a level-one heading, e.g. # ) corresponds with the title of the resource as shown in the frontend. The extension is always .md . The paragraph(s) immediately succeeding the level-one heading are taken to be the description, for tools (unless it concerns a tool suite), this generally be omitted as the description should be automatically drawn from the metadata. It is recommended to keep filename to a simple subset of lowercase ASCII characters, without spaces or any punctuation aside from dashes. The markdown file contains sections marked with level two headings ( ## ), each should correspond to agreed-upon tabs shown in the Ineo frontend, we currently distinguish the following: Overview Learn Mentions The content editors and frontend developers can introduce new ones when needed. Any subsections (level-three headings and beyond) can be used freely.","title":"Directory Structure"},{"location":"#yaml-frontmatter","text":"The markdown files can be enriched with YAML frontmatter to convey some extra information. This is NOT intended for extensive tool/data metadata, as those come directly from different data provisioning pipelines and are not kept in this repository. The only metadata we always duplicate is the title of the resource, which typically also determines the filename (in a 'slug' form in lowercase only and with spaces replaced by hyphens). --- title: The title of the resource --- For tools , we use either the identifier or group field to link to https://tools.clariah.nl . The identifier links to an exact resource, such as frog for https://tools.clariah.nl/frog , i.e. it is identical to what appears in the URL. The other one, group , links to a group or tool suite because the level of granularity offered on https://tools.clariah.nl may be too fine-grained compared to what is desirable for Ineo or other portals. It should be exact name of the group as it appears on the tool discovery overview. The name s of the group can be easily spotted by looking at the table of contents there; the groups are the ones with sub-items. --- identifier: tool-identifier-from-tools-clariah-nl title: Tool --- If both identifier and group are set, the identifier is used to associate metadata of one specific tool in the group/suite with the group as a whole. This allows you to still describe a whole tool suite in Ineo, but picks one of the tools in the suite as being its representative and have its metadata prominently features. Note that neither identifier nor group uniquely identify a markdown file, they may be reused from multiple files and are merely intended to link to https://tools.clariah.nl . It is possible that a Rich Content description is focussed on certain Software-as-a-Service whereas the codemeta is more focussed on the underlying software. In such cases it can happen that the codemeta representation lists multiple services/deployments of that software (expressed via the targetProduct property in the codemeta JSON-LD), whereas in the Rich User Content you may want to only cover a particular one. An example of this is https://tools.clariah.nl/#corpus-frontend . To constrain a rich user content page to a single service, add the following to the YAML frontmatter: --- identifier: tool-identifier-from-tools-clariah-nl title: Service name service: https://some.domain/service --- The URL to the service must exactly match the url property of the targetProduct in the metadata. This service option should be interpreted as a signal to the consolidation pipeline merging rich content and metadata that only this single service should be expressed (and none of the others, if any). Moreover, it indicates that expressing metadata of the service (the matching object under targetProduct ) has precedence over expressing metadata of the source code, in situations where there might otherwise be a tie. The following Ineo-specific metadata can be added: (TODO: add specification of part of Ineo's YAML syntax that are reusable. This is something for the Ineo developers to specify.) Ineo currently allows for a carousel widget showing several images in sequence (see for example https://www.ineo.tools/resources/media-suite ). The data definition for such a widget would go in the YAML frontmatter, I would propose something like: --- carousel: - /media/mediasuite-cover1.png - https://vimeo.com/503507411?embedded=true&source=vimeo_logo&owner=115309374 - /media/mediasuite-cover2.png - /media/mediasuite-cover3.png - /media/mediasuite-cover4.png --- Note these are specifically for Ineo and won't be previewed in the editor. Whether you images interspersed in the flowing text (easier) or the carousel is up to the maintainer team to decide. Note that for data, no way of linking to underlying data registry via identifiers has been defined yet (take this up with Menzo).","title":"YAML frontmatter"},{"location":"#template","text":"The following template serves as an example for tools: --- identifier: tool-identifier-from-tools-clariah-nl title: Name of the Tool carousel: - /media/tool-screenshot.png --- # Name of the Tool (a short description can go here) ## Overview (all text you want on the overview tab goes here) ## Learn (all text you want on the learn tab goes here) ## Mentions (all text you want on the mentions tab goes here)","title":"Template"},{"location":"#editing","text":"","title":"Editing"},{"location":"#content-management-system-cms","text":"A headless CMS ( DecapCMS previously known as NetlifyCMS) is available to make it easier for content editors to do their editing. Within the CMS, both the Markdown content and the YAML frontmatter can be edited in a WYSIWYG fashion without knowledge of either Markdown or YAML. A GitHub account is required to edit. The CMS can be accessed here The CMS is a nice convenience but it also enforced some limitations, it has put the following constraints on this specification: All media files are stored in a single directory /media/ and does not allow further divisions in subdirectories.","title":"Content Management System (CMS)"},{"location":"#online-ide","text":"An alternative to using the CMS is to use GitHub's online Visual Studio Code IDE. Each page on https://clariah.github.io/ineo-content has an edit this page link that automatically takes you to the web-based IDE. This offers a bit more freedom than the CMS.","title":"Online IDE"},{"location":"#local-editing","text":"Being a simple git repository with MarkDown files, you are not constrained to either of the above options but can simply clone this git repository locally and work with your text editor of choice, such as vim, emacs, VSCode, Sublime, or something specifically geared for comfortable Markdown editing such as Zettlr .","title":"Local editing"},{"location":"#version-control","text":"As a version control system, git will ensure all versions of the content are tracked throughout their history. You can use git tools to revert to previous versions if needed. It also offers an exact provenance trail of who edited what and when, offering maximum transparency.","title":"Version Control"},{"location":"#contributions-and-maintainers","text":"Anybody can contribute directly to the content by simply doing pull requests. The maintainers of this repository review and accept or decline those pull requests. This procedure enables the maintainers to easily collaborate with tool and data developers. The maintainers, from CLARIAH's communication department, are: Sebastiaan Fluitsma Janessa Vleghert Liselore Tissen","title":"Contributions and Maintainers"},{"location":"tools/alpino/","text":"Alpino Alpino is a dependency parser for Dutch that also analyses sentences in terms of constituents. Apart from a purely syntactic analysis, Alpino also provides part-of-speech tagging, lemmatization and morphological tagging. The output is formatted in XML. Overview Alpino parses, POS-tags, lemmatizes and analyses morphologically any Dutch sentence that it is given. Alpino reliably analyses an input sentence syntactically, yielding a fully annotated syntactic tree with both constituents as well as explicitly labelled syntactic relations. Alpino is a rule-based parser with a statistics-based disambiguation component. Alpino's grammar has been augmented to build structures based on the guidelines of CGN (Corpus of Spoken Dutch) and D-COI . Alpino's output is formatted in XML, allowing it to be queried by formal query languages such as XPath. Tools such as PaQu and GrETEL leverage this feature and use Alpino in the background for querying purposes. Developed by the University of Groningen, Alpino is available as a webservice hosted by the Radboud University Nijmegen, but can also be installed locally . Learn Quick Use Using Alpino is easiest with the webservice , but requires one the log in using an institutional account. Once logged in, the user can create a new project, upload a tokenized or un-tokenized file (or input the text directly as plain text), and have have Alpino parse all input sentences. The output consists of one FoLIA XML file for the entire input, as well as one file per input sentence in standard Alpino annotation. For quick single-sentence parses, one can use the online demo or GrETEL . Neither options require a log-in, and both showcase the parse yielded as a tree for quick inspection. Local Installation Alpino can also be installed locally. For this, we refer to the tool's general User Guide and GitHub page . Some more comments on using Alpino on Windows are necessary, however. Please read Dani\u00ebl de Kok's blog post on this, if one desires to use Alpino on Windows. Annotation Guidelines In the end, the hardest part about using Alpino is understanding its annotations. For a detailed description of the syntactic annotations used by Alpino, one should check the document: Lassy Syntactische Annotatie . For the annotation of parts-of-speech and lemmas, one should check the document: Part of speech tagging en lemmatisering van het D-coi corpus . These documents are, however, only available in Dutch. The following document (in English) may also be useful: Manual for syntactic annotators . Mentions The Lassy corpus was parsed with Alpino. The Lassy Klein subcorpus was manually corrected. Van Noord, Gertjan, Bouma, Gosse, Van Eynde, Frank, De Kok, Dani\u00ebl, Van der Linde, Jelmer, Schuurman, Ineke, Tjong Kim Sang, Erik, & Vandeghinste, Vincent (2013). Large scale syntactic annotation of written Dutch: Lassy. In Peter Spyns, & Jan Odijk (Eds.), Essential speech and language technology for Dutch: Results by the STEVIN programme (pp. 147-164). Springer Berlin, Heidelberg. https://doi.org/10.1007/978-3-642-30910-6 Press release on Alpino (in Dutch) Publications Van Noord, Gertjan. (2006, april 10\u201313). At Last Parsing Is Now Operational. In Actes de la 13\u00e8me conf\u00e9rence sur le Traitement Automatique des Langues Naturelles. Conf\u00e9rences invit\u00e9es (pp. 20\u201342). ATALA, Leuven. https://aclanthology.org/2006.jeptalnrecital-invite.2/ Webpages Alpino home page GitHub page Alpino web demo Alpino User Guide Alpino on Windows PaQu - Parse and Query makes it possible to search in syntactically annotated corpora in Dutch. PaQu uses the Alpino parser to make treebanks of your own text corpus, and to search in these treebanks. GrETEL is a tool to query-by-example corpora and treebanks that were parsed by Alpino. AlpinoGraph is a tool query syntactically annotated corpora as graphs instead of treebanks, allowing for some other flexibilities. SASTA , a tool for the semi-automatic analysis of spontaneous-language fragments of children with an SLI, uses Alpino to analyse the utterances grammatically. Redekundig.nl is a tool for Dutch high-schoolers, that uses Alpino as backend to classify parts of speech and grammatical functions of phrases in sentences (so-called \"taalkundig ontleden\" and \"redekundig ontleden\"). Credits and Contact Information Alpino was developed in the context of the PIONIER Project Algorithms for Linguistic Processing . Alpino was released under the Gnu Lesser General Public License .","title":"Alpino"},{"location":"tools/alpino/#alpino","text":"Alpino is a dependency parser for Dutch that also analyses sentences in terms of constituents. Apart from a purely syntactic analysis, Alpino also provides part-of-speech tagging, lemmatization and morphological tagging. The output is formatted in XML.","title":"Alpino"},{"location":"tools/alpino/#overview","text":"Alpino parses, POS-tags, lemmatizes and analyses morphologically any Dutch sentence that it is given. Alpino reliably analyses an input sentence syntactically, yielding a fully annotated syntactic tree with both constituents as well as explicitly labelled syntactic relations. Alpino is a rule-based parser with a statistics-based disambiguation component. Alpino's grammar has been augmented to build structures based on the guidelines of CGN (Corpus of Spoken Dutch) and D-COI . Alpino's output is formatted in XML, allowing it to be queried by formal query languages such as XPath. Tools such as PaQu and GrETEL leverage this feature and use Alpino in the background for querying purposes. Developed by the University of Groningen, Alpino is available as a webservice hosted by the Radboud University Nijmegen, but can also be installed locally .","title":"Overview"},{"location":"tools/alpino/#learn","text":"","title":"Learn"},{"location":"tools/alpino/#quick-use","text":"Using Alpino is easiest with the webservice , but requires one the log in using an institutional account. Once logged in, the user can create a new project, upload a tokenized or un-tokenized file (or input the text directly as plain text), and have have Alpino parse all input sentences. The output consists of one FoLIA XML file for the entire input, as well as one file per input sentence in standard Alpino annotation. For quick single-sentence parses, one can use the online demo or GrETEL . Neither options require a log-in, and both showcase the parse yielded as a tree for quick inspection.","title":"Quick Use"},{"location":"tools/alpino/#local-installation","text":"Alpino can also be installed locally. For this, we refer to the tool's general User Guide and GitHub page . Some more comments on using Alpino on Windows are necessary, however. Please read Dani\u00ebl de Kok's blog post on this, if one desires to use Alpino on Windows.","title":"Local Installation"},{"location":"tools/alpino/#annotation-guidelines","text":"In the end, the hardest part about using Alpino is understanding its annotations. For a detailed description of the syntactic annotations used by Alpino, one should check the document: Lassy Syntactische Annotatie . For the annotation of parts-of-speech and lemmas, one should check the document: Part of speech tagging en lemmatisering van het D-coi corpus . These documents are, however, only available in Dutch. The following document (in English) may also be useful: Manual for syntactic annotators .","title":"Annotation Guidelines"},{"location":"tools/alpino/#mentions","text":"The Lassy corpus was parsed with Alpino. The Lassy Klein subcorpus was manually corrected. Van Noord, Gertjan, Bouma, Gosse, Van Eynde, Frank, De Kok, Dani\u00ebl, Van der Linde, Jelmer, Schuurman, Ineke, Tjong Kim Sang, Erik, & Vandeghinste, Vincent (2013). Large scale syntactic annotation of written Dutch: Lassy. In Peter Spyns, & Jan Odijk (Eds.), Essential speech and language technology for Dutch: Results by the STEVIN programme (pp. 147-164). Springer Berlin, Heidelberg. https://doi.org/10.1007/978-3-642-30910-6 Press release on Alpino (in Dutch)","title":"Mentions"},{"location":"tools/alpino/#publications","text":"Van Noord, Gertjan. (2006, april 10\u201313). At Last Parsing Is Now Operational. In Actes de la 13\u00e8me conf\u00e9rence sur le Traitement Automatique des Langues Naturelles. Conf\u00e9rences invit\u00e9es (pp. 20\u201342). ATALA, Leuven. https://aclanthology.org/2006.jeptalnrecital-invite.2/","title":"Publications"},{"location":"tools/alpino/#webpages","text":"Alpino home page GitHub page Alpino web demo Alpino User Guide Alpino on Windows PaQu - Parse and Query makes it possible to search in syntactically annotated corpora in Dutch. PaQu uses the Alpino parser to make treebanks of your own text corpus, and to search in these treebanks. GrETEL is a tool to query-by-example corpora and treebanks that were parsed by Alpino. AlpinoGraph is a tool query syntactically annotated corpora as graphs instead of treebanks, allowing for some other flexibilities. SASTA , a tool for the semi-automatic analysis of spontaneous-language fragments of children with an SLI, uses Alpino to analyse the utterances grammatically. Redekundig.nl is a tool for Dutch high-schoolers, that uses Alpino as backend to classify parts of speech and grammatical functions of phrases in sentences (so-called \"taalkundig ontleden\" and \"redekundig ontleden\").","title":"Webpages"},{"location":"tools/alpino/#credits-and-contact-information","text":"Alpino was developed in the context of the PIONIER Project Algorithms for Linguistic Processing . Alpino was released under the Gnu Lesser General Public License .","title":"Credits and Contact Information"},{"location":"tools/alpinograph/","text":"AlpinoGraph AlpinoGraph is a tool to query syntactically annotated corpora. The tool makes use of AgensGraph, which combines database technology (PostgreSQL) and Cypher, the standard query language for graphs. The queries that one can use in AlpinoGraph are thus a mix of SQL and Cypher. AlpinoGraph additionally provides some extra extensions, such as a simple system of macros, and a visualization of results. Overview AlpinoGraph is a tool to query syntactically annotated corpora as graphs instead of trees. This allows for some extra flexibilities. The query language used by AlpinoGraph is a mix of SQL and Cypher. AlpinoGraph makes use of Alpino's annotations and Universal Dependencies . Data AlpinoGraph ships with (treebanks of) the LASSY-SMALL , the CGN (Corpus Gesproken Nederlands) and the Eindhoven corpus , among others. See this page for a full list (in Dutch). Learn Instruction The key publication of AlpinoGraph has a detailed documentation of AlpinoGraph's workings, and how queries are formulated. It is recommended to read this. Additionally, the AlpinoGraph page has some useful examples and explanations that can be found in the menu in the top-left of the page. For a more extensive documentation of AlpinoGraph, users are invited to visit the Help page of AlpinoGraph . These pages are, however, only available in Dutch. Alpino Annotations AlpinoGraph makes use of the annotations and tagging provided by Alpino . For a detailed description of the syntactic annotations used by Alpino, one should check the document: Lassy Syntactische Annotatie . For the annotation of parts-of-speech and lemmas, one should check the document: Part of speech tagging en lemmatisering van het D-coi corpus . These documents are, however, only available in Dutch. The following document (in English) may also be useful: Manual for syntactic annotators . Universal Dependencies AlpinoGraph also makes use of annotations and tagging following Universal Dependencies (UD), a programme that aims at cross-linguistically consistent tagging and dependency parsing. UD is an open community effort with over 500 contributors producing over 200 treebanks in over 100 languages. If you\u2019re new to UD, you should start by reading the first part of the Short Introduction and then browsing the annotation guidelines on the UD website. User support AlpinoGraph was developed at the Center for Language and Cognition of the University of Groningen by Peter Kleiweg. Any issues can be reported on AlpinoGraph's GitHub . Mentions Key publications Peter Kleiweg and Gertjan van Noord. 2020. AlpinoGraph: A Graph-based Search Engine for Flexible and Efficient Treebank Search. In Proceedings of the 19th International Workshop on Treebanks and Linguistic Theories , pages 151\u2013161, D\u00fcsseldorf, Germany. Association for Computational Linguistics. Webpages Main page Online documentation (in Dutch) Source code Alpino Universal Dependencies TODO: alud","title":"AlpinoGraph"},{"location":"tools/alpinograph/#alpinograph","text":"AlpinoGraph is a tool to query syntactically annotated corpora. The tool makes use of AgensGraph, which combines database technology (PostgreSQL) and Cypher, the standard query language for graphs. The queries that one can use in AlpinoGraph are thus a mix of SQL and Cypher. AlpinoGraph additionally provides some extra extensions, such as a simple system of macros, and a visualization of results.","title":"AlpinoGraph"},{"location":"tools/alpinograph/#overview","text":"AlpinoGraph is a tool to query syntactically annotated corpora as graphs instead of trees. This allows for some extra flexibilities. The query language used by AlpinoGraph is a mix of SQL and Cypher. AlpinoGraph makes use of Alpino's annotations and Universal Dependencies .","title":"Overview"},{"location":"tools/alpinograph/#data","text":"AlpinoGraph ships with (treebanks of) the LASSY-SMALL , the CGN (Corpus Gesproken Nederlands) and the Eindhoven corpus , among others. See this page for a full list (in Dutch).","title":"Data"},{"location":"tools/alpinograph/#learn","text":"","title":"Learn"},{"location":"tools/alpinograph/#instruction","text":"The key publication of AlpinoGraph has a detailed documentation of AlpinoGraph's workings, and how queries are formulated. It is recommended to read this. Additionally, the AlpinoGraph page has some useful examples and explanations that can be found in the menu in the top-left of the page. For a more extensive documentation of AlpinoGraph, users are invited to visit the Help page of AlpinoGraph . These pages are, however, only available in Dutch.","title":"Instruction"},{"location":"tools/alpinograph/#alpino-annotations","text":"AlpinoGraph makes use of the annotations and tagging provided by Alpino . For a detailed description of the syntactic annotations used by Alpino, one should check the document: Lassy Syntactische Annotatie . For the annotation of parts-of-speech and lemmas, one should check the document: Part of speech tagging en lemmatisering van het D-coi corpus . These documents are, however, only available in Dutch. The following document (in English) may also be useful: Manual for syntactic annotators .","title":"Alpino Annotations"},{"location":"tools/alpinograph/#universal-dependencies","text":"AlpinoGraph also makes use of annotations and tagging following Universal Dependencies (UD), a programme that aims at cross-linguistically consistent tagging and dependency parsing. UD is an open community effort with over 500 contributors producing over 200 treebanks in over 100 languages. If you\u2019re new to UD, you should start by reading the first part of the Short Introduction and then browsing the annotation guidelines on the UD website.","title":"Universal Dependencies"},{"location":"tools/alpinograph/#user-support","text":"AlpinoGraph was developed at the Center for Language and Cognition of the University of Groningen by Peter Kleiweg. Any issues can be reported on AlpinoGraph's GitHub .","title":"User support"},{"location":"tools/alpinograph/#mentions","text":"","title":"Mentions"},{"location":"tools/alpinograph/#key-publications","text":"Peter Kleiweg and Gertjan van Noord. 2020. AlpinoGraph: A Graph-based Search Engine for Flexible and Efficient Treebank Search. In Proceedings of the 19th International Workshop on Treebanks and Linguistic Theories , pages 151\u2013161, D\u00fcsseldorf, Germany. Association for Computational Linguistics.","title":"Key publications"},{"location":"tools/alpinograph/#webpages","text":"Main page Online documentation (in Dutch) Source code Alpino Universal Dependencies TODO: alud","title":"Webpages"},{"location":"tools/alud/","text":"alud alud is a Go package for deriving Universal Dependencies from Dutch sentences parsed with Alpino . Overview Alpino parses, POS-tags, lemmatizes and analyses morphologically any Dutch sentence that it is given. Alpino reliably analyses an input sentence syntactically, yielding a fully annotated syntactic tree with both constituents as well as explicitly labelled syntactic relations. alud derives Universal Dependencies from Alpino's output (in XML, in the alpino_ds format), and writes it in the CoNLL-U format . Alternatively, output can be embedded into the alpino_ds format, making them available for XPath queries. alud is also capable of inserting given Universal Dependencies into the alpino_ds format. alud was based on a translation of an xquery script, lassy2ud . Universal Dependencies Universal Dependencies (UD) is a programme that aims at cross-linguistically consistent tagging and dependency parsing. UD is an open community effort with over 500 contributors producing over 200 treebanks in over 100 languages. If you\u2019re new to UD, you should start by reading the first part of the Short Introduction and then browsing the annotation guidelines on the UD website. Learn alud is available as a Go package. Up-to-date documentation of the package can be found here . alud's GitHub page provides extra information, such as how to install it. If you want to install alud, we kindly refer you to the up-to-date GitHub page . Mentions Alpino : alud is developed to convert Alpino's output into Universal Dependencies. Universal Dependencies : UD is a programme that aims at cross-linguistically consistent tagging and dependency parsing. UD is an open community effort with over 500 contributors producing over 200 treebanks in over 100 languages. If you\u2019re new to UD, you should start by reading the first part of the Short Introduction and then browsing the annotation guidelines on the UD website. CoNLL-U : alud can write its output in CoNLL-U, a widely used format for dependency parses. CoNLL-U visualization : the main developer of alud also wrote a webservice for visualizing CoNLL-U files. Go : alud is a package for the Go programming language. Publications Gosse Bouma & Gertjan van Noord (2017). Increasing return on annotation investment: the automatic construction of a Universal Dependency treebank for Dutch. In: Proceedings of the NoDaLiDa 2017 Workshop on Universal Dependencies (UDW 2017). Gosse Bouma (2018). Comparing two methods for adding Enhanced Dependencies to UD treebanks. In: Proceedings of the 17th International Workshop on Treebanks and Linguistic Theories (TLT 2018), December 13\u201314, 2018, Oslo University, Norway. Webpages alud GitHub page alud Go as package lassy2ud Credits alud was developed by the Computational Linguistics group of the Faculty of Arts, Groningen University .","title":"alud"},{"location":"tools/alud/#alud","text":"alud is a Go package for deriving Universal Dependencies from Dutch sentences parsed with Alpino .","title":"alud"},{"location":"tools/alud/#overview","text":"Alpino parses, POS-tags, lemmatizes and analyses morphologically any Dutch sentence that it is given. Alpino reliably analyses an input sentence syntactically, yielding a fully annotated syntactic tree with both constituents as well as explicitly labelled syntactic relations. alud derives Universal Dependencies from Alpino's output (in XML, in the alpino_ds format), and writes it in the CoNLL-U format . Alternatively, output can be embedded into the alpino_ds format, making them available for XPath queries. alud is also capable of inserting given Universal Dependencies into the alpino_ds format. alud was based on a translation of an xquery script, lassy2ud .","title":"Overview"},{"location":"tools/alud/#universal-dependencies","text":"Universal Dependencies (UD) is a programme that aims at cross-linguistically consistent tagging and dependency parsing. UD is an open community effort with over 500 contributors producing over 200 treebanks in over 100 languages. If you\u2019re new to UD, you should start by reading the first part of the Short Introduction and then browsing the annotation guidelines on the UD website.","title":"Universal Dependencies"},{"location":"tools/alud/#learn","text":"alud is available as a Go package. Up-to-date documentation of the package can be found here . alud's GitHub page provides extra information, such as how to install it. If you want to install alud, we kindly refer you to the up-to-date GitHub page .","title":"Learn"},{"location":"tools/alud/#mentions","text":"Alpino : alud is developed to convert Alpino's output into Universal Dependencies. Universal Dependencies : UD is a programme that aims at cross-linguistically consistent tagging and dependency parsing. UD is an open community effort with over 500 contributors producing over 200 treebanks in over 100 languages. If you\u2019re new to UD, you should start by reading the first part of the Short Introduction and then browsing the annotation guidelines on the UD website. CoNLL-U : alud can write its output in CoNLL-U, a widely used format for dependency parses. CoNLL-U visualization : the main developer of alud also wrote a webservice for visualizing CoNLL-U files. Go : alud is a package for the Go programming language.","title":"Mentions"},{"location":"tools/alud/#publications","text":"Gosse Bouma & Gertjan van Noord (2017). Increasing return on annotation investment: the automatic construction of a Universal Dependency treebank for Dutch. In: Proceedings of the NoDaLiDa 2017 Workshop on Universal Dependencies (UDW 2017). Gosse Bouma (2018). Comparing two methods for adding Enhanced Dependencies to UD treebanks. In: Proceedings of the 17th International Workshop on Treebanks and Linguistic Theories (TLT 2018), December 13\u201314, 2018, Oslo University, Norway.","title":"Publications"},{"location":"tools/alud/#webpages","text":"alud GitHub page alud Go as package lassy2ud","title":"Webpages"},{"location":"tools/alud/#credits","text":"alud was developed by the Computational Linguistics group of the Faculty of Arts, Groningen University .","title":"Credits"},{"location":"tools/asr_nl/","text":"Automatic Speech Recognition for Dutch CLARIAH offers a webservice for automatic speech recognition to provide the transcriptions of recordings spoken in Dutch. Overview The webservice is hosted by the Radboud University and requires an institute login to be used. The user can upload one file per project. Bulk processing is possible after contacting the developers. The webservice offers multiple ASR models to choose from, including one for daily conversations, oral history interviews, parliamentary discussions and Belgian Dutch. The code of the webservice is shared on GitHub . Privacy notice All data you upload to the service and data obtained using the service will remain yours and is accessible only by you and our technical staff. Your data will not be shared with third parties and not be used for any purpose other than the service's operation. You can remove your projects at any time and are encouraged to do so, which will remove your data from our servers permanently. We can not guarantee any long-term storage of your data so you are recommended to download the results and store it yourself immediately; projects on the server will be automatically deleted after 30 days. Despite our security precautions, we do discourage use of this service for highly confidential material as there is no encryption on the storage. Last, we also collect some statistics on the frequency of use of the service, when shared this will always be anonymised. Learn How to use In order to use the ASR webservice, the user needs an institute login. Having logged in, the user can create a project. Old projects are listed on the same page. Once the user has created a project, they can upload a file from their disk. Before doing so, they need to specify the file type (e.g. .wav or .mp3). The user is then prompted to choose the ASR model they want to use. A simple click on the Start button will start the processing of the file. The user may safely close their browser or shut down their computer during this process, the system will keep running on the server and is available when they return another time. The output consists of a textual transcription, including timestamps and speaker diarisation, in plain text as well as XML, and a logging file. User support The current version of the ASR webservice is hosted and maintained by the Centre for Language and Speech Technology of the Radboud University. If you have any suggestions, questions, or general feedback you can contact Henk van den Heuvel . Mentions Webpages Webservice: https://webservices.cls.ru.nl/asr_nl/ Source code: https://github.com/opensource-spraakherkenning-nl/asr_nl","title":"Automatic Speech Recognition for Dutch"},{"location":"tools/asr_nl/#automatic-speech-recognition-for-dutch","text":"CLARIAH offers a webservice for automatic speech recognition to provide the transcriptions of recordings spoken in Dutch.","title":"Automatic Speech Recognition for Dutch"},{"location":"tools/asr_nl/#overview","text":"The webservice is hosted by the Radboud University and requires an institute login to be used. The user can upload one file per project. Bulk processing is possible after contacting the developers. The webservice offers multiple ASR models to choose from, including one for daily conversations, oral history interviews, parliamentary discussions and Belgian Dutch. The code of the webservice is shared on GitHub .","title":"Overview"},{"location":"tools/asr_nl/#privacy-notice","text":"All data you upload to the service and data obtained using the service will remain yours and is accessible only by you and our technical staff. Your data will not be shared with third parties and not be used for any purpose other than the service's operation. You can remove your projects at any time and are encouraged to do so, which will remove your data from our servers permanently. We can not guarantee any long-term storage of your data so you are recommended to download the results and store it yourself immediately; projects on the server will be automatically deleted after 30 days. Despite our security precautions, we do discourage use of this service for highly confidential material as there is no encryption on the storage. Last, we also collect some statistics on the frequency of use of the service, when shared this will always be anonymised.","title":"Privacy notice"},{"location":"tools/asr_nl/#learn","text":"","title":"Learn"},{"location":"tools/asr_nl/#how-to-use","text":"In order to use the ASR webservice, the user needs an institute login. Having logged in, the user can create a project. Old projects are listed on the same page. Once the user has created a project, they can upload a file from their disk. Before doing so, they need to specify the file type (e.g. .wav or .mp3). The user is then prompted to choose the ASR model they want to use. A simple click on the Start button will start the processing of the file. The user may safely close their browser or shut down their computer during this process, the system will keep running on the server and is available when they return another time. The output consists of a textual transcription, including timestamps and speaker diarisation, in plain text as well as XML, and a logging file.","title":"How to use"},{"location":"tools/asr_nl/#user-support","text":"The current version of the ASR webservice is hosted and maintained by the Centre for Language and Speech Technology of the Radboud University. If you have any suggestions, questions, or general feedback you can contact Henk van den Heuvel .","title":"User support"},{"location":"tools/asr_nl/#mentions","text":"","title":"Mentions"},{"location":"tools/asr_nl/#webpages","text":"Webservice: https://webservices.cls.ru.nl/asr_nl/ Source code: https://github.com/opensource-spraakherkenning-nl/asr_nl","title":"Webpages"},{"location":"tools/auchann/","text":"AuChAnn AuChAnn (Automatic CHAT Annotation) is a python package that provides CHAT annotations based on a transcript string and an interpretation (or 'corrected') string. For example, the following transcript and correction Transcript: 'ik wilt nu eh na huis'\\ Correction: 'Ik wil nu naar huis.' will yield CHAT-Annotation: 'ik wilt [: wil] nu &-eh na(ar) [* s:r:prep] huis' CHAT is an annotation convention that was developed for the CHILDES corpus (MacWinney, 2000) and is used by many linguists to annotate speech. For more information on CHAT, you can read their manual . AuChAnn was specifically developed to enhance linguistic data in the form of a transcript and interpretation by a linguist for use with SASTA . Overview AuChAnn \u2013 Automatic CHAT Annotation tool \u2013 is a python library that can read a Dutch transcript and interpretation pair and generate a fitting CHAT annotation. AuChAnn was specifically developed to enhance linguistic data in the form of a transcript and interpretation by a linguist for use with SASTA . AuChAnn consistently outperforms human annotators, and provides annotations in a fraction of the time, making it a useful improvement for SASTA, but also for any other linguists that want to use information-rich CHAT annotations for their research. Learn Professor Frank Wijnen gave a lunch lecture on AuChAnn on 25 May 2023. During this lecture, Wijnen expanded on how complete and correct CHAT annotations can be generated on the basis of transcribed actual utterance-correct version pairs: Getting Started with AuChAnn You can install AuChAnn using pip: pip install auchann You can also optionally install sastadev which is used for detecting inflection errors. pip install auchann[NL] When installed, the program can be run interactively from the console using the command auchann . Import as Library To use AuChAnn in your own Python applications, you can import the align_words function from align_words , see below. This is the main functionality of the package. from auchann.align_words import align_words transcript = input(\"Transcript: \") correction = input(\"Correction: \") alignment = align_words(transcript, correction) print(alignment) Settings Various settings can be adjusted. Default values are used for every unchanged property. from auchann.align_words import align_words, AlignmentSettings import editdistance settings = AlignmentSettings() # Return the edit distance between the original and correction settings.calc_distance = lambda original, correction: editdistance.distance(original, correction) # Return an override of the distance and the error type; # if error type is None the distance returned will be ignored # Default method detects inflection errors settings.detect_error = lambda original, correction: (1, \"m\") if original == \"geloopt\" and correction == \"liep\" else (0, None) ### Sastadev contains a helper function for Dutch which detects inflection errors from sastadev.deregularise import detect_error settings.detect_error = detect_error # How many words could be split from one? # e.g. das -> da(t) (i)s requires a lookahead of 2 # hoest -> hoe (i)s (he)t requires a lookahead of 3 settings.lookahead = 5 # Allow detection of replacements within a group # e.g. swapping articles this will then be marked with # the specified key # EXAMPLE: # Transcript: de huis # Correction: het huis # de [: het] [* s:r:gc:art] huis settings.replacements = { 's:r:gc:art': ['de', 'het', 'een'], 's:r:gc:pro': ['dit', 'dat', 'deze'], 's:r:prep': ['aan', 'uit'] } # Other lists to adjust settings.fillers = ['eh', 'hm', 'uh'] settings.fragments = ['ba', 'to', 'mu'] ### Example usage transcript = input(\"Transcript: \") correction = input(\"Correction: \") alignment = align_words(transcript, correction, settings) print(alignment) How it Works The align_words function scans the transcript and correction and determines for each token whether a correction token is copied exactly from the transcript, replaces a token from the transcript, is inserted, or whether a transcript token has been omitted. Based on which of these operations has occurred, the function adds the appropriate CHAT annotation to the output string. The algorithm uses edit distance to establish which words are replacements of each other, i.e. it links a transcript token to a correction token. Words with the lowest available edit distance are matched together, and based on this match the operations COPY and REPLACE are determined. If two candidates have the same edit distance to a token, word position is used to determine the match. The operations REMOVE and INSERT are established if no suitable match can be found for a transcript and correction token respectively. In addition to establishing these four operations, the function detects several other properties of the transcript and correction which can be expressed in CHAT. For example, it determines whether a word is a filler or fragment, whether a conjugation error has occurred, or if a pronoun, preposition, or article has been used incorrectly. Development To install the requirements: pip install -r requirements.txt To run the AuChAnn command-line function from the console: python -m auchann Run Tests pip install pytest pytest Upload to PyPi pip install pip-tools twine python setup.py sdist twine upload dist/*.tar.gz Mentions Acknowledgments AuChAnn was developed by the Centre for Digital Humanities \u2013 Research Software Lab (Sheean Spoel and Mees van Stiphout), in collaboration with Frank Wijnen, Professor of psycholinguistics at the Department of Languages, Literature and Communication. The research for this software was made possible by the CLARIAH-PLUS project financed by NWO (Grant 184.034.023). Lunch Lecture Professor Frank Wijnen gave a lunch lecture on AuChAnn on 25 May 2023. During this lecture, Wijnen expanded on how complete and correct CHAT annotations can be generated on the basis of transcribed actual utterance-correct version pairs: https://www.clariah.nl/nl/evenementen/lunchlezing-auchann Webpages AuChAnn on pypi AuChAnn GitHub page AuChAnn in the RSL portfolio Other SASTA : AuChAnn was specifically developed to enhance linguistic data in the form of a transcript and interpretation by a linguist for use with SASTA. References MacWhinney, B. (2000). The CHILDES project: Tools for analyzing talk: Transcription format and programs (3rd ed.). Lawrence Erlbaum Associates Publishers.","title":"AuChAnn"},{"location":"tools/auchann/#auchann","text":"AuChAnn (Automatic CHAT Annotation) is a python package that provides CHAT annotations based on a transcript string and an interpretation (or 'corrected') string. For example, the following transcript and correction Transcript: 'ik wilt nu eh na huis'\\ Correction: 'Ik wil nu naar huis.' will yield CHAT-Annotation: 'ik wilt [: wil] nu &-eh na(ar) [* s:r:prep] huis' CHAT is an annotation convention that was developed for the CHILDES corpus (MacWinney, 2000) and is used by many linguists to annotate speech. For more information on CHAT, you can read their manual . AuChAnn was specifically developed to enhance linguistic data in the form of a transcript and interpretation by a linguist for use with SASTA .","title":"AuChAnn"},{"location":"tools/auchann/#overview","text":"AuChAnn \u2013 Automatic CHAT Annotation tool \u2013 is a python library that can read a Dutch transcript and interpretation pair and generate a fitting CHAT annotation. AuChAnn was specifically developed to enhance linguistic data in the form of a transcript and interpretation by a linguist for use with SASTA . AuChAnn consistently outperforms human annotators, and provides annotations in a fraction of the time, making it a useful improvement for SASTA, but also for any other linguists that want to use information-rich CHAT annotations for their research.","title":"Overview"},{"location":"tools/auchann/#learn","text":"Professor Frank Wijnen gave a lunch lecture on AuChAnn on 25 May 2023. During this lecture, Wijnen expanded on how complete and correct CHAT annotations can be generated on the basis of transcribed actual utterance-correct version pairs:","title":"Learn"},{"location":"tools/auchann/#getting-started-with-auchann","text":"You can install AuChAnn using pip: pip install auchann You can also optionally install sastadev which is used for detecting inflection errors. pip install auchann[NL] When installed, the program can be run interactively from the console using the command auchann .","title":"Getting Started with AuChAnn"},{"location":"tools/auchann/#import-as-library","text":"To use AuChAnn in your own Python applications, you can import the align_words function from align_words , see below. This is the main functionality of the package. from auchann.align_words import align_words transcript = input(\"Transcript: \") correction = input(\"Correction: \") alignment = align_words(transcript, correction) print(alignment)","title":"Import as Library"},{"location":"tools/auchann/#settings","text":"Various settings can be adjusted. Default values are used for every unchanged property. from auchann.align_words import align_words, AlignmentSettings import editdistance settings = AlignmentSettings() # Return the edit distance between the original and correction settings.calc_distance = lambda original, correction: editdistance.distance(original, correction) # Return an override of the distance and the error type; # if error type is None the distance returned will be ignored # Default method detects inflection errors settings.detect_error = lambda original, correction: (1, \"m\") if original == \"geloopt\" and correction == \"liep\" else (0, None) ### Sastadev contains a helper function for Dutch which detects inflection errors from sastadev.deregularise import detect_error settings.detect_error = detect_error # How many words could be split from one? # e.g. das -> da(t) (i)s requires a lookahead of 2 # hoest -> hoe (i)s (he)t requires a lookahead of 3 settings.lookahead = 5 # Allow detection of replacements within a group # e.g. swapping articles this will then be marked with # the specified key # EXAMPLE: # Transcript: de huis # Correction: het huis # de [: het] [* s:r:gc:art] huis settings.replacements = { 's:r:gc:art': ['de', 'het', 'een'], 's:r:gc:pro': ['dit', 'dat', 'deze'], 's:r:prep': ['aan', 'uit'] } # Other lists to adjust settings.fillers = ['eh', 'hm', 'uh'] settings.fragments = ['ba', 'to', 'mu'] ### Example usage transcript = input(\"Transcript: \") correction = input(\"Correction: \") alignment = align_words(transcript, correction, settings) print(alignment)","title":"Settings"},{"location":"tools/auchann/#how-it-works","text":"The align_words function scans the transcript and correction and determines for each token whether a correction token is copied exactly from the transcript, replaces a token from the transcript, is inserted, or whether a transcript token has been omitted. Based on which of these operations has occurred, the function adds the appropriate CHAT annotation to the output string. The algorithm uses edit distance to establish which words are replacements of each other, i.e. it links a transcript token to a correction token. Words with the lowest available edit distance are matched together, and based on this match the operations COPY and REPLACE are determined. If two candidates have the same edit distance to a token, word position is used to determine the match. The operations REMOVE and INSERT are established if no suitable match can be found for a transcript and correction token respectively. In addition to establishing these four operations, the function detects several other properties of the transcript and correction which can be expressed in CHAT. For example, it determines whether a word is a filler or fragment, whether a conjugation error has occurred, or if a pronoun, preposition, or article has been used incorrectly.","title":"How it Works"},{"location":"tools/auchann/#development","text":"To install the requirements: pip install -r requirements.txt To run the AuChAnn command-line function from the console: python -m auchann","title":"Development"},{"location":"tools/auchann/#run-tests","text":"pip install pytest pytest","title":"Run Tests"},{"location":"tools/auchann/#upload-to-pypi","text":"pip install pip-tools twine python setup.py sdist twine upload dist/*.tar.gz","title":"Upload to PyPi"},{"location":"tools/auchann/#mentions","text":"","title":"Mentions"},{"location":"tools/auchann/#acknowledgments","text":"AuChAnn was developed by the Centre for Digital Humanities \u2013 Research Software Lab (Sheean Spoel and Mees van Stiphout), in collaboration with Frank Wijnen, Professor of psycholinguistics at the Department of Languages, Literature and Communication. The research for this software was made possible by the CLARIAH-PLUS project financed by NWO (Grant 184.034.023).","title":"Acknowledgments"},{"location":"tools/auchann/#lunch-lecture","text":"Professor Frank Wijnen gave a lunch lecture on AuChAnn on 25 May 2023. During this lecture, Wijnen expanded on how complete and correct CHAT annotations can be generated on the basis of transcribed actual utterance-correct version pairs: https://www.clariah.nl/nl/evenementen/lunchlezing-auchann","title":"Lunch Lecture"},{"location":"tools/auchann/#webpages","text":"AuChAnn on pypi AuChAnn GitHub page AuChAnn in the RSL portfolio","title":"Webpages"},{"location":"tools/auchann/#other","text":"SASTA : AuChAnn was specifically developed to enhance linguistic data in the form of a transcript and interpretation by a linguist for use with SASTA.","title":"Other"},{"location":"tools/auchann/#references","text":"MacWhinney, B. (2000). The CHILDES project: Tools for analyzing talk: Transcription format and programs (3rd ed.). Lawrence Erlbaum Associates Publishers.","title":"References"},{"location":"tools/autosearch/","text":"AutoSearch AutoSearch is a tool to upload corpora annotated with part of speech, lemma and word form in FoLiA or TEI format, and to define one or several corpora and to search them. Overview Autosearch allows users to define one or more corpora and upload data for the corpora, after which the corpora will be made automatically searchable in a private workspace. Users can upload text data annotated with lemma + part of speech tags in TEI or FoLiA format, either as a single XML file or as an archive (zip or tar.gz) containing several XML files. Corpus size is limited to begin with (25 MB limit per uploaded file; 500,000 token limit for an entire corpus), but these limits may be increased at a later point in time. The search application is powered by the INT BlackLab corpus search engine . The search interface is the same as the one used in for example the Corpus of Contemporary Dutch / Corpus Hedendaags Nederlands. To use this application you need an account. Employees of universities or research institutes can log in with the user ID and password of their own organization. Click on the login button, select your organization from the list, and log into the Corpus of Contemporary Dutch by using your academic account. If you do not have an account at an academic institute, please apply for an account at (CLARIN.EU). Learn Documentation The AutoSearch manual is available here . Page guide After logging in, two buttons are available: New corpus : Here you can create a new private corpus. A private corpus allows you to upload and search through your own data. Corpora you create are not visible to others unless you explictly share them, and they are restricted in their maximum size. Select the format of the data you intend to upload to this corpus here. Because annotated data can be structured in many different ways, you will need to define how the data you intend to upload to this corpus should be indexed. Some of the more well-known types, such as TEI and FoLiA are already pre-supported. If your data is in a format that's not in this list, it's possible to create your own custom format definition by clicking the new format button at the bottom of the page. The new format will then become available in this list. New format : If your corpus material is in a format that we don't support out of the box (yet), you can customize how your data is treated by creating a new format here. After you've done so, you will need to create a new corpus that uses the format and add some files to it. Formats can be written in either json or yaml . Changing this setting will also change the syntax highlighting so you can more easily spot mistakes. A good place to start writing a format is usually to download one of our presets, and edit it until it matches the structure of your corpus material. Select a format to start with in the dropdown then click download to open it in in the editor. You can also download another user's format, if you know the name. To do so, enter their username followed by ':', followed by the name of the format username:format in the box next to the download button. When you load one of your own formats, its name will automatically be filled in, so any changes you save will overwrite the format. When you're done editing your format, save it by clicking here. The format will be saved using the name you entered to the left. If you already own a format with this name, the format will be overwitten . If you save over a format that's already being used in one of your corpora, then any new data you upload to that corpus will be indexed according to the updated format. Information on how to write a format can be found here . Mentions Credits The software powering this website was developed at the Dutch Language Institute . The corpus search is powered by BlackLab , an open source Lucene-based corpus retrieval engine allowing fast and complex searches on large volumes of annotated text.science.ru.nl.","title":"AutoSearch"},{"location":"tools/autosearch/#autosearch","text":"AutoSearch is a tool to upload corpora annotated with part of speech, lemma and word form in FoLiA or TEI format, and to define one or several corpora and to search them.","title":"AutoSearch"},{"location":"tools/autosearch/#overview","text":"Autosearch allows users to define one or more corpora and upload data for the corpora, after which the corpora will be made automatically searchable in a private workspace. Users can upload text data annotated with lemma + part of speech tags in TEI or FoLiA format, either as a single XML file or as an archive (zip or tar.gz) containing several XML files. Corpus size is limited to begin with (25 MB limit per uploaded file; 500,000 token limit for an entire corpus), but these limits may be increased at a later point in time. The search application is powered by the INT BlackLab corpus search engine . The search interface is the same as the one used in for example the Corpus of Contemporary Dutch / Corpus Hedendaags Nederlands. To use this application you need an account. Employees of universities or research institutes can log in with the user ID and password of their own organization. Click on the login button, select your organization from the list, and log into the Corpus of Contemporary Dutch by using your academic account. If you do not have an account at an academic institute, please apply for an account at (CLARIN.EU).","title":"Overview"},{"location":"tools/autosearch/#learn","text":"","title":"Learn"},{"location":"tools/autosearch/#documentation","text":"The AutoSearch manual is available here .","title":"Documentation"},{"location":"tools/autosearch/#page-guide","text":"After logging in, two buttons are available: New corpus : Here you can create a new private corpus. A private corpus allows you to upload and search through your own data. Corpora you create are not visible to others unless you explictly share them, and they are restricted in their maximum size. Select the format of the data you intend to upload to this corpus here. Because annotated data can be structured in many different ways, you will need to define how the data you intend to upload to this corpus should be indexed. Some of the more well-known types, such as TEI and FoLiA are already pre-supported. If your data is in a format that's not in this list, it's possible to create your own custom format definition by clicking the new format button at the bottom of the page. The new format will then become available in this list. New format : If your corpus material is in a format that we don't support out of the box (yet), you can customize how your data is treated by creating a new format here. After you've done so, you will need to create a new corpus that uses the format and add some files to it. Formats can be written in either json or yaml . Changing this setting will also change the syntax highlighting so you can more easily spot mistakes. A good place to start writing a format is usually to download one of our presets, and edit it until it matches the structure of your corpus material. Select a format to start with in the dropdown then click download to open it in in the editor. You can also download another user's format, if you know the name. To do so, enter their username followed by ':', followed by the name of the format username:format in the box next to the download button. When you load one of your own formats, its name will automatically be filled in, so any changes you save will overwrite the format. When you're done editing your format, save it by clicking here. The format will be saved using the name you entered to the left. If you already own a format with this name, the format will be overwitten . If you save over a format that's already being used in one of your corpora, then any new data you upload to that corpus will be indexed according to the updated format. Information on how to write a format can be found here .","title":"Page guide"},{"location":"tools/autosearch/#mentions","text":"","title":"Mentions"},{"location":"tools/autosearch/#credits","text":"The software powering this website was developed at the Dutch Language Institute . The corpus search is powered by BlackLab , an open source Lucene-based corpus retrieval engine allowing fast and complex searches on large volumes of annotated text.science.ru.nl.","title":"Credits"},{"location":"tools/blacklab/","text":"Blacklab BlackLab is a corpus search engine built on top of Apache Lucene . It supports token-based querying and querying (dependency) relations. Overview BlackLab was designed primarily for linguists, but is also used for other purposes, like historical research and knowledge extraction. It is available as a REST API (web service), so you can use it from any programming language. BlackLab's features include: Index annotated text, so you can search for specific headwords or parts of speech. Easy to use, well-documented REST API. Fast and scalable: find complex patterns in large corpora in seconds. Index your data using a built-in format or by writing a configuration file. Search for complex patterns using the powerful BlackLab Corpus Query Language . Search within spans to e.g. find named entities containing tower at the end of a sentence. Search (dependency) relations, to find specific (tree) structures in your text. (NEW in v4) Capture parts of matches. Group and sort result sets on many criteria, such as the text preceding the match. Highlight hits in a document and keyword-in-context (KWIC) view of hits. Actively developed since 2010, with many plans for the future . Learn Try it online For a quick example of the BlackLab Frontend web application, have a look at either of these: Brieven als Buit (\"Letters as Loot\"), where you can search a collection of historical letters to and from sailors from the 17th to the 19th century; Corpus Gysseling , a small corpus of historic Dutch (1200-1300). With a free CLARIN account , you can also check out: Corpus Hedendaags Nederlands ; OpenSonar . Here are a few searches you can try (click on the Extended tab): Lemma: \"koe\" Finds all forms of the word \"koe\" (cow). Other words to try: \"wet\" (law), \"zien\" (to see), \"groot\" (large). Part of speech: \"NOU-C\" Find all common nouns. Other values to try: \"VRB \" (verbs), \"ADJ \" (adjectives). Word: \"coe\" Find a specific historic spelling of \"koe\". This is just a small sample of the capabilities of BlackLab. Documentation If you're excited about the possibilities and want to get BlackLab up and running yourself, we kindly refer to the BlackLab Guide . BlackLab Corpus Query Language (BCQL) BlackLab Corpus Query Language or BCQL is a powerful query language for text corpora. A guide to speaking BCQL can be found here . Mentions Corpora using BlackLab (selection) Brieven als Buit Corpus Gysseling Corpus Hedendaags Nederlands OpenSonar Webpages GitHub repositories: BlackLab Server and Core and BlackLab Frontend Release history Credits and Contact Information BlackLab was developed at the Dutch Language Institute (INT) to provide a fast and feature-rich search interface on our historical and contemporary text corpora. It was released as open source (Apache License 2.0) in 2012 and has since gathered a number of users and contributors. It is still in active development. For technical questions, contact Jan Niestadt . Follow @BlackLab_IvdNT on Twitter (very low activity).","title":"BlackLab"},{"location":"tools/blacklab/#blacklab","text":"BlackLab is a corpus search engine built on top of Apache Lucene . It supports token-based querying and querying (dependency) relations.","title":"Blacklab"},{"location":"tools/blacklab/#overview","text":"BlackLab was designed primarily for linguists, but is also used for other purposes, like historical research and knowledge extraction. It is available as a REST API (web service), so you can use it from any programming language. BlackLab's features include: Index annotated text, so you can search for specific headwords or parts of speech. Easy to use, well-documented REST API. Fast and scalable: find complex patterns in large corpora in seconds. Index your data using a built-in format or by writing a configuration file. Search for complex patterns using the powerful BlackLab Corpus Query Language . Search within spans to e.g. find named entities containing tower at the end of a sentence. Search (dependency) relations, to find specific (tree) structures in your text. (NEW in v4) Capture parts of matches. Group and sort result sets on many criteria, such as the text preceding the match. Highlight hits in a document and keyword-in-context (KWIC) view of hits. Actively developed since 2010, with many plans for the future .","title":"Overview"},{"location":"tools/blacklab/#learn","text":"","title":"Learn"},{"location":"tools/blacklab/#try-it-online","text":"For a quick example of the BlackLab Frontend web application, have a look at either of these: Brieven als Buit (\"Letters as Loot\"), where you can search a collection of historical letters to and from sailors from the 17th to the 19th century; Corpus Gysseling , a small corpus of historic Dutch (1200-1300). With a free CLARIN account , you can also check out: Corpus Hedendaags Nederlands ; OpenSonar . Here are a few searches you can try (click on the Extended tab): Lemma: \"koe\" Finds all forms of the word \"koe\" (cow). Other words to try: \"wet\" (law), \"zien\" (to see), \"groot\" (large). Part of speech: \"NOU-C\" Find all common nouns. Other values to try: \"VRB \" (verbs), \"ADJ \" (adjectives). Word: \"coe\" Find a specific historic spelling of \"koe\". This is just a small sample of the capabilities of BlackLab.","title":"Try it online"},{"location":"tools/blacklab/#documentation","text":"If you're excited about the possibilities and want to get BlackLab up and running yourself, we kindly refer to the BlackLab Guide .","title":"Documentation"},{"location":"tools/blacklab/#blacklab-corpus-query-language-bcql","text":"BlackLab Corpus Query Language or BCQL is a powerful query language for text corpora. A guide to speaking BCQL can be found here .","title":"BlackLab Corpus Query Language (BCQL)"},{"location":"tools/blacklab/#mentions","text":"","title":"Mentions"},{"location":"tools/blacklab/#corpora-using-blacklab-selection","text":"Brieven als Buit Corpus Gysseling Corpus Hedendaags Nederlands OpenSonar","title":"Corpora using BlackLab (selection)"},{"location":"tools/blacklab/#webpages","text":"GitHub repositories: BlackLab Server and Core and BlackLab Frontend Release history","title":"Webpages"},{"location":"tools/blacklab/#credits-and-contact-information","text":"BlackLab was developed at the Dutch Language Institute (INT) to provide a fast and feature-rich search interface on our historical and contemporary text corpora. It was released as open source (Apache License 2.0) in 2012 and has since gathered a number of users and contributors. It is still in active development. For technical questions, contact Jan Niestadt . Follow @BlackLab_IvdNT on Twitter (very low activity).","title":"Credits and Contact Information"},{"location":"tools/burgerLinker/","text":"BurgerLinker BurgerLinker is a command line tool for linking civil registries. It is designed for researchers to link multiple civil records (e.g. birth, marriage and death certificates) to describe a life course. However, burgerLinker is also capable of linking other personal records. Overview BurgerLinker is a command line tool which allows researchers to link civil records, relying on the mentioned names of ego, father, mother, and partner on the civil record, modeled according to our Civil Registries schema. Because the Dutch civil registry contains millions of records, BurgerLinker is designed to be scalable and link the records fast BurgerLinker uses a RDF dataset as input and converts them into HDT for efficient computing. BurgerLinker produces a CSV file or a N-QUADS file as output. Learn Instructions Prerequisites: BurgerLinker wiki to learn the basic command line queries. Use the BurgerLinker wiki to prepare your data correctly to be able to use BurgerLinker. It shows how to standardise your data and how to adapt the data model. Mentions Raad, J. (2023) Burgerlinker, linking dutch civil registries [CLARIAH Lunch Lecture]","title":"BurgerLinker"},{"location":"tools/burgerLinker/#burgerlinker","text":"BurgerLinker is a command line tool for linking civil registries. It is designed for researchers to link multiple civil records (e.g. birth, marriage and death certificates) to describe a life course. However, burgerLinker is also capable of linking other personal records.","title":"BurgerLinker"},{"location":"tools/burgerLinker/#overview","text":"BurgerLinker is a command line tool which allows researchers to link civil records, relying on the mentioned names of ego, father, mother, and partner on the civil record, modeled according to our Civil Registries schema. Because the Dutch civil registry contains millions of records, BurgerLinker is designed to be scalable and link the records fast BurgerLinker uses a RDF dataset as input and converts them into HDT for efficient computing. BurgerLinker produces a CSV file or a N-QUADS file as output.","title":"Overview"},{"location":"tools/burgerLinker/#learn","text":"","title":"Learn"},{"location":"tools/burgerLinker/#instructions","text":"Prerequisites: BurgerLinker wiki to learn the basic command line queries. Use the BurgerLinker wiki to prepare your data correctly to be able to use BurgerLinker. It shows how to standardise your data and how to adapt the data model.","title":"Instructions"},{"location":"tools/burgerLinker/#mentions","text":"Raad, J. (2023) Burgerlinker, linking dutch civil registries [CLARIAH Lunch Lecture]","title":"Mentions"},{"location":"tools/clam/","text":"Frog CLAM allows you to quickly and transparently transform your command line application into a RESTful webservice and web interface, with which both human end-users as well as automated clients can interact. Overview CLAM expects a description of your system and wraps itself around the system, allowing end-users or automated clients to upload input files to your application, start your application with specific parameters of their choice, and download and view the output of the application once it is completed. CLAM is set up in a universal fashion, requiring minimal effort on the part of the service developer. Your actual application is treated as a black box, of which only the parameters, input formats and output formats need to be described. Your application itself needs not be network aware in any way, nor aware of CLAM, and the handling and validation of input can be taken care of by CLAM. CLAM is entirely written in Python and runs on UNIX-derived systems. A Python API is provided, but knowledge of Python is not necessary to use CLAM. CLAM communicates using a transparent XML format, and uses XSL transformation offers a full web 2.0 web-interface for human end users. CLAM is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation . CLAM is written by Maarten van Gompel, Centre of Language Studies, Radboud University Nijmegen. Learn Documentation Please consult the CLAM Documentation for extensive documentation. Support Check back often as updates are released regularly for CLAM. These can be easily installed through the Python Package Index or directly from Github. Bug reports, feature requests and requests for assistance can be submitted through Github , the mailing list, or by mailing to proycon@anaproy.nl . A public roadmap of planned features for the future, as well as bug reports about existing features, can be found on the CLAM Issue Tracker . Feel free to add any problems or feature requests if you have a (free) github account! CLAM support is funded by the NWO CLARIN-NL programme and its successor CLARIAH . Download and installation CLAM is maintained at the Python Package Index . It is available for Linux and other UNIX-like distributions and is intended for users who want to build a webservice around their own tool, want to write a client for existing CLAM webservices, or are system administrators setting up existing CLAM services. To automatically download and install CLAM from the Python Package Index, the convenient tool pip is available, it may be called pip3 if you want to use Python 3 (recommended): $ pip3 install clam We recommend the use of a Python Virtual Environment The source code is also available through the source code repository at Github . Please consult the CLAM Documentation for further instructions, including installation details. After reading the documentation, it is recommended to watch the CLAM tutorial videos at the bottom of this page. Architecture CLAM has a layered architecture, with at the core the NLP application(s) you want to turn into a webservice. The application itself can remain untouched and unaware of CLAM. The scheme on the right illustrates the various layers. Read more in the CLAM Manual and also see the CLAM poster presented at CLIN 21. Instruction videos Instruction videos Make sure to watch the latest tutorial video first: Tutorial (September 2015) - This tutorial shows everything you need to get started, it shows how to make a webservice for a simple command-line tool The below instruction videos are older (2011), but still useful: Part I - In this first video we will create a simple webservice around an ad-hoc NLP application. Part II - We will expand the webservice we created in the first video. Part III - We will take a deeper look into the metadata and parameter abilities of CLAM. Part IV - In this video we will see how to write a client in Python to connect to the webservice we created earlier. We will use the CLAM Client API. Mentions Credits and Contact Information CLAM is written by Maarten van Gompel, Centre of Language Studies, Radboud University Nijmegen. CLAM support is funded by the NWO CLARIN-NL programme and its successor CLARIAH. Webpages CLAM mainpage CLAM documentation CLAM GitHub page CLAM Python Package Poster presentation Maarten van Gompel, Martin Reynaert & Antal van den Bosch (2011, February). CLAM: Computational Linguistics Application Mediator . [Poster presentation]. CLIN 21, Tilburg.","title":"CLAM"},{"location":"tools/clam/#frog","text":"CLAM allows you to quickly and transparently transform your command line application into a RESTful webservice and web interface, with which both human end-users as well as automated clients can interact.","title":"Frog"},{"location":"tools/clam/#overview","text":"CLAM expects a description of your system and wraps itself around the system, allowing end-users or automated clients to upload input files to your application, start your application with specific parameters of their choice, and download and view the output of the application once it is completed. CLAM is set up in a universal fashion, requiring minimal effort on the part of the service developer. Your actual application is treated as a black box, of which only the parameters, input formats and output formats need to be described. Your application itself needs not be network aware in any way, nor aware of CLAM, and the handling and validation of input can be taken care of by CLAM. CLAM is entirely written in Python and runs on UNIX-derived systems. A Python API is provided, but knowledge of Python is not necessary to use CLAM. CLAM communicates using a transparent XML format, and uses XSL transformation offers a full web 2.0 web-interface for human end users. CLAM is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation . CLAM is written by Maarten van Gompel, Centre of Language Studies, Radboud University Nijmegen.","title":"Overview"},{"location":"tools/clam/#learn","text":"","title":"Learn"},{"location":"tools/clam/#documentation","text":"Please consult the CLAM Documentation for extensive documentation.","title":"Documentation"},{"location":"tools/clam/#support","text":"Check back often as updates are released regularly for CLAM. These can be easily installed through the Python Package Index or directly from Github. Bug reports, feature requests and requests for assistance can be submitted through Github , the mailing list, or by mailing to proycon@anaproy.nl . A public roadmap of planned features for the future, as well as bug reports about existing features, can be found on the CLAM Issue Tracker . Feel free to add any problems or feature requests if you have a (free) github account! CLAM support is funded by the NWO CLARIN-NL programme and its successor CLARIAH .","title":"Support"},{"location":"tools/clam/#download-and-installation","text":"CLAM is maintained at the Python Package Index . It is available for Linux and other UNIX-like distributions and is intended for users who want to build a webservice around their own tool, want to write a client for existing CLAM webservices, or are system administrators setting up existing CLAM services. To automatically download and install CLAM from the Python Package Index, the convenient tool pip is available, it may be called pip3 if you want to use Python 3 (recommended): $ pip3 install clam We recommend the use of a Python Virtual Environment The source code is also available through the source code repository at Github . Please consult the CLAM Documentation for further instructions, including installation details. After reading the documentation, it is recommended to watch the CLAM tutorial videos at the bottom of this page.","title":"Download and installation"},{"location":"tools/clam/#architecture","text":"CLAM has a layered architecture, with at the core the NLP application(s) you want to turn into a webservice. The application itself can remain untouched and unaware of CLAM. The scheme on the right illustrates the various layers. Read more in the CLAM Manual and also see the CLAM poster presented at CLIN 21.","title":"Architecture"},{"location":"tools/clam/#instruction-videos","text":"Instruction videos Make sure to watch the latest tutorial video first: Tutorial (September 2015) - This tutorial shows everything you need to get started, it shows how to make a webservice for a simple command-line tool The below instruction videos are older (2011), but still useful: Part I - In this first video we will create a simple webservice around an ad-hoc NLP application. Part II - We will expand the webservice we created in the first video. Part III - We will take a deeper look into the metadata and parameter abilities of CLAM. Part IV - In this video we will see how to write a client in Python to connect to the webservice we created earlier. We will use the CLAM Client API.","title":"Instruction videos"},{"location":"tools/clam/#mentions","text":"","title":"Mentions"},{"location":"tools/clam/#credits-and-contact-information","text":"CLAM is written by Maarten van Gompel, Centre of Language Studies, Radboud University Nijmegen. CLAM support is funded by the NWO CLARIN-NL programme and its successor CLARIAH.","title":"Credits and Contact Information"},{"location":"tools/clam/#webpages","text":"CLAM mainpage CLAM documentation CLAM GitHub page CLAM Python Package","title":"Webpages"},{"location":"tools/clam/#poster-presentation","text":"Maarten van Gompel, Martin Reynaert & Antal van den Bosch (2011, February). CLAM: Computational Linguistics Application Mediator . [Poster presentation]. CLIN 21, Tilburg.","title":"Poster presentation"},{"location":"tools/colibricore/","text":"Colibri Core Colibri Core is an NLP tool as well as a C++ and Python library for working with basic linguistic constructions such as n-grams and skipgrams (i.e. patterns with one or more gaps, either of fixed or dynamic size) in a quick and memory-efficient way. Overview Colibri Core is software to quickly and efficiently count and extract patterns from large corpus data, to extract various statistics on the extracted patterns, and to compute relations between the extracted patterns. The employed notion of pattern or construction encompasses the following categories: n-gram -- n consecutive words; skipgram -- An abstract pattern of predetermined length with one or multiple gaps (of specific size); flexgram -- An abstract pattern with one or more gaps of variable-size. At the heart of the sofware is the notion of pattern models. A pattern model is simply a collection of extracted patterns (any of the three categories) and their counts from a specific corpus. Colibri Core is available as a collection of standalone command-line tools , as a C++ library , and as a Python library . Learn Colibri Core is software to quickly and efficiently count and extract patterns from large corpus data, to extract various statistics on the extracted patterns, and to compute relations between the extracted patterns. The employed notion of pattern or construction encompasses the following categories: n-gram -- n consecutive words; skipgram -- An abstract pattern of predetermined length with one or multiple gaps (of specific size); flexgram -- An abstract pattern with one or more gaps of variable-size. N-gram extraction may seem fairly trivial at first, with a few lines in your favourite scripting language, you can move a simple sliding window of size n over your corpus and store the results in some kind of hashmap. This trivial approach however makes an unnecessarily high demand on memory resources, this often becomes prohibitive if unleashed on large corpora. Colibri Core tries to minimise these space requirements in several ways: Compressed binary representation -- Each word type is assigned a numeric class, which is encoded in a compact binary format in which highly frequent classes take less space than less frequent classes. Colibri core always uses this representation rather than a full string representation, both on disk and in memory. Informed iterative counting -- Counting is performed more intelligently by iteratively processing the corpus in several passes and quickly discarding patterns that won't reach the desired occurrence threshold. Skipgram and flexgram extraction are computationally more demanding but have been implemented with similar optimisations. Skipgrams are computed by abstracting over n-grams, and flexgrams in turn are computed either by abstracting over skipgrams, or directly from n-grams on the basis of co-occurrence information (mutual pointwise information). At the heart of the sofware is the notion of pattern models . The core tool, to be used from the command-line, is colibri-patternmodeller which enables you to build pattern models, generate statistical reports, query for specific patterns and relations, and manipulate models. A pattern model is simply a collection of extracted patterns (any of the three categories) and their counts from a specific corpus. Pattern models come in two varieties: Unindexed Pattern Model -- The simplest form, which simply stores the patterns and their count; Indexed Pattern Model -- The more informed form, which retains all indices to the original corpus, at the cost of more memory/diskspace. The Indexed Pattern Model is much more powerful, and allows more statistics and relations to be inferred. The generation of pattern models is optionally parametrised by a minimum occurrence threshold, a maximum pattern length, and a lower-boundary on the different types that may instantiate a skipgram (i.e. possible fillings of the gaps). Distributions Colibri Core is available as a collection of standalone command-line tools , as a C++ library , and as a Python library . Documentation and resources Colibri Core is well documented in the following places: van Gompel, M., & van den Bosch, A. (2016). Efficient n-gram, Skipgram and Flexgram Modelling with Colibri Core. Journal of Open Research Software, 4 (1), e30. http://dx.doi.org/10.5334/jors.105 Source code on GitHub Documentation and Python API reference C++ API reference Python Tutorial (ipython notebook) User support Colibri Core was developed by Maarten van Gompel at the Centre of Language Studies, Radboud University Nijmegen, under supervision of Antal van den Bosch. Mentions Key publications van Gompel, M., & van den Bosch, A. (2016). Efficient n-gram, Skipgram and Flexgram Modelling with Colibri Core. Journal of Open Research Software, 4 (1), e30. http://dx.doi.org/10.5334/jors.105 Webpages Source code on GitHub Documentation and Python API reference C++ API reference Python Tutorial (ipython notebook)","title":"Colibri Core"},{"location":"tools/colibricore/#colibri-core","text":"Colibri Core is an NLP tool as well as a C++ and Python library for working with basic linguistic constructions such as n-grams and skipgrams (i.e. patterns with one or more gaps, either of fixed or dynamic size) in a quick and memory-efficient way.","title":"Colibri Core"},{"location":"tools/colibricore/#overview","text":"Colibri Core is software to quickly and efficiently count and extract patterns from large corpus data, to extract various statistics on the extracted patterns, and to compute relations between the extracted patterns. The employed notion of pattern or construction encompasses the following categories: n-gram -- n consecutive words; skipgram -- An abstract pattern of predetermined length with one or multiple gaps (of specific size); flexgram -- An abstract pattern with one or more gaps of variable-size. At the heart of the sofware is the notion of pattern models. A pattern model is simply a collection of extracted patterns (any of the three categories) and their counts from a specific corpus. Colibri Core is available as a collection of standalone command-line tools , as a C++ library , and as a Python library .","title":"Overview"},{"location":"tools/colibricore/#learn","text":"Colibri Core is software to quickly and efficiently count and extract patterns from large corpus data, to extract various statistics on the extracted patterns, and to compute relations between the extracted patterns. The employed notion of pattern or construction encompasses the following categories: n-gram -- n consecutive words; skipgram -- An abstract pattern of predetermined length with one or multiple gaps (of specific size); flexgram -- An abstract pattern with one or more gaps of variable-size. N-gram extraction may seem fairly trivial at first, with a few lines in your favourite scripting language, you can move a simple sliding window of size n over your corpus and store the results in some kind of hashmap. This trivial approach however makes an unnecessarily high demand on memory resources, this often becomes prohibitive if unleashed on large corpora. Colibri Core tries to minimise these space requirements in several ways: Compressed binary representation -- Each word type is assigned a numeric class, which is encoded in a compact binary format in which highly frequent classes take less space than less frequent classes. Colibri core always uses this representation rather than a full string representation, both on disk and in memory. Informed iterative counting -- Counting is performed more intelligently by iteratively processing the corpus in several passes and quickly discarding patterns that won't reach the desired occurrence threshold. Skipgram and flexgram extraction are computationally more demanding but have been implemented with similar optimisations. Skipgrams are computed by abstracting over n-grams, and flexgrams in turn are computed either by abstracting over skipgrams, or directly from n-grams on the basis of co-occurrence information (mutual pointwise information). At the heart of the sofware is the notion of pattern models . The core tool, to be used from the command-line, is colibri-patternmodeller which enables you to build pattern models, generate statistical reports, query for specific patterns and relations, and manipulate models. A pattern model is simply a collection of extracted patterns (any of the three categories) and their counts from a specific corpus. Pattern models come in two varieties: Unindexed Pattern Model -- The simplest form, which simply stores the patterns and their count; Indexed Pattern Model -- The more informed form, which retains all indices to the original corpus, at the cost of more memory/diskspace. The Indexed Pattern Model is much more powerful, and allows more statistics and relations to be inferred. The generation of pattern models is optionally parametrised by a minimum occurrence threshold, a maximum pattern length, and a lower-boundary on the different types that may instantiate a skipgram (i.e. possible fillings of the gaps).","title":"Learn"},{"location":"tools/colibricore/#distributions","text":"Colibri Core is available as a collection of standalone command-line tools , as a C++ library , and as a Python library .","title":"Distributions"},{"location":"tools/colibricore/#documentation-and-resources","text":"Colibri Core is well documented in the following places: van Gompel, M., & van den Bosch, A. (2016). Efficient n-gram, Skipgram and Flexgram Modelling with Colibri Core. Journal of Open Research Software, 4 (1), e30. http://dx.doi.org/10.5334/jors.105 Source code on GitHub Documentation and Python API reference C++ API reference Python Tutorial (ipython notebook)","title":"Documentation and resources"},{"location":"tools/colibricore/#user-support","text":"Colibri Core was developed by Maarten van Gompel at the Centre of Language Studies, Radboud University Nijmegen, under supervision of Antal van den Bosch.","title":"User support"},{"location":"tools/colibricore/#mentions","text":"","title":"Mentions"},{"location":"tools/colibricore/#key-publications","text":"van Gompel, M., & van den Bosch, A. (2016). Efficient n-gram, Skipgram and Flexgram Modelling with Colibri Core. Journal of Open Research Software, 4 (1), e30. http://dx.doi.org/10.5334/jors.105","title":"Key publications"},{"location":"tools/colibricore/#webpages","text":"Source code on GitHub Documentation and Python API reference C++ API reference Python Tutorial (ipython notebook)","title":"Webpages"},{"location":"tools/corpus-frontend/","text":"INT Corpus Frontend INT Corpus Frontend is a corpus search application that works with BlackLab Server. At the Dutch Language Institute, it i used to publish corpora such as CHN (CLARIN login required), Letters as Loot and AutoSearch (CLARIN login required). Overview Learn Mentions","title":"INT Corpus Frontend"},{"location":"tools/corpus-frontend/#int-corpus-frontend","text":"INT Corpus Frontend is a corpus search application that works with BlackLab Server. At the Dutch Language Institute, it i used to publish corpora such as CHN (CLARIN login required), Letters as Loot and AutoSearch (CLARIN login required).","title":"INT Corpus Frontend"},{"location":"tools/corpus-frontend/#overview","text":"","title":"Overview"},{"location":"tools/corpus-frontend/#learn","text":"","title":"Learn"},{"location":"tools/corpus-frontend/#mentions","text":"","title":"Mentions"},{"location":"tools/cow/","text":"CoW CoW (CSV on the Web) is a conversion tool that transposes tabular datasets in CSV to Linked Data in RDF-format. Overview CoW is a comprehensive and high-performance tool for batch conversion of multiple datasets expressed in CSV to RDF (Linked Data). In a first step, CoW generates a JSON schema file from the input CSV, expressed using an extended version of the W3C standard CSVW, which can be manually adjusted by the user to accommodate their needs: selecting specific columns, creating new virtual columns, combining the values of different columns to mint URIs, etc. In a scond step, CoW uses the instructions in this JSON schema file to build a RDF file correspondingly. CoW uses the W3C standard CSVW for rich semantic table specifications, and nanopublications as an output RDF model. CoW uses a command line interface (CLI) for Python scripts. Instead of using the command line tool there is also the webservice cattle, providing the same functionality that CoW provides without having to install it. However, note that there's a limit to the size of the CSVs you can upload to cattle, conversion could take longer and cattle is no longer being maintained will eventually be taken offline. Learn Instruction webpages The ReadMe-section shows which commands to use if you want to use CoW in the most straightforward way. Read a more elaborate explanation of CoW\u2019s usage . The CoW wiki explains how to augment the JSON-schema (with several examples) produced by CoW in the first step of convertion. The developers have organized workshops to show the usage of CoW. Take a look at the workshops slides. Instruction videos General instruction CoW Basic Conversion . In this tutorial (in Dutch) you will see the most basic conversion of a .csv file into triples (RDF) using CoW. For more information on CoW goto https://github.com/clariah/cow/wiki or raise a question via https://github.com/clariah/cow/issues (and click the 'new issue' button). CoW Change Base Uri . In this tutorial (in Dutch) on CoW, it is explained how to create triples using your own domain as the base URI. For more information on CoW goto https://github.com/clariah/cow/wiki or raise a question via https://github.com/clariah/cow/issues (and click the 'new issue' button). Introduction Cliopatria TripleStore . In this tutorial it is shown how to setup a Triplestore on your machine. For more information or support visit: https://cliopatria.swi-prolog.org/swish/pldoc/doc/home/swipl/src/ClioPatria/ClioPatria/web/help/Download.txt Tutorial Tutorial for Cow part 1 and part 2 . Workshops and courses For master and PhD-students within economic and social history, we organize a yearly course Data management for historians. This course lets students work according to the FAIR data principles, instructs them on the basics of quantitative data management, and lets students make and query their own Linked Data. The course consists of 8 weeks, during which experts in the field discuss: The principles of FAIR data Data bases and scripts Pipelines and data cleaning Data visualization The quantitative research cycle The principles of Linked Data Querying Linked Data Download the Data management for historians full course description as pdf . Apply via the Application Form . For questions and more information, contact course coordinator Dr. Rick Mourits . M\ufeffentions Articles (incl. conference papers, presentations and demo\u2019s) Ashkan Ashkpour. \u201cTheory and Practice of Historical Census Data Harmonization. The Dutch Historical Census Use Case: A flexible, structured and accountable approach using Linked Data. Erasmus University (2019). PhD Thesis. Albert Mero\u00f1o-Pe\u00f1uela, Marnix van Berchum, Bram van den Hout. \u201cThe Oldest Song Score in the Newest Notation: The Hurrian Hymn to Nikkal as Linked Data\u201d. Digital Humanities Conference (DH2019), Utrecht, July 9-12 (2019) Albert Mero\u00f1o-Pe\u00f1uela, Ashkan Ashkpour, Richard Zijdeman, Rinke Hoekstra. \u201cMaking Social Science More Reproducible by Encapsulating Access to Linked Data\u201d. In: European Social Science History Conference (ESSHC 2018), 4-7 April, Belfast, North Ireland, UK (2018) Albert Mero\u00f1o-Pe\u00f1uela, Ashkan Ashkpour, Valentijn Gilissen, Jan Jonker, Tom Vreugdenhil, Peter Doorn. \u201cImproving Access to the Dutch Historical Censuses with Linked Open Data\u201d. Research Data Journal for the Humanities and Social Sciences (in print) (2018) Ashkan Ashkpour, Albert Mero\u00f1o-Pe\u00f1uela. \u201cLOCS AND KEYS: Linked Open Classification System and Opening up Knowledge\u201d. 12th European Social Science History Conference. Belfast United Kingdom (2018). Rinke Hoekstra, Albert Mero\u00f1o-Pe\u00f1uela, Auke Rijpma, Richard Zijdeman, Ashkan Ashkpour, Kathrin Dentler, Ivo Zandhuis, Laurens Rietveld. \u201cThe dataLegend Ecosystem for Historical Statistics\u201d. Journal of Web Semantics: Science, Services and Agents on the World Wide Web, volume 50, pp. 49-61 (2018) Tobias Kuhn, Albert Mero\u00f1o-Pe\u00f1uela, Alexander Malic, Jorrit H. Poelen, Allen H. Hurlbert, Emilio Centeno Ortiz, Laura I. Furlong, N\u00faria Queralt-Rosinach, Christine Chichester, Juan M. Banda, Egon Willighagen, Friederike Ehrhart, Chris Evelo, Tareq B. Malas, Michel Dumontier. \u201cNanopublications: A Growing Resource of Provenance-Centric Scientific Linked Data\u201d. IEEE eScience Conference 2018, 29 October \u2013 1 November, Amsterdam, The Netherlands (2018) Ruben Schalk, Auke Rijpma & Richard Zijdeman, \u2018Clariah Datalegend: Linked Economic and Social History Datasets in the Cloud\u2019, World Economic History Conference, Boston (August 2018) Ashkan Ashkpour. LICR Classification System for Religions. (2017). https://datasets.socialhistory.org/dataverse/LICR Ashkan Ashkpour. http://www.licr.io An interface linking to different tools and systems in CLARIAH WP4. (2017). Ashkan Ashkpour. QBer Demonstration, CLARIAH Tech Dag, Utrecht 7 oktober 2016. http://www.clariah.nl/evenementen/tech-dag-2016 . Ashkpour, A., Mandemakers, K., & Boonstra, O. W. A. (2016). Source Oriented Harmonization of Aggregate Historical Census Data: a flexible and accountable approach in RDF. Historical Social Research. 41(4) pp. 296-307. Ashkan Ashkpour and Albert Mero\u00f1o Pe\u00f1uela. Linked Classifications Systems and Religion. Conference: SSHA, Montreal, Canada, November 2-5, 2017. Antal van den Bosch, Lex Heerma van Voss, Karina van Dalen-Oskam and Richard L. Zijdeman, 2017. Panel Digital Humanities. [online] Available at: https://www.eventbrite.nl/e/tickets-knaw-humanities-cluster-opening-29740908859 Francesca Ceolan, Dimitris Alivanistos, Kathrin Dentler, and Auke Rijpma. 2017. Artificial Intelligence and Simulation of Behaviour Convention. The benefits of Linked Data for the Social Sciences. Analyzing economic drivers and network effects of international migration based on semantically integrated data. Forthcoming. Rinke Hoekstra QBer \u2013 Connect your data to the cloud Rinke Hoekstra and Mero\u00f1o-Pe\u00f1uela. QBer \u2013 Crowd Based Coding and Harmonization using Linked Data Albert Mero\u00f1o Pe\u00f1uela, Ashkan Ashkpour, Actionable Data Links: Tools for Reproducibility in Social Science and History. Conference: SSHA, Montreal, Canada, November 2-5, 2017. Rinke Hoekstra & Stefan Slobach Knowledge Representation on the Web Victor de Boer, Albert Mero\u00f1o-Pe\u00f1uela, Niels Ockeloen. \u201cLinked Data for Digital History. Lessons Learned from Three Case Studies\u201d. Mirella Romer Recio, M. Jes\u00fas Colmenero (eds.). Historiograf\u00eda digital: proyectos para almacenar y construir la Historia. Anejos de la Revista de Historiograf\u00eda 4. Universidad Carlos III de Madrid (2016). (PDF) Rinke Hoekstra, Albert Mero\u00f1o-Pe\u00f1uela, Kathrin Dentler, Auke Rijpma, Richard Zijdeman & Ivo Zandhuis An Ecosystem for Linked Humanities Data Albert Mero\u00f1o-Pe\u00f1uela, Ashkan Ashkpour. \u201cHistorical Quantitative Reasoning on the Web\u201d. European Social Science History Conference (ESSHC 2016), Valencia, Spain (2016). (PDF) Albert Mero\u00f1o-Pe\u00f1uela, Rinke Hoekstra. \u201cgrlc Makes GitHub Taste Like Linked Data APIs\u201d. SALAD 2016 \u2014 Services and Applications over Linked Data APIs and Data. International workshop, ESWC 2016, May 29th, Heraklion, Crete, Greece (2016). (PDF) Best SALAD2016 paper award Rinke Hoekstra, Albert Mero\u00f1o-Pe\u00f1uela, Kathrin Dentler, Auke Rijpma, Richard Zijdeman, Ivo Zandhuis. \u201cAn Ecosystem for Linked Humanities Data\u201d. In: Proceedings of the 1st Workshop on Humanities in the SEmantic web (WHiSE 2016). ESWC 2016, May 29th, Heraklion, Crete, Greece (2016). (PDF) Best WHiSE2016 paper award Albert Mero\u00f1o-Pe\u00f1uela. \u201cRefining Statistical Data on the Web\u201d. Vrije Universiteit Amsterdam (2016) (Amazon) (VU-DARE) PhD thesis Ashkan Ashkpour, Albert Mero\u00f1o-Pe\u00f1uela, Kees Mandemakers. \u201cThe Aggregate Dutch Historical Censuses: Harmonization and RDF\u201d. Historical Methods: A Journal of Quantitative and Interdisciplinary History, 48(4), pp. 230-245, 2015. (PDF) Albert Mero\u00f1o-Pe\u00f1uela, Ashkan Ashkpour, Andrea Scharnhorst, Christophe Gu\u00e9ret, Sally Wyatt. \u201cLinked Open Census Data\u201d. DHCommons Journal, 1st issue. (PDF) (HTML) Albert Mero\u00f1o-Pe\u00f1uela, Christophe Gu\u00e9ret, Stefan Schlobach. \u201cLinked Edit Rules: A Web Friendly Way of Checking Quality of RDF Data Cubes\u201d. Proceedings of the 3rd International Workshop on Semantic Statistics (SemStats 2015), ISWC 2015, Bethlehem, PA, USA (2015). (PDF) Bas Stringer, Albert Mero\u00f1o-Pe\u00f1uela, Antonis Loizou, Sanne Abeln, Jaap Heringa. \u201cTo SCRY Linked Data: Extending SPARQL the Easy Way\u201d. Diversity++ workshop, ISWC 2015, Bethlehem, PA, USA (2015). (PDF) Albert Mero\u00f1o-Pe\u00f1uela, Ashkan Ashkpour, Marieke van Erp, Kees Mandemakers, Leen Breure, Andrea Scharnhorst, Stefan Schlobach, Frank van Harmelen. \u201cSemantic Technologies for Historical Research: A Survey\u201d. Semantic Web \u2014 Interoperability, Usability, Applicability, 6(6), pp. 539\u2013564. IOS Press (2015). http://www.semantic-web-journal.net/content/semantic-technologies-historical-research-survey-0 (PDF) Albert Mero\u00f1o-Pe\u00f1uela, Ashkan Ashkpour, Christophe Gu\u00e9ret, Stefan Schlobach. \u201cCEDAR: The Dutch Historical Censuses as Linked Open Data\u201d. Semantic Web \u2014 Interoperability, Usability, Applicability, 8(2), pp. 297\u2013310. IOS Press (2015). http://semantic-web-journal.net/content/cedar-dutch-historical-censuses-linked-open-data-1 (PDF) Albert Mero\u00f1o-Pe\u00f1uela, Rinke Hoekstra. \u201cThe Song Remains the Same: Lossless Conversion and Streaming of MIDI to RDF and Back\u201d. In: 13th Extended Semantic Web Conference (ESWC 2016), posters and demos track. May 29th \u2014 June 2nd, Heraklion, Crete, Greece (2016). (PDF) Albert Mero\u00f1o-Pe\u00f1uela, Rinke Hoekstra. \u201cSPARQL2Git: Transparent SPARQL and Linked Data API Curation via Git\u201d. In: 14th Extended Semantic Web Conference (ESWC 2017), posters and demos track. (under review) (2017) Laurens Rietveld, Rinke Hoekstra. 2015. The YASGUI family of SPARQL clients1. Semantic Web:1-11. IOS Press. Laurens Rietveld, Wouter Beek, Stefan Schlobach, Rinke Hoekstra. 2016. Semantic Web. [PDF] from semantic-web-journal.org Auke Rijpma. WP4-demo, Clariah Toogdag 2017, Amsterdam Auke Rijpma. Presentation \u201cLinked data in the Clariah Structured Datahub\u201d, CREATE Salon, UvA Auke Rijpma. Presentatie \u201cBrede Welvaartsindicator: Een integrale indicator voor het welbevinden van Nederlanders\u201d. Auke Rijpma. Presentation \u201cLinked data in the Clariah Structured Datahub\u201d, Global social science research workshop, Pittsburgh Auke Rijpma. (met Tine de Moor) \u2013 Wat is citizen science en wat moeten we er mee?, KNAW-symposium citizen science. De betrokkenheid van burgers in het wetenschappelijke proces Auke Rijpma. Presentation \u201cWP4: structured data\u201d, CLARIAH-dag, Amersfoort Auke Rijpma. \u201cCombining multiple repositories: the case of the quantity-quality trade-off\u201d. Big Questions, Big Data Conference, International Institute of Social History, Amsterdam. Auke Rijpma. \u201cWhat can\u2019t money buy? Wellbeing and GDP since 1820\u201d. World Economic History Congress (WEHC), Kyoto. Auke Rijpma. \u201cQuantity versus Quality: Household structure, number of siblings, and educational attainment in the long nineteenth century\u201d, WEHC, Kyoto. (With Sarah Carmichael and Lotte van der Vleuten.) Auke Rijpma. \u201cThe Clariah-project and the quantity-quality trade-off : understanding household size and investment in human capital through big data\u201d, Posthumus Conference, Brussels. Auke Rijpma. \u201cFamily constraints on women\u2019s agency: measurement and persistence\u201d, Workshop Murdock and Goody Re-visited: (Pre)history and evolution of Eurasian and African family systems. Max Planck Institute for Social Anthropology, Halle. Veruska Zamborlini, Rinke Hoekstra, Marcos da Silveira, Cedric Pruski, Annette ten Teije, Frank van Harmelen. 2016. Generalizing the Detection of Clinical Guideline Interactions Enhanced with LOD. International Joint Conference on Biomedical Engineering Systems and Technologies: 360-386. Springer, Cham. Awards and grants Auke Rijpma, Kathrin Dentler, Rinke Hoekstra, Albert Mero\u00f1o-Penuela & Richard Zijdeman. TRUMP: The RDF Unified Migration Portal. Scholarship for a research master. Albert Mero\u00f1o-Pe\u00f1uela, Rinke Hoekstra. \u201cgrlc Makes GitHub Taste Like Linked Data APIs\u201d. SALAD 2016 \u2014 Services and Applications over Linked Data APIs and Data. International workshop, ESWC 2016, May 29th, Heraklion, Crete, Greece (2016). (PDF) Best SALAD2016 paper award Marieke van Erp & Richard Zijdeman. 2016. \u201cIf buildings could talk\u201d. Audience Award for demo build during the Royal Library and National Digital Heritage Hack-a-LOD. [PDF] Rinke Hoekstra, Albert Mero\u00f1o-Pe\u00f1uela, Kathrin Dentler, Auke Rijpma, Richard Zijdeman, Ivo Zandhuis. \u201cAn Ecosystem for Linked Humanities Data\u201d. In: Proceedings of the 1st Workshop on Humanities in the SEmantic web (WHiSE 2016). ESWC 2016, May 29th, Heraklion, Crete, Greece (2016). (PDF) Best WHiSE2016 paper award. Richard Zijdeman & Ashkan Ashkpour. KDP DANS, k\u20ac10. (2017). Project name: Linking past and present: Augmenting historical municipality characteristics through harmonization and linkage with contemporary data. Richard Zijdeman, Antske Fokkens, Auke Rijpma & Martijn Kleppe. CLARIAH research call, k\u20ac55. (t.b.c. April 2017). HHuCap: The History of Human Capital. Books Auke Rijpma. Cliometrics and the family: global patterns and diverging development. Berlin: Springer Verlag, forthcoming. (With Claude Diebolt, Sarah Carmichael, Selin Dilli, Charlotte St\u00f6rmer, eds.) Auke Rijpma. Agency, gender, and economic development in the world economy, 1850\u20132000: testing the Sen hypothesis. Forthcoming at Routledge/Ashgate Publishing. (With Jan Luiten van Zanden en Jan Kok, eds.) Book chapters Auke Rijpma. \u201cQuantity versus Quality: Household structure, number of siblings, and educational attainment in the long nineteenth century\u201d. Forthcoming in Van Zanden, Kok and Rijpma eds. Agency, gender, and economic development in the world economy, 1850\u20132000: testing the Sen hypothesis (with Sarah Carmichael and Lotte van der Vleuten). Auke Rijpma. \u201cMeasuring agency\u201d. Forthcoming in Van Zanden, Kok and Rijpma eds. Agency, gender, and economic development in the world economy, 1850\u20132000: testing the Sen hypothesis (with Sarah Carmichael). Auke Rijpma. \u201cWomen in global economic history\u201d. In Joerg Baten (ed.), A History of the Global Economy: From 1500 to the Present (Cambridge: Cambridge University Press). (With Selin Dilli and Sarah Carmichael.) Projects Auke Rijpma. Record linkage for Cape of Good Hope Panel. Teaching and Instruction Paul S. Lambert and Richard L. Zijdeman, 2015. Introduction to Multilevel Models. Paul S. Lambert and Richard L. Zijdeman, 2016. Introduction to Multilevel Models. Auke Rijpma. Introduction R and record linkage at LEAP-conference, South Africa Auke Rijpma. Introduction Linked Data and SPARQL @ Amsterdam ThatCamp Richard L. Zijdeman, 2016d. Augmenting Historical Data. [online] Available at: https://arthist.net/archive/13218/view=pdf Richard L. Zijdeman, 2016e. Course: Introduction into R. Richard L. Zijdeman, 2015b. Workshop: Introduction to R. [online] Available at: http://www.ehps-net.eu/sites/default/files/program_cluj_summer_school_2015.pdf Richard L. Zijdeman, 2017b. Historical Occupational Classification and Stratification Schemes: HISCO, HISCLASS & HISCAM. [online] Available at: http://iegd.csic.es/sites/default/files/content/event/2017/programa_del_ciclofbbva2017final_1.pdf Richard L. Zijdeman, 2015a. The not so U-shaped curve of female labour force participation of married women: The United States, 1860-2010. [online] Available at: https://pure.knaw.nl/portal/files/1011394/ushp_lund02.pdf Richard L. Zijdeman, 2016a. Advancing the comparability of occupations through Linked Open Data. [online] Available at: http://www.ed.lu.se/previous_seminars Richard L. Zijdeman, 2016b. Advancing the comparability of occupations through Linked Open Data. [online] Available at: http://www.ed.lu.se/previous_seminars Richard L. Zijdeman, 2016c. Advancing the comparability of occupations through Linked Open Data. [online] Available at: http://www.cedar.umu.se/digitalAssets/176/176124_agenda-ehps-net-cedar-database-workshop-16-18-feb-2016.pdf Richard L. Zijdeman, 2016f. Historical Demography: Reconstructing Life Course Dynamics. [online] Available at: http://www.slideshare.net/rlzijdeman/historical-occupational-classification-and-occupational-stratification-schemes Richard L. Zijdeman, 2016g. The not so U-shaped Curve of Female Labour Force Participation of Married Women: the United States, 1860-2010. [online] Available at: https://www.slideshare.net/rlzijdeman/labour-force-participation-of-married-women-us-18602010 Richard L. Zijdeman, 2017a. HISCO to RDF. Anticipating expansion of occupational descriptions and tooling. Invited Lecture at Stirling University. 13-01-2017. Richard L. Zijdeman, 2017c. WP4: dataLegend (project update). [online] Available at: https://www.slideshare.net/rlzijdeman/toogdag-2017 Richard L. Zijdeman, Antske Fokkens, Auke Rijpma and Martijn Kleppe, 2017. HHuCap: The History of HUman Capital. [online] Available at: https://www.clariah.nl/evenementen/toog-dag-2017#research-pilot-presentations Richard L. Zijdeman and Marieke van Erp, 2016. If Buildings Could Talk: Constructing Buildings\u2019 Biographies. [online] Available at: http://www.pilod.nl/wiki/LinkedDataSeminar-December2,2016 Richard L. Zijdeman and Rombert Stapel, 2016. Work in a globalized world. An algorithm allocating labour relations to digitized census data. [online] Available at: http://dh2016.adho.org/ * Rombert Stapel and Richard L. Zijdeman, 2016. The Influence of Educational Attainment on Self-Employment across Occupational Sectors: United States, 1850-2010. [online] Available at: https://socialhistory.org/en/events/workshop-self-employment-historical-perspective","title":"CoW"},{"location":"tools/cow/#cow","text":"CoW (CSV on the Web) is a conversion tool that transposes tabular datasets in CSV to Linked Data in RDF-format.","title":"CoW"},{"location":"tools/cow/#overview","text":"CoW is a comprehensive and high-performance tool for batch conversion of multiple datasets expressed in CSV to RDF (Linked Data). In a first step, CoW generates a JSON schema file from the input CSV, expressed using an extended version of the W3C standard CSVW, which can be manually adjusted by the user to accommodate their needs: selecting specific columns, creating new virtual columns, combining the values of different columns to mint URIs, etc. In a scond step, CoW uses the instructions in this JSON schema file to build a RDF file correspondingly. CoW uses the W3C standard CSVW for rich semantic table specifications, and nanopublications as an output RDF model. CoW uses a command line interface (CLI) for Python scripts. Instead of using the command line tool there is also the webservice cattle, providing the same functionality that CoW provides without having to install it. However, note that there's a limit to the size of the CSVs you can upload to cattle, conversion could take longer and cattle is no longer being maintained will eventually be taken offline.","title":"Overview"},{"location":"tools/cow/#learn","text":"","title":"Learn"},{"location":"tools/cow/#instruction-webpages","text":"The ReadMe-section shows which commands to use if you want to use CoW in the most straightforward way. Read a more elaborate explanation of CoW\u2019s usage . The CoW wiki explains how to augment the JSON-schema (with several examples) produced by CoW in the first step of convertion. The developers have organized workshops to show the usage of CoW. Take a look at the workshops slides.","title":"Instruction webpages"},{"location":"tools/cow/#instruction-videos","text":"General instruction CoW Basic Conversion . In this tutorial (in Dutch) you will see the most basic conversion of a .csv file into triples (RDF) using CoW. For more information on CoW goto https://github.com/clariah/cow/wiki or raise a question via https://github.com/clariah/cow/issues (and click the 'new issue' button). CoW Change Base Uri . In this tutorial (in Dutch) on CoW, it is explained how to create triples using your own domain as the base URI. For more information on CoW goto https://github.com/clariah/cow/wiki or raise a question via https://github.com/clariah/cow/issues (and click the 'new issue' button). Introduction Cliopatria TripleStore . In this tutorial it is shown how to setup a Triplestore on your machine. For more information or support visit: https://cliopatria.swi-prolog.org/swish/pldoc/doc/home/swipl/src/ClioPatria/ClioPatria/web/help/Download.txt","title":"Instruction videos"},{"location":"tools/cow/#tutorial","text":"Tutorial for Cow part 1 and part 2 .","title":"Tutorial"},{"location":"tools/cow/#workshops-and-courses","text":"For master and PhD-students within economic and social history, we organize a yearly course Data management for historians. This course lets students work according to the FAIR data principles, instructs them on the basics of quantitative data management, and lets students make and query their own Linked Data. The course consists of 8 weeks, during which experts in the field discuss: The principles of FAIR data Data bases and scripts Pipelines and data cleaning Data visualization The quantitative research cycle The principles of Linked Data Querying Linked Data Download the Data management for historians full course description as pdf . Apply via the Application Form . For questions and more information, contact course coordinator Dr. Rick Mourits .","title":"Workshops and courses"},{"location":"tools/cow/#mentions","text":"","title":"M\ufeffentions"},{"location":"tools/cow/#articles-incl-conference-papers-presentations-and-demos","text":"Ashkan Ashkpour. \u201cTheory and Practice of Historical Census Data Harmonization. The Dutch Historical Census Use Case: A flexible, structured and accountable approach using Linked Data. Erasmus University (2019). PhD Thesis. Albert Mero\u00f1o-Pe\u00f1uela, Marnix van Berchum, Bram van den Hout. \u201cThe Oldest Song Score in the Newest Notation: The Hurrian Hymn to Nikkal as Linked Data\u201d. Digital Humanities Conference (DH2019), Utrecht, July 9-12 (2019) Albert Mero\u00f1o-Pe\u00f1uela, Ashkan Ashkpour, Richard Zijdeman, Rinke Hoekstra. \u201cMaking Social Science More Reproducible by Encapsulating Access to Linked Data\u201d. In: European Social Science History Conference (ESSHC 2018), 4-7 April, Belfast, North Ireland, UK (2018) Albert Mero\u00f1o-Pe\u00f1uela, Ashkan Ashkpour, Valentijn Gilissen, Jan Jonker, Tom Vreugdenhil, Peter Doorn. \u201cImproving Access to the Dutch Historical Censuses with Linked Open Data\u201d. Research Data Journal for the Humanities and Social Sciences (in print) (2018) Ashkan Ashkpour, Albert Mero\u00f1o-Pe\u00f1uela. \u201cLOCS AND KEYS: Linked Open Classification System and Opening up Knowledge\u201d. 12th European Social Science History Conference. Belfast United Kingdom (2018). Rinke Hoekstra, Albert Mero\u00f1o-Pe\u00f1uela, Auke Rijpma, Richard Zijdeman, Ashkan Ashkpour, Kathrin Dentler, Ivo Zandhuis, Laurens Rietveld. \u201cThe dataLegend Ecosystem for Historical Statistics\u201d. Journal of Web Semantics: Science, Services and Agents on the World Wide Web, volume 50, pp. 49-61 (2018) Tobias Kuhn, Albert Mero\u00f1o-Pe\u00f1uela, Alexander Malic, Jorrit H. Poelen, Allen H. Hurlbert, Emilio Centeno Ortiz, Laura I. Furlong, N\u00faria Queralt-Rosinach, Christine Chichester, Juan M. Banda, Egon Willighagen, Friederike Ehrhart, Chris Evelo, Tareq B. Malas, Michel Dumontier. \u201cNanopublications: A Growing Resource of Provenance-Centric Scientific Linked Data\u201d. IEEE eScience Conference 2018, 29 October \u2013 1 November, Amsterdam, The Netherlands (2018) Ruben Schalk, Auke Rijpma & Richard Zijdeman, \u2018Clariah Datalegend: Linked Economic and Social History Datasets in the Cloud\u2019, World Economic History Conference, Boston (August 2018) Ashkan Ashkpour. LICR Classification System for Religions. (2017). https://datasets.socialhistory.org/dataverse/LICR Ashkan Ashkpour. http://www.licr.io An interface linking to different tools and systems in CLARIAH WP4. (2017). Ashkan Ashkpour. QBer Demonstration, CLARIAH Tech Dag, Utrecht 7 oktober 2016. http://www.clariah.nl/evenementen/tech-dag-2016 . Ashkpour, A., Mandemakers, K., & Boonstra, O. W. A. (2016). Source Oriented Harmonization of Aggregate Historical Census Data: a flexible and accountable approach in RDF. Historical Social Research. 41(4) pp. 296-307. Ashkan Ashkpour and Albert Mero\u00f1o Pe\u00f1uela. Linked Classifications Systems and Religion. Conference: SSHA, Montreal, Canada, November 2-5, 2017. Antal van den Bosch, Lex Heerma van Voss, Karina van Dalen-Oskam and Richard L. Zijdeman, 2017. Panel Digital Humanities. [online] Available at: https://www.eventbrite.nl/e/tickets-knaw-humanities-cluster-opening-29740908859 Francesca Ceolan, Dimitris Alivanistos, Kathrin Dentler, and Auke Rijpma. 2017. Artificial Intelligence and Simulation of Behaviour Convention. The benefits of Linked Data for the Social Sciences. Analyzing economic drivers and network effects of international migration based on semantically integrated data. Forthcoming. Rinke Hoekstra QBer \u2013 Connect your data to the cloud Rinke Hoekstra and Mero\u00f1o-Pe\u00f1uela. QBer \u2013 Crowd Based Coding and Harmonization using Linked Data Albert Mero\u00f1o Pe\u00f1uela, Ashkan Ashkpour, Actionable Data Links: Tools for Reproducibility in Social Science and History. Conference: SSHA, Montreal, Canada, November 2-5, 2017. Rinke Hoekstra & Stefan Slobach Knowledge Representation on the Web Victor de Boer, Albert Mero\u00f1o-Pe\u00f1uela, Niels Ockeloen. \u201cLinked Data for Digital History. Lessons Learned from Three Case Studies\u201d. Mirella Romer Recio, M. Jes\u00fas Colmenero (eds.). Historiograf\u00eda digital: proyectos para almacenar y construir la Historia. Anejos de la Revista de Historiograf\u00eda 4. Universidad Carlos III de Madrid (2016). (PDF) Rinke Hoekstra, Albert Mero\u00f1o-Pe\u00f1uela, Kathrin Dentler, Auke Rijpma, Richard Zijdeman & Ivo Zandhuis An Ecosystem for Linked Humanities Data Albert Mero\u00f1o-Pe\u00f1uela, Ashkan Ashkpour. \u201cHistorical Quantitative Reasoning on the Web\u201d. European Social Science History Conference (ESSHC 2016), Valencia, Spain (2016). (PDF) Albert Mero\u00f1o-Pe\u00f1uela, Rinke Hoekstra. \u201cgrlc Makes GitHub Taste Like Linked Data APIs\u201d. SALAD 2016 \u2014 Services and Applications over Linked Data APIs and Data. International workshop, ESWC 2016, May 29th, Heraklion, Crete, Greece (2016). (PDF) Best SALAD2016 paper award Rinke Hoekstra, Albert Mero\u00f1o-Pe\u00f1uela, Kathrin Dentler, Auke Rijpma, Richard Zijdeman, Ivo Zandhuis. \u201cAn Ecosystem for Linked Humanities Data\u201d. In: Proceedings of the 1st Workshop on Humanities in the SEmantic web (WHiSE 2016). ESWC 2016, May 29th, Heraklion, Crete, Greece (2016). (PDF) Best WHiSE2016 paper award Albert Mero\u00f1o-Pe\u00f1uela. \u201cRefining Statistical Data on the Web\u201d. Vrije Universiteit Amsterdam (2016) (Amazon) (VU-DARE) PhD thesis Ashkan Ashkpour, Albert Mero\u00f1o-Pe\u00f1uela, Kees Mandemakers. \u201cThe Aggregate Dutch Historical Censuses: Harmonization and RDF\u201d. Historical Methods: A Journal of Quantitative and Interdisciplinary History, 48(4), pp. 230-245, 2015. (PDF) Albert Mero\u00f1o-Pe\u00f1uela, Ashkan Ashkpour, Andrea Scharnhorst, Christophe Gu\u00e9ret, Sally Wyatt. \u201cLinked Open Census Data\u201d. DHCommons Journal, 1st issue. (PDF) (HTML) Albert Mero\u00f1o-Pe\u00f1uela, Christophe Gu\u00e9ret, Stefan Schlobach. \u201cLinked Edit Rules: A Web Friendly Way of Checking Quality of RDF Data Cubes\u201d. Proceedings of the 3rd International Workshop on Semantic Statistics (SemStats 2015), ISWC 2015, Bethlehem, PA, USA (2015). (PDF) Bas Stringer, Albert Mero\u00f1o-Pe\u00f1uela, Antonis Loizou, Sanne Abeln, Jaap Heringa. \u201cTo SCRY Linked Data: Extending SPARQL the Easy Way\u201d. Diversity++ workshop, ISWC 2015, Bethlehem, PA, USA (2015). (PDF) Albert Mero\u00f1o-Pe\u00f1uela, Ashkan Ashkpour, Marieke van Erp, Kees Mandemakers, Leen Breure, Andrea Scharnhorst, Stefan Schlobach, Frank van Harmelen. \u201cSemantic Technologies for Historical Research: A Survey\u201d. Semantic Web \u2014 Interoperability, Usability, Applicability, 6(6), pp. 539\u2013564. IOS Press (2015). http://www.semantic-web-journal.net/content/semantic-technologies-historical-research-survey-0 (PDF) Albert Mero\u00f1o-Pe\u00f1uela, Ashkan Ashkpour, Christophe Gu\u00e9ret, Stefan Schlobach. \u201cCEDAR: The Dutch Historical Censuses as Linked Open Data\u201d. Semantic Web \u2014 Interoperability, Usability, Applicability, 8(2), pp. 297\u2013310. IOS Press (2015). http://semantic-web-journal.net/content/cedar-dutch-historical-censuses-linked-open-data-1 (PDF) Albert Mero\u00f1o-Pe\u00f1uela, Rinke Hoekstra. \u201cThe Song Remains the Same: Lossless Conversion and Streaming of MIDI to RDF and Back\u201d. In: 13th Extended Semantic Web Conference (ESWC 2016), posters and demos track. May 29th \u2014 June 2nd, Heraklion, Crete, Greece (2016). (PDF) Albert Mero\u00f1o-Pe\u00f1uela, Rinke Hoekstra. \u201cSPARQL2Git: Transparent SPARQL and Linked Data API Curation via Git\u201d. In: 14th Extended Semantic Web Conference (ESWC 2017), posters and demos track. (under review) (2017) Laurens Rietveld, Rinke Hoekstra. 2015. The YASGUI family of SPARQL clients1. Semantic Web:1-11. IOS Press. Laurens Rietveld, Wouter Beek, Stefan Schlobach, Rinke Hoekstra. 2016. Semantic Web. [PDF] from semantic-web-journal.org Auke Rijpma. WP4-demo, Clariah Toogdag 2017, Amsterdam Auke Rijpma. Presentation \u201cLinked data in the Clariah Structured Datahub\u201d, CREATE Salon, UvA Auke Rijpma. Presentatie \u201cBrede Welvaartsindicator: Een integrale indicator voor het welbevinden van Nederlanders\u201d. Auke Rijpma. Presentation \u201cLinked data in the Clariah Structured Datahub\u201d, Global social science research workshop, Pittsburgh Auke Rijpma. (met Tine de Moor) \u2013 Wat is citizen science en wat moeten we er mee?, KNAW-symposium citizen science. De betrokkenheid van burgers in het wetenschappelijke proces Auke Rijpma. Presentation \u201cWP4: structured data\u201d, CLARIAH-dag, Amersfoort Auke Rijpma. \u201cCombining multiple repositories: the case of the quantity-quality trade-off\u201d. Big Questions, Big Data Conference, International Institute of Social History, Amsterdam. Auke Rijpma. \u201cWhat can\u2019t money buy? Wellbeing and GDP since 1820\u201d. World Economic History Congress (WEHC), Kyoto. Auke Rijpma. \u201cQuantity versus Quality: Household structure, number of siblings, and educational attainment in the long nineteenth century\u201d, WEHC, Kyoto. (With Sarah Carmichael and Lotte van der Vleuten.) Auke Rijpma. \u201cThe Clariah-project and the quantity-quality trade-off : understanding household size and investment in human capital through big data\u201d, Posthumus Conference, Brussels. Auke Rijpma. \u201cFamily constraints on women\u2019s agency: measurement and persistence\u201d, Workshop Murdock and Goody Re-visited: (Pre)history and evolution of Eurasian and African family systems. Max Planck Institute for Social Anthropology, Halle. Veruska Zamborlini, Rinke Hoekstra, Marcos da Silveira, Cedric Pruski, Annette ten Teije, Frank van Harmelen. 2016. Generalizing the Detection of Clinical Guideline Interactions Enhanced with LOD. International Joint Conference on Biomedical Engineering Systems and Technologies: 360-386. Springer, Cham.","title":"Articles (incl. conference papers, presentations and demo\u2019s)"},{"location":"tools/cow/#awards-and-grants","text":"Auke Rijpma, Kathrin Dentler, Rinke Hoekstra, Albert Mero\u00f1o-Penuela & Richard Zijdeman. TRUMP: The RDF Unified Migration Portal. Scholarship for a research master. Albert Mero\u00f1o-Pe\u00f1uela, Rinke Hoekstra. \u201cgrlc Makes GitHub Taste Like Linked Data APIs\u201d. SALAD 2016 \u2014 Services and Applications over Linked Data APIs and Data. International workshop, ESWC 2016, May 29th, Heraklion, Crete, Greece (2016). (PDF) Best SALAD2016 paper award Marieke van Erp & Richard Zijdeman. 2016. \u201cIf buildings could talk\u201d. Audience Award for demo build during the Royal Library and National Digital Heritage Hack-a-LOD. [PDF] Rinke Hoekstra, Albert Mero\u00f1o-Pe\u00f1uela, Kathrin Dentler, Auke Rijpma, Richard Zijdeman, Ivo Zandhuis. \u201cAn Ecosystem for Linked Humanities Data\u201d. In: Proceedings of the 1st Workshop on Humanities in the SEmantic web (WHiSE 2016). ESWC 2016, May 29th, Heraklion, Crete, Greece (2016). (PDF) Best WHiSE2016 paper award. Richard Zijdeman & Ashkan Ashkpour. KDP DANS, k\u20ac10. (2017). Project name: Linking past and present: Augmenting historical municipality characteristics through harmonization and linkage with contemporary data. Richard Zijdeman, Antske Fokkens, Auke Rijpma & Martijn Kleppe. CLARIAH research call, k\u20ac55. (t.b.c. April 2017). HHuCap: The History of Human Capital.","title":"Awards and grants"},{"location":"tools/cow/#books","text":"Auke Rijpma. Cliometrics and the family: global patterns and diverging development. Berlin: Springer Verlag, forthcoming. (With Claude Diebolt, Sarah Carmichael, Selin Dilli, Charlotte St\u00f6rmer, eds.) Auke Rijpma. Agency, gender, and economic development in the world economy, 1850\u20132000: testing the Sen hypothesis. Forthcoming at Routledge/Ashgate Publishing. (With Jan Luiten van Zanden en Jan Kok, eds.)","title":"Books"},{"location":"tools/cow/#book-chapters","text":"Auke Rijpma. \u201cQuantity versus Quality: Household structure, number of siblings, and educational attainment in the long nineteenth century\u201d. Forthcoming in Van Zanden, Kok and Rijpma eds. Agency, gender, and economic development in the world economy, 1850\u20132000: testing the Sen hypothesis (with Sarah Carmichael and Lotte van der Vleuten). Auke Rijpma. \u201cMeasuring agency\u201d. Forthcoming in Van Zanden, Kok and Rijpma eds. Agency, gender, and economic development in the world economy, 1850\u20132000: testing the Sen hypothesis (with Sarah Carmichael). Auke Rijpma. \u201cWomen in global economic history\u201d. In Joerg Baten (ed.), A History of the Global Economy: From 1500 to the Present (Cambridge: Cambridge University Press). (With Selin Dilli and Sarah Carmichael.)","title":"Book chapters"},{"location":"tools/cow/#projects","text":"Auke Rijpma. Record linkage for Cape of Good Hope Panel.","title":"Projects"},{"location":"tools/cow/#teaching-and-instruction","text":"Paul S. Lambert and Richard L. Zijdeman, 2015. Introduction to Multilevel Models. Paul S. Lambert and Richard L. Zijdeman, 2016. Introduction to Multilevel Models. Auke Rijpma. Introduction R and record linkage at LEAP-conference, South Africa Auke Rijpma. Introduction Linked Data and SPARQL @ Amsterdam ThatCamp Richard L. Zijdeman, 2016d. Augmenting Historical Data. [online] Available at: https://arthist.net/archive/13218/view=pdf Richard L. Zijdeman, 2016e. Course: Introduction into R. Richard L. Zijdeman, 2015b. Workshop: Introduction to R. [online] Available at: http://www.ehps-net.eu/sites/default/files/program_cluj_summer_school_2015.pdf Richard L. Zijdeman, 2017b. Historical Occupational Classification and Stratification Schemes: HISCO, HISCLASS & HISCAM. [online] Available at: http://iegd.csic.es/sites/default/files/content/event/2017/programa_del_ciclofbbva2017final_1.pdf Richard L. Zijdeman, 2015a. The not so U-shaped curve of female labour force participation of married women: The United States, 1860-2010. [online] Available at: https://pure.knaw.nl/portal/files/1011394/ushp_lund02.pdf Richard L. Zijdeman, 2016a. Advancing the comparability of occupations through Linked Open Data. [online] Available at: http://www.ed.lu.se/previous_seminars Richard L. Zijdeman, 2016b. Advancing the comparability of occupations through Linked Open Data. [online] Available at: http://www.ed.lu.se/previous_seminars Richard L. Zijdeman, 2016c. Advancing the comparability of occupations through Linked Open Data. [online] Available at: http://www.cedar.umu.se/digitalAssets/176/176124_agenda-ehps-net-cedar-database-workshop-16-18-feb-2016.pdf Richard L. Zijdeman, 2016f. Historical Demography: Reconstructing Life Course Dynamics. [online] Available at: http://www.slideshare.net/rlzijdeman/historical-occupational-classification-and-occupational-stratification-schemes Richard L. Zijdeman, 2016g. The not so U-shaped Curve of Female Labour Force Participation of Married Women: the United States, 1860-2010. [online] Available at: https://www.slideshare.net/rlzijdeman/labour-force-participation-of-married-women-us-18602010 Richard L. Zijdeman, 2017a. HISCO to RDF. Anticipating expansion of occupational descriptions and tooling. Invited Lecture at Stirling University. 13-01-2017. Richard L. Zijdeman, 2017c. WP4: dataLegend (project update). [online] Available at: https://www.slideshare.net/rlzijdeman/toogdag-2017 Richard L. Zijdeman, Antske Fokkens, Auke Rijpma and Martijn Kleppe, 2017. HHuCap: The History of HUman Capital. [online] Available at: https://www.clariah.nl/evenementen/toog-dag-2017#research-pilot-presentations Richard L. Zijdeman and Marieke van Erp, 2016. If Buildings Could Talk: Constructing Buildings\u2019 Biographies. [online] Available at: http://www.pilod.nl/wiki/LinkedDataSeminar-December2,2016 Richard L. Zijdeman and Rombert Stapel, 2016. Work in a globalized world. An algorithm allocating labour relations to digitized census data. [online] Available at: http://dh2016.adho.org/ * Rombert Stapel and Richard L. Zijdeman, 2016. The Influence of Educational Attainment on Self-Employment across Occupational Sectors: United States, 1850-2010. [online] Available at: https://socialhistory.org/en/events/workshop-self-employment-historical-perspective","title":"Teaching and Instruction"},{"location":"tools/ewald/","text":"title: e-WALD: Electronisch woordenboek van de Achterhoekse en Liemerse dialecten carousel: - /media/taalkaart-wald.jpg - /media/boeken-wald.jpg - /media/dialectkaart-wald.jpg - /media/taalkaart-wald_vindplaatsen.jpg identifier: ewald e-WALD: Electronisch woordenboek van de Achterhoekse en Liemerse dialecten The e-WALD is an online dictionary of Dutch dialects from the regions 'Achterhoek' and 'Liemers'. It stands for Electronisch woordenboek van de Achterhoekse en Liemerse dialecten . Overview As of yet, the e-WALD contains the first five volumes of the Woordenboek van de Achterhoekse en Liemerse dialecten , that was published between 1984 and 2018. Users can query the e-WALD for keywords and concepts, as well as locations (using a placename or a Kloekecode ) and dialects. Together with the e-WBD , e-WGD and the e-WLD , the e-WALD forms a series of electronic dictionaries. About Editing and digitization The first meeting about the digitization of the WALD between the steering group Digitization of Dialect Dictionaries and Lex Schaars and Dirk Jan Eertink took place in August of 2013. A complicating factor was that there was no usable digital version of the volumes published between 1984 and 2004 and we had to work with scanned versions. After the OCR-edited scans were provided structure using a program that immediately corrected common OCR errors, everything still had to be checked 'by hand'; this was done by Kee van Tuinen. Because there are no standardized forms (keywords) in the WALD, they have been added. For the parts published in the e-WALD, this was done by Gregory Metallinos. In the future, the volumes of the WALD that have now been published as well as the volumes that are yet to be published will be included in the e-WALD. Dialect areas The map includes the WALD research area. Although the Oude IJssel is considered a geographical border between the Liemers and the Achterhoek, it has no significance from a dialectological point of view: north and south of the Oude IJssel there is a wide transitional area between Low Saxon in the Achterhoek and Low Franconian in the south of the Liemers. Places The 75 places in Achterhoek and Liemers where material was collected are listed in the map with the places and their place names. In a large number of cases, material from adjacent areas has also been included. This was done to show what is happening across the border. Words often pay little attention to regional, provincial and national boundaries. Research that is limited within those boundaries suggests that the area studied forms a unity. The language of Winterswijk is very similar to that of Vreden in Westphalia; the language of Lobith with that of Millingen. Gorssels and Wilps are similar, as are the dialects of Gelselaar and Markelo. Learn A manual on how to query the e-WALD can be found on the e-WALD webpage . ( This manual is only available in Dutch, however. ) Mentions Together with three other electronic dictionaries, the e-WALD forms a series: * The e-WBD (Electronisch woordenboek van de Brabantse dialecten); * The e-WGD (Electronisch woordenboek van de Gelderse dialecten); * The e-WLD (Electronisch woordenboek van de Limburgse dialecten). The e-WALD was (and is) originally published as an in-press publication. More information on the books and volumes can be found here . The source code that facilitates the Django web applications of the electronic dialect dictionaries can be found on GitHub here . Credits and Contact Information The e-WALD application was developed by Erwin Komen of the Technical Support Group of the Humanities Lab of Radboud University under the supervision of Henk van den Heuvel, with Dirk Jan Eertink digitally making the WALD material suitable for the e-WALD. Thijs Hermsen made the maps. For technical information or issues regarding the e-WALD, you can contact dr. Henk van den Heuvel (H.vandenHeuvel@Let.ru.nl), Director CLST & Head of the Humanities Lab. Regarding the contents of the e-WALD, you can contact prof. dr. Nicoline van der Sijs (post@nicolinevdsijs.nl).","title":"Ewald"},{"location":"tools/ewald/#e-wald-electronisch-woordenboek-van-de-achterhoekse-en-liemerse-dialecten","text":"The e-WALD is an online dictionary of Dutch dialects from the regions 'Achterhoek' and 'Liemers'. It stands for Electronisch woordenboek van de Achterhoekse en Liemerse dialecten .","title":"e-WALD: Electronisch woordenboek van de Achterhoekse en Liemerse dialecten"},{"location":"tools/ewald/#overview","text":"As of yet, the e-WALD contains the first five volumes of the Woordenboek van de Achterhoekse en Liemerse dialecten , that was published between 1984 and 2018. Users can query the e-WALD for keywords and concepts, as well as locations (using a placename or a Kloekecode ) and dialects. Together with the e-WBD , e-WGD and the e-WLD , the e-WALD forms a series of electronic dictionaries.","title":"Overview"},{"location":"tools/ewald/#about","text":"","title":"About"},{"location":"tools/ewald/#editing-and-digitization","text":"The first meeting about the digitization of the WALD between the steering group Digitization of Dialect Dictionaries and Lex Schaars and Dirk Jan Eertink took place in August of 2013. A complicating factor was that there was no usable digital version of the volumes published between 1984 and 2004 and we had to work with scanned versions. After the OCR-edited scans were provided structure using a program that immediately corrected common OCR errors, everything still had to be checked 'by hand'; this was done by Kee van Tuinen. Because there are no standardized forms (keywords) in the WALD, they have been added. For the parts published in the e-WALD, this was done by Gregory Metallinos. In the future, the volumes of the WALD that have now been published as well as the volumes that are yet to be published will be included in the e-WALD.","title":"Editing and digitization"},{"location":"tools/ewald/#dialect-areas","text":"The map includes the WALD research area. Although the Oude IJssel is considered a geographical border between the Liemers and the Achterhoek, it has no significance from a dialectological point of view: north and south of the Oude IJssel there is a wide transitional area between Low Saxon in the Achterhoek and Low Franconian in the south of the Liemers.","title":"Dialect areas"},{"location":"tools/ewald/#places","text":"The 75 places in Achterhoek and Liemers where material was collected are listed in the map with the places and their place names. In a large number of cases, material from adjacent areas has also been included. This was done to show what is happening across the border. Words often pay little attention to regional, provincial and national boundaries. Research that is limited within those boundaries suggests that the area studied forms a unity. The language of Winterswijk is very similar to that of Vreden in Westphalia; the language of Lobith with that of Millingen. Gorssels and Wilps are similar, as are the dialects of Gelselaar and Markelo.","title":"Places"},{"location":"tools/ewald/#learn","text":"A manual on how to query the e-WALD can be found on the e-WALD webpage . ( This manual is only available in Dutch, however. )","title":"Learn"},{"location":"tools/ewald/#mentions","text":"Together with three other electronic dictionaries, the e-WALD forms a series: * The e-WBD (Electronisch woordenboek van de Brabantse dialecten); * The e-WGD (Electronisch woordenboek van de Gelderse dialecten); * The e-WLD (Electronisch woordenboek van de Limburgse dialecten). The e-WALD was (and is) originally published as an in-press publication. More information on the books and volumes can be found here . The source code that facilitates the Django web applications of the electronic dialect dictionaries can be found on GitHub here .","title":"Mentions"},{"location":"tools/ewald/#credits-and-contact-information","text":"The e-WALD application was developed by Erwin Komen of the Technical Support Group of the Humanities Lab of Radboud University under the supervision of Henk van den Heuvel, with Dirk Jan Eertink digitally making the WALD material suitable for the e-WALD. Thijs Hermsen made the maps. For technical information or issues regarding the e-WALD, you can contact dr. Henk van den Heuvel (H.vandenHeuvel@Let.ru.nl), Director CLST & Head of the Humanities Lab. Regarding the contents of the e-WALD, you can contact prof. dr. Nicoline van der Sijs (post@nicolinevdsijs.nl).","title":"Credits and Contact Information"},{"location":"tools/ewbd/","text":"title: e-WBD: Electronisch woordenboek van de Brabantse dialecten carousel: - /media/taalkaart-wbd.jpg - /media/boeken-wbd.jpg - /media/dialectkaart-wbd.jpg identifier: ewbd e-WALD: Electronisch woordenboek van de Brabantse dialecten The e-WBD is an online dictionary of dialects from the Dutch province Noord-Brabant, as well as the Belgian provinces of Antwerpen, Vlaams-Brabant and Brussels. It stands for Electronisch woordenboek van de Brabantse dialecten . Overview The e-WBD contains the three volumes of the Woordenboek van de Brabantse dialecten , that was published between 1967 and 2005. Users can query the e-WBD for keywords and concepts, as well as locations (using a placename or a Kloekecode ). The e-WBD contains 15,794 concepts, 140,091 keywords, and 1,704,116 dialectal entries, collected in over 2950 Brabantian places, with each place corresponding to its own dialect. Together with the e-WALD , e-WGD and the e-WLD , the e-WBD forms a series of electronic dictionaries. About Overview research area The research area for the e-WBD comprises the Dutch province of Noord-Brabant (north),the Belgian province of Antwerpen (middle) and Vlaams-Brabant (south). On the map, another closed area has been indicated in Vlaams-Brabant, the Brussels Capital Region. Brusselian is originally a Brabantian dialect. Brabantian dialects Below is a dialect map showing the Brabant dialect areas. These areas have been used to designate areas in the WBD. An explanation of this classification can be found in WBD Inleiding op deel III: R. Belemans, J. Goossens, Inleiding en klankgeografie van de Brabantse dialecten , Assen 2000 (232p.) ( in Dutch ). That episode included a transparent card that could be placed on top of the printed cards. That transparent map has been converted into the dialect map shown here, in which the larger places are indicated for orientation. The larger dialect areas are demarcated with lines. The further division is indicated in color. Learn A manual on how to query the e-WBD can be found on the e-WBD webpage . ( This manual is only available in Dutch, however. ) Mentions Together with three other electronic dictionaries, the e-WALD forms a series: * The e-WALD (Electronisch woordenboek van de Achterhoekse en Liemerse dialecten); * The e-WGD (Electronisch woordenboek van de Gelderse dialecten); * The e-WLD (Electronisch woordenboek van de Limburgse dialecten). The e-WBD was originally published as an in-press publication. More information on the books and volumes can be found here . The source code that facilitates the Django web applications of the electronic dialect dictionaries can be found on GitHub here . Credits and Contact Information The e-WBD application was developed by Erwin Komen of the Technical Support Group of the Humanities Lab of Radboud University under the supervision of Henk van den Heuvel. Thijs Hermsen made the maps. For technical information or issues regarding the e-WBD, you can contact dr. Henk van den Heuvel (H.vandenHeuvel@Let.ru.nl), Director CLST & Head of the Humanities Lab. Regarding the contents of the e-WBD, you can contact prof. dr. Nicoline van der Sijs (post@nicolinevdsijs.nl). Editing and digitization In 2015, the first two volumes of the Woordenboek van de Brabantse dialecten were digitized thanks to a grant awarded by CLARIN for the project 'CARE: Curation and integration of regional dictionaries' led by Nicoline van der Sijs, Henk van den Heuvel and Roeland van Hout (RU). Eric Sanders (RU) designed a macro and script for semi-automatic digitization. Research assistant Aukje Borkent, data curator Linda van Meel (partly thanks to the chair for Diversity in Language and Culture in Brabant, Tilburg University), student assistants Jorik van Engeland, Inge Otto, Lisette van der Heijde and Hanna van den Heuvel, and interns Maaike Borst and Eline Dimmendaal had all texts read by the macro and the result was accurately corrected manually. Volunteers Jantien Kettenes-Van den Bosch and Herman Wiltink have carried out a pre-correction for many parts. Joep Kruijsen, one of the original editors, offered his enormous know-how for the digitization and carried out a lot of correction work; for example, he corrected all phonetic signs from volumes I and II. These corrections will be gradually added to the current application in the near future.","title":"Ewbd"},{"location":"tools/ewbd/#e-wald-electronisch-woordenboek-van-de-brabantse-dialecten","text":"The e-WBD is an online dictionary of dialects from the Dutch province Noord-Brabant, as well as the Belgian provinces of Antwerpen, Vlaams-Brabant and Brussels. It stands for Electronisch woordenboek van de Brabantse dialecten .","title":"e-WALD: Electronisch woordenboek van de Brabantse dialecten"},{"location":"tools/ewbd/#overview","text":"The e-WBD contains the three volumes of the Woordenboek van de Brabantse dialecten , that was published between 1967 and 2005. Users can query the e-WBD for keywords and concepts, as well as locations (using a placename or a Kloekecode ). The e-WBD contains 15,794 concepts, 140,091 keywords, and 1,704,116 dialectal entries, collected in over 2950 Brabantian places, with each place corresponding to its own dialect. Together with the e-WALD , e-WGD and the e-WLD , the e-WBD forms a series of electronic dictionaries.","title":"Overview"},{"location":"tools/ewbd/#about","text":"","title":"About"},{"location":"tools/ewbd/#overview-research-area","text":"The research area for the e-WBD comprises the Dutch province of Noord-Brabant (north),the Belgian province of Antwerpen (middle) and Vlaams-Brabant (south). On the map, another closed area has been indicated in Vlaams-Brabant, the Brussels Capital Region. Brusselian is originally a Brabantian dialect.","title":"Overview research area"},{"location":"tools/ewbd/#brabantian-dialects","text":"Below is a dialect map showing the Brabant dialect areas. These areas have been used to designate areas in the WBD. An explanation of this classification can be found in WBD Inleiding op deel III: R. Belemans, J. Goossens, Inleiding en klankgeografie van de Brabantse dialecten , Assen 2000 (232p.) ( in Dutch ). That episode included a transparent card that could be placed on top of the printed cards. That transparent map has been converted into the dialect map shown here, in which the larger places are indicated for orientation. The larger dialect areas are demarcated with lines. The further division is indicated in color.","title":"Brabantian dialects"},{"location":"tools/ewbd/#learn","text":"A manual on how to query the e-WBD can be found on the e-WBD webpage . ( This manual is only available in Dutch, however. )","title":"Learn"},{"location":"tools/ewbd/#mentions","text":"Together with three other electronic dictionaries, the e-WALD forms a series: * The e-WALD (Electronisch woordenboek van de Achterhoekse en Liemerse dialecten); * The e-WGD (Electronisch woordenboek van de Gelderse dialecten); * The e-WLD (Electronisch woordenboek van de Limburgse dialecten). The e-WBD was originally published as an in-press publication. More information on the books and volumes can be found here . The source code that facilitates the Django web applications of the electronic dialect dictionaries can be found on GitHub here .","title":"Mentions"},{"location":"tools/ewbd/#credits-and-contact-information","text":"The e-WBD application was developed by Erwin Komen of the Technical Support Group of the Humanities Lab of Radboud University under the supervision of Henk van den Heuvel. Thijs Hermsen made the maps. For technical information or issues regarding the e-WBD, you can contact dr. Henk van den Heuvel (H.vandenHeuvel@Let.ru.nl), Director CLST & Head of the Humanities Lab. Regarding the contents of the e-WBD, you can contact prof. dr. Nicoline van der Sijs (post@nicolinevdsijs.nl).","title":"Credits and Contact Information"},{"location":"tools/ewbd/#editing-and-digitization","text":"In 2015, the first two volumes of the Woordenboek van de Brabantse dialecten were digitized thanks to a grant awarded by CLARIN for the project 'CARE: Curation and integration of regional dictionaries' led by Nicoline van der Sijs, Henk van den Heuvel and Roeland van Hout (RU). Eric Sanders (RU) designed a macro and script for semi-automatic digitization. Research assistant Aukje Borkent, data curator Linda van Meel (partly thanks to the chair for Diversity in Language and Culture in Brabant, Tilburg University), student assistants Jorik van Engeland, Inge Otto, Lisette van der Heijde and Hanna van den Heuvel, and interns Maaike Borst and Eline Dimmendaal had all texts read by the macro and the result was accurately corrected manually. Volunteers Jantien Kettenes-Van den Bosch and Herman Wiltink have carried out a pre-correction for many parts. Joep Kruijsen, one of the original editors, offered his enormous know-how for the digitization and carried out a lot of correction work; for example, he corrected all phonetic signs from volumes I and II. These corrections will be gradually added to the current application in the near future.","title":"Editing and digitization"},{"location":"tools/ewgd/","text":"title: e-WGD: Electronisch woordenboek van de Gelderse dialecten carousel: - /media/taalkaart-wgd.jpg - /media/boeken-wgd.jpg - /media/taalkaart-wgd_vindplaatsen.jpg identifier: ewgd e-WGD: Electronisch woordenboek van de Gelderse dialecten The e-WGD is an online dictionary of dialects from the Dutch province Gelderland, specifically the regions Rivierenstreek and Veluwe. It stands for Electronisch woordenboek van de Gelderse dialecten . Overview The e-WGD contains comprises the six volumes of the Woordenboek van de Gelderse dialecten , that were compiled between 2001 and 2006. The (e-)WGD contains information about the dialects of the Rivierenstreek and Veluwe, the two darker shaded areas on the map shown below. The two lighter shaded areas on the map are the regions Liemers and Achterhoek, the dialects of which are described in the e-WALD . Together they cover the entire Dutch province of Gelderland. The e-WGD contains more information than the WGB. With questionnaires extra material was collected, that was not included in the printed issues for various reasons. Users can query the e-WGD for keywords and concepts, as well as locations (using a placename or a Kloekecode ) and dialects. The e-WGD contains 3,544 concepts, 55,745 keywords, and 168,417 dialectal entries, collected in 107 places throughout Gelderland, with each place corresponding to its own dialect. Together with the e-WALD , e-WBD and the e-WLD , the e-WGD forms a series of electronic dictionaries. About Research areas The map below includes the research area of the WGD. The area includes the Rivierenstreek and the Veluwe. The map shows which places participated; these are the places where volunteers were found to complete the questionnaires. Unfortunately the city of Arnhem is not among those places. Some places outside the area have also been included to see how adjacent areas connect. For the Rivierenstreek these are Rhenen, Achterberg and Leerbroek. For the Veluwe these are Bunschoten-Spakenburg and Huizen. Learn A manual on how to query the e-WGD can be found on the e-WGD webpage . ( This manual is only available in Dutch, however. ) Mentions Together with three other electronic dictionaries, the e-WALD forms a series: * The e-WALD (Electronisch woordenboek van de Achterhoekse en Liemerse dialecten); * The e-WBD ((Electronisch woordenboek van de Brabantse dialecten); * The e-WLD (Electronisch woordenboek van de Limburgse dialecten). The e-WGD was originally published as an in-press publication. More information on the books and volumes can be found here . The source code that facilitates the Django web applications of the electronic dialect dictionaries can be found on GitHub here . Credits and Contact Information The e-WGD application was developed by Erwin Komen of the Technical Support Group of the Humanities Lab of Radboud University under the supervision of Henk van den Heuvel. Roeland van Hout was responsible for designing the content of the e-WGD. Thijs Hermsen made the maps. For technical information or issues regarding the e-WGD, you can contact dr. Henk van den Heuvel (H.vandenHeuvel@Let.ru.nl), Director CLST & Head of the Humanities Lab. Regarding the contents of the e-WGD, you can contact prof. dr. Roeland van Hout (r.v.hout@let.ru.nl). Editing and digitization Between 2001 and 2006, six printed issues of the Woordenboek van de Gelderse dialecten (WGD) were compiled: three parts for the Rivierenstreek (edited by Dr. Charlotte Giesbers) and three parts for the Veluwe (edited by Dr. Harrie Scholtmeijer, for the part about humanity together with Dr. Charlotte Giesbers). The dictionaries describe specific parts of the general vocabulary: the house, man and the world. The information in the dictionaries comes from a large number of informants who voluntarily completed the questionnaires. The project was funded by the Province of Gelderland and the Faculty of Arts of Radboud University Nijmegen. In addition, several parties were involved in the creation of the WGD: Gelders Erfgoed, the Staring Institute, the IJssel Academy and the Cross-border Regional Languages Foundation. In 2015, the first volumes of the Woordenboek van de Gelderse dialecten (WGD) were digitized thanks to a subsidy awarded by CLARIN for the project 'CARE: Curation and integration of regional dictionaries' led by Nicoline van der Sijs, Henk van den Heuvel and Roeland van Hout. Subsequently, in 2021, a subsidy was awarded by the Lower Saxony Gelderland incentive fund of the Erfgoedcentrum Achterhoek Liemers (ECAL) to further develop the WGD website for the Veluwe dialects. In addition, the material from the Rivierenstreek has been made completely accessible. The new version of the WGD is available from May 2022. The e-WGD contains more material than the printed parts. The questionnaires collected additional material that, for various reasons, was not included in the printed issues of the WGD. The website contains all useful material from the questionnaires. In the printed editions, on the other hand, data from an older dialect survey and local dictionaries were used. This data is not included in the e-WGD.","title":"Ewgd"},{"location":"tools/ewgd/#e-wgd-electronisch-woordenboek-van-de-gelderse-dialecten","text":"The e-WGD is an online dictionary of dialects from the Dutch province Gelderland, specifically the regions Rivierenstreek and Veluwe. It stands for Electronisch woordenboek van de Gelderse dialecten .","title":"e-WGD: Electronisch woordenboek van de Gelderse dialecten"},{"location":"tools/ewgd/#overview","text":"The e-WGD contains comprises the six volumes of the Woordenboek van de Gelderse dialecten , that were compiled between 2001 and 2006. The (e-)WGD contains information about the dialects of the Rivierenstreek and Veluwe, the two darker shaded areas on the map shown below. The two lighter shaded areas on the map are the regions Liemers and Achterhoek, the dialects of which are described in the e-WALD . Together they cover the entire Dutch province of Gelderland. The e-WGD contains more information than the WGB. With questionnaires extra material was collected, that was not included in the printed issues for various reasons. Users can query the e-WGD for keywords and concepts, as well as locations (using a placename or a Kloekecode ) and dialects. The e-WGD contains 3,544 concepts, 55,745 keywords, and 168,417 dialectal entries, collected in 107 places throughout Gelderland, with each place corresponding to its own dialect. Together with the e-WALD , e-WBD and the e-WLD , the e-WGD forms a series of electronic dictionaries.","title":"Overview"},{"location":"tools/ewgd/#about","text":"","title":"About"},{"location":"tools/ewgd/#research-areas","text":"The map below includes the research area of the WGD. The area includes the Rivierenstreek and the Veluwe. The map shows which places participated; these are the places where volunteers were found to complete the questionnaires. Unfortunately the city of Arnhem is not among those places. Some places outside the area have also been included to see how adjacent areas connect. For the Rivierenstreek these are Rhenen, Achterberg and Leerbroek. For the Veluwe these are Bunschoten-Spakenburg and Huizen.","title":"Research areas"},{"location":"tools/ewgd/#learn","text":"A manual on how to query the e-WGD can be found on the e-WGD webpage . ( This manual is only available in Dutch, however. )","title":"Learn"},{"location":"tools/ewgd/#mentions","text":"Together with three other electronic dictionaries, the e-WALD forms a series: * The e-WALD (Electronisch woordenboek van de Achterhoekse en Liemerse dialecten); * The e-WBD ((Electronisch woordenboek van de Brabantse dialecten); * The e-WLD (Electronisch woordenboek van de Limburgse dialecten). The e-WGD was originally published as an in-press publication. More information on the books and volumes can be found here . The source code that facilitates the Django web applications of the electronic dialect dictionaries can be found on GitHub here .","title":"Mentions"},{"location":"tools/ewgd/#credits-and-contact-information","text":"The e-WGD application was developed by Erwin Komen of the Technical Support Group of the Humanities Lab of Radboud University under the supervision of Henk van den Heuvel. Roeland van Hout was responsible for designing the content of the e-WGD. Thijs Hermsen made the maps. For technical information or issues regarding the e-WGD, you can contact dr. Henk van den Heuvel (H.vandenHeuvel@Let.ru.nl), Director CLST & Head of the Humanities Lab. Regarding the contents of the e-WGD, you can contact prof. dr. Roeland van Hout (r.v.hout@let.ru.nl).","title":"Credits and Contact Information"},{"location":"tools/ewgd/#editing-and-digitization","text":"Between 2001 and 2006, six printed issues of the Woordenboek van de Gelderse dialecten (WGD) were compiled: three parts for the Rivierenstreek (edited by Dr. Charlotte Giesbers) and three parts for the Veluwe (edited by Dr. Harrie Scholtmeijer, for the part about humanity together with Dr. Charlotte Giesbers). The dictionaries describe specific parts of the general vocabulary: the house, man and the world. The information in the dictionaries comes from a large number of informants who voluntarily completed the questionnaires. The project was funded by the Province of Gelderland and the Faculty of Arts of Radboud University Nijmegen. In addition, several parties were involved in the creation of the WGD: Gelders Erfgoed, the Staring Institute, the IJssel Academy and the Cross-border Regional Languages Foundation. In 2015, the first volumes of the Woordenboek van de Gelderse dialecten (WGD) were digitized thanks to a subsidy awarded by CLARIN for the project 'CARE: Curation and integration of regional dictionaries' led by Nicoline van der Sijs, Henk van den Heuvel and Roeland van Hout. Subsequently, in 2021, a subsidy was awarded by the Lower Saxony Gelderland incentive fund of the Erfgoedcentrum Achterhoek Liemers (ECAL) to further develop the WGD website for the Veluwe dialects. In addition, the material from the Rivierenstreek has been made completely accessible. The new version of the WGD is available from May 2022. The e-WGD contains more material than the printed parts. The questionnaires collected additional material that, for various reasons, was not included in the printed issues of the WGD. The website contains all useful material from the questionnaires. In the printed editions, on the other hand, data from an older dialect survey and local dictionaries were used. This data is not included in the e-WGD.","title":"Editing and digitization"},{"location":"tools/ewld/","text":"title: e-WLD: Electronisch woordenboek van de Limburgse dialecten carousel: - /media/taalkaart-wld.jpg - /media/boeken-wld.jpg - /media/dialectkaart-wld.jpg identifier: ewld e-WLD: Electronisch woordenboek van de Limburgse dialecten The e-WLD is an online dictionary of the Limburgian dialects of Dutch. It stands for Electronisch woordenboek van de Limburgse dialecten . Overview The e-WLD contains the three volumes of the Woordenboek van de Limburgse dialecten , that was published between 1983 and 2008. Users can query the e-WLD for keywords and concepts, as well as locations (using a placename or a Kloekecode ) and dialects. The e-WLD contains 17,539 concepts, 137,231 keywords, and 1,759,090 dialectal entries, collected in over 1000 Limburgian places, with each place corresponding to its own dialect. Volume II.5 of the WLD was devoted to the 19 mines of Limburg (12 in the Netherlands, 7 in Belgium). The mines were home to a special subdialects of Limburgian, and these mines therefore form an extension to the Limburg town network. It is possible to search for these mines specifically in the e-WLD. Together with the e-WALD , e-WBD and the e-WGD , the e-WLD forms a series of electronic dictionaries. Learn A manual on how to query the e-WLD can be found on the e-WLD webpage . ( This manual is only available in Dutch, however. ) Mentions Together with three other electronic dictionaries, the e-WALD forms a series: * The e-WALD (Electronisch woordenboek van de Achterhoekse en Liemerse dialecten); * The e-WBD (Electronisch woordenboek van de Brabantse dialecten); * The e-WGD (Electronisch woordenboek van de Gelderse dialecten). The e-WLD was originally published as an in-press publication. More information on the books and volumes can be found here . The source code that facilitates the Django web applications of the electronic dialect dictionaries can be found on GitHub here . Credits and Contact Information The e-WLD application was developed by Erwin Komen of the Technical Support Group of the Humanities Lab of Radboud University under the supervision of Henk van den Heuvel. Thijs Hermsen made the map on the homepage. For technical information or issues regarding the e-WLD, you can contact dr. Henk van den Heuvel (H.vandenHeuvel@Let.ru.nl), Director CLST & Head of the Humanities Lab. Regarding the contents of the e-WLD, you can contact prof. dr. Nicoline van der Sijs (post@nicolinevdsijs.nl). Editing and digitization In 2015, the first two volumes of the Woordenboek van de Limburgse dialecten were digitized thanks to a grant awarded by CLARIN for the project 'CARE: Curation and integration of regional dictionaries' led by Nicoline van der Sijs, Henk van den Heuvel and Roeland van Hout (RU). Eric Sanders (RU) designed a macro and script for semi-automatic digitization. Research assistant Aukje Borkent, data curator Linda van Meel (thanks to a subsidy grom the Raod veur 't Limburgs), student assistants Jorik van Engeland, Inge Otto, Lisette van der Heijde and Hanna van den Heuvel, and interns Maaike Borst and Eline Dimmendaal had all texts read by the macro and the result was accurately corrected manually. Volunteers Jantien Kettenes-Van den Bosch and Herman Wiltink have carried out a pre-correction for many parts. Joep Kruijsen, one of the original editors, offered his enormous know-how for the digitization and carried out a lot of correction work; for example, he corrected all phonetic signs from volume II. Corrections of the first volume are not yet available on the e-WLD.","title":"Ewld"},{"location":"tools/ewld/#e-wld-electronisch-woordenboek-van-de-limburgse-dialecten","text":"The e-WLD is an online dictionary of the Limburgian dialects of Dutch. It stands for Electronisch woordenboek van de Limburgse dialecten .","title":"e-WLD: Electronisch woordenboek van de Limburgse dialecten"},{"location":"tools/ewld/#overview","text":"The e-WLD contains the three volumes of the Woordenboek van de Limburgse dialecten , that was published between 1983 and 2008. Users can query the e-WLD for keywords and concepts, as well as locations (using a placename or a Kloekecode ) and dialects. The e-WLD contains 17,539 concepts, 137,231 keywords, and 1,759,090 dialectal entries, collected in over 1000 Limburgian places, with each place corresponding to its own dialect. Volume II.5 of the WLD was devoted to the 19 mines of Limburg (12 in the Netherlands, 7 in Belgium). The mines were home to a special subdialects of Limburgian, and these mines therefore form an extension to the Limburg town network. It is possible to search for these mines specifically in the e-WLD. Together with the e-WALD , e-WBD and the e-WGD , the e-WLD forms a series of electronic dictionaries.","title":"Overview"},{"location":"tools/ewld/#learn","text":"A manual on how to query the e-WLD can be found on the e-WLD webpage . ( This manual is only available in Dutch, however. )","title":"Learn"},{"location":"tools/ewld/#mentions","text":"Together with three other electronic dictionaries, the e-WALD forms a series: * The e-WALD (Electronisch woordenboek van de Achterhoekse en Liemerse dialecten); * The e-WBD (Electronisch woordenboek van de Brabantse dialecten); * The e-WGD (Electronisch woordenboek van de Gelderse dialecten). The e-WLD was originally published as an in-press publication. More information on the books and volumes can be found here . The source code that facilitates the Django web applications of the electronic dialect dictionaries can be found on GitHub here .","title":"Mentions"},{"location":"tools/ewld/#credits-and-contact-information","text":"The e-WLD application was developed by Erwin Komen of the Technical Support Group of the Humanities Lab of Radboud University under the supervision of Henk van den Heuvel. Thijs Hermsen made the map on the homepage. For technical information or issues regarding the e-WLD, you can contact dr. Henk van den Heuvel (H.vandenHeuvel@Let.ru.nl), Director CLST & Head of the Humanities Lab. Regarding the contents of the e-WLD, you can contact prof. dr. Nicoline van der Sijs (post@nicolinevdsijs.nl).","title":"Credits and Contact Information"},{"location":"tools/ewld/#editing-and-digitization","text":"In 2015, the first two volumes of the Woordenboek van de Limburgse dialecten were digitized thanks to a grant awarded by CLARIN for the project 'CARE: Curation and integration of regional dictionaries' led by Nicoline van der Sijs, Henk van den Heuvel and Roeland van Hout (RU). Eric Sanders (RU) designed a macro and script for semi-automatic digitization. Research assistant Aukje Borkent, data curator Linda van Meel (thanks to a subsidy grom the Raod veur 't Limburgs), student assistants Jorik van Engeland, Inge Otto, Lisette van der Heijde and Hanna van den Heuvel, and interns Maaike Borst and Eline Dimmendaal had all texts read by the macro and the result was accurately corrected manually. Volunteers Jantien Kettenes-Van den Bosch and Herman Wiltink have carried out a pre-correction for many parts. Joep Kruijsen, one of the original editors, offered his enormous know-how for the digitization and carried out a lot of correction work; for example, he corrected all phonetic signs from volume II. Corrections of the first volume are not yet available on the e-WLD.","title":"Editing and digitization"},{"location":"tools/frog/","text":"Frog Frog is a suite containing a tokeniser, Part-of-Speech tagger, lemmatiser, morphological analyser, shallow parser, and dependency parser for Dutch. Overview Frog is an integration of memory-based natural language processing (NLP) modules developed for Dutch. All NLP modules are based on Timbl , the Tilburg memory-based learning software package. Most modules were created in the 1990s at the ILK Research Group (Tilburg University, the Netherlands) and the CLiPS Research Centre (University of Antwerp, Belgium). Over the years they have been integrated into a single text processing tool, which is currently maintained and developed by the Language Machines Research Group and the Centre for Language and Speech Technology at Radboud University Nijmegen . A dependency parser, a base phrase chunker, and a named-entity recognizer module were added more recently. Where possible, Frog makes use of multi-processor support to run subtasks in parallel. Frog is also available as a webservice on (https://webservices.cls.ru.nl/frog). What does it do? Frog's current version will tokenize, tag, lemmatize, and morphologically segment word tokens in Dutch text files, will assign a dependency graph to each sentence, will identify the base phrase chunks in the sentence, and will attempt to find and label all named entities. Frog produces FoLiA XML , or tab-delimited column-formatted output, one line per token, that looks as follows: The ten columns contain the following information: * Token number (resets every sentence) * Token * Lemma * Morphological segmentation * PoS tag ( CGN tagset ) * Confidence in the POS tag, a number between 0 and 1, representing the probability mass assigned to the best guess tag in the tag distribution * Named entity type, identifying person (PER), organization (ORG), location (LOC), product (PRO), event (EVE), and miscellaneous (MISC), using a BIO (or IOB2) encoding * Base (non-embedded) phrase chunk in BIO encoding * Token number of head word in dependency graph (according to CSI-DP) * Type of dependency relation with head word Documentation The Frog manual is available here . It describes in detail how to install Frog, how to use it, as well as explains the underlying principles upon which Frog is built. The API reference is available from here . Learn Download and installation Frog is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation . To download and install Frog: First check if there are up-to-date packages included in your distribution's package manager. There are packages for Alpine Linux, Homebrew (macos), Debian, Ubuntu and Arch Linux. Use a docker container as instructed here . Alternatively, you can always download, compile and install Frog manually, as shown next. Manual installation Source code Stable releases Because of file sizes and to cleanly separate code from data, the data and configuration files for the modules of Frog have been packaged separately: Source repository Stable releases To compile these manually consult the included INSTALL documents, you will need current versions of the following dependencies of our software: ticcutils - A shared utility library libfolia - A library for the FoLiA format. ucto - A rule-based tokenizer timbl - The memory-based classifier engine timblserver - For server functionality around Timbl mbt - The memory-based tagger As well as the following 3rd party dependencies: icu - A C++ library for Unicode and Globalization support. On Debian/Ubuntu systems, install the package libicu-dev. libxml2 - An XML library. On Debian/Ubuntu systems install the package libxml2-dev. A sane build environment with a C++ compiler (e.g. gcc or clang), autotools, libtool, pkg-config. Usage instructions: Making Frog leap To let Frog leap, simply invoking frog without arguments will produce a list of available commandline options. Some main options are: frog -t [file] will run all modules on the text in [file] frog --testdir=[dir] will let Frog process all files in the directory [dir] . frog -S [port] starts up a Frog server listening on port number [port] . With --skip=[mptnc] you can tell Frog to skip tokenization ( t ), base phrase chunking ( c ), named-entity recognition ( n ), multi-word unit chunking for the parser ( m ), or parsing ( p ). Frog can be used from Python through the python-frog binding, which has to be obtained separately unless you are using LaMachine . A python-frog example is shown below: import frog frog = frog.Frog(frog.FrogOptions(parser=False)) output = frog.process_raw(\"Dit is een test\") print(\"RAW OUTPUT=\",output) output = frog.process(\"Dit is nog een test.\") print(\"PARSED OUTPUT=\",output) If you want to connect to the Frog server using Python, then you can use the Frog client included in PyNLPl (also included as part of LaMachine ). from pynlpl.clients.frogclient import FrogClient port = 8020 frogclient = FrogClient('localhost',port) for data in frogclient.process(\"Een voorbeeldbericht om te froggen\"): word, lemma, morph, pos = data[:4] #TODO: further processing Wouter van Atteveldt has developed a Frog client for R, frogr . This package contains functions for connecting to a Frog server from R and creating a document-term matrix from the resulting tokens. Since this yields a standard term-document matrix, it can be used with other R packages e.g. for corpus analysis or text classification using RTextTools . Machiel Molenaar developed a Frog client for Go, aptly named gorf . Notice : we are in the process of writing a reference guide for Frog that explains all options in detail. Mentions Publications If you use Frog for your own work, please cite the following paper: * Van den Bosch, A., Busser, G.J., Daelemans, W., and Canisius, S. (2007). An efficient memory-based morphosyntactic tagger and parser for Dutch, In F. van Eynde, P. Dirix, I. Schuurman, and V. Vandeghinste (Eds.), Selected Papers of the 17th Computational Linguistics in the Netherlands Meeting, Leuven, Belgium, pp. 99-114 Frog uses the CGN part-of-speech tagset. Full documentation can be found as: * Van Eynde, F. (2004). Part of speech tagging en lemmatisering van het Corpus Gesproken Nederlands. KU Leuven. Credits and Contact Information Frog, formerly known as Tadpole and before that as MB-TALPA, was coded by Bertjan Busser, Ko van der Sloot, Maarten van Gompel, and Peter Berck, subsuming code by Sander Canisius (constraint satisfaction inference-based dependency parser), Antal van den Bosch (MBMA, MBLEM, tagger-lemmatizer integration), Jakub Zavrel (MBT), and Maarten van Gompel (Ucto). In the context of the CLARIN-NL infrastructure project TTNWW, Frederik Vaassen (CLiPS, Antwerp) created the base phrase chunking module, and Bart Desmet (LT3, Ghent) provided the data for the named-entity module. Maarten van Gompel designed the FoLiA XML output format that Frog produces, and also wrote a Frog client in Python. Wouter van Atteveldt wrote a Frog client in R. The development of Frog relies on earlier work and ideas from Ko van der Sloot (lead programmer of MBT and TiMBL and the TiMBL API), Walter Daelemans, Jakub Zavrel, Peter Berck, Gert Durieux, and Ton Weijters. The development and improvement of Frog also relies on your bug reports, suggestions, and comments. Use the github issue tracker or mail lamasoftware (at) science.ru.nl. Webpages Frog mainpage Frog as a webservice Frog documentation Frog GitHub page","title":"Frog"},{"location":"tools/frog/#frog","text":"Frog is a suite containing a tokeniser, Part-of-Speech tagger, lemmatiser, morphological analyser, shallow parser, and dependency parser for Dutch.","title":"Frog"},{"location":"tools/frog/#overview","text":"Frog is an integration of memory-based natural language processing (NLP) modules developed for Dutch. All NLP modules are based on Timbl , the Tilburg memory-based learning software package. Most modules were created in the 1990s at the ILK Research Group (Tilburg University, the Netherlands) and the CLiPS Research Centre (University of Antwerp, Belgium). Over the years they have been integrated into a single text processing tool, which is currently maintained and developed by the Language Machines Research Group and the Centre for Language and Speech Technology at Radboud University Nijmegen . A dependency parser, a base phrase chunker, and a named-entity recognizer module were added more recently. Where possible, Frog makes use of multi-processor support to run subtasks in parallel. Frog is also available as a webservice on (https://webservices.cls.ru.nl/frog).","title":"Overview"},{"location":"tools/frog/#what-does-it-do","text":"Frog's current version will tokenize, tag, lemmatize, and morphologically segment word tokens in Dutch text files, will assign a dependency graph to each sentence, will identify the base phrase chunks in the sentence, and will attempt to find and label all named entities. Frog produces FoLiA XML , or tab-delimited column-formatted output, one line per token, that looks as follows: The ten columns contain the following information: * Token number (resets every sentence) * Token * Lemma * Morphological segmentation * PoS tag ( CGN tagset ) * Confidence in the POS tag, a number between 0 and 1, representing the probability mass assigned to the best guess tag in the tag distribution * Named entity type, identifying person (PER), organization (ORG), location (LOC), product (PRO), event (EVE), and miscellaneous (MISC), using a BIO (or IOB2) encoding * Base (non-embedded) phrase chunk in BIO encoding * Token number of head word in dependency graph (according to CSI-DP) * Type of dependency relation with head word","title":"What does it do?"},{"location":"tools/frog/#documentation","text":"The Frog manual is available here . It describes in detail how to install Frog, how to use it, as well as explains the underlying principles upon which Frog is built. The API reference is available from here .","title":"Documentation"},{"location":"tools/frog/#learn","text":"","title":"Learn"},{"location":"tools/frog/#download-and-installation","text":"Frog is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation . To download and install Frog: First check if there are up-to-date packages included in your distribution's package manager. There are packages for Alpine Linux, Homebrew (macos), Debian, Ubuntu and Arch Linux. Use a docker container as instructed here . Alternatively, you can always download, compile and install Frog manually, as shown next.","title":"Download and installation"},{"location":"tools/frog/#manual-installation","text":"Source code Stable releases Because of file sizes and to cleanly separate code from data, the data and configuration files for the modules of Frog have been packaged separately: Source repository Stable releases To compile these manually consult the included INSTALL documents, you will need current versions of the following dependencies of our software: ticcutils - A shared utility library libfolia - A library for the FoLiA format. ucto - A rule-based tokenizer timbl - The memory-based classifier engine timblserver - For server functionality around Timbl mbt - The memory-based tagger As well as the following 3rd party dependencies: icu - A C++ library for Unicode and Globalization support. On Debian/Ubuntu systems, install the package libicu-dev. libxml2 - An XML library. On Debian/Ubuntu systems install the package libxml2-dev. A sane build environment with a C++ compiler (e.g. gcc or clang), autotools, libtool, pkg-config.","title":"Manual installation"},{"location":"tools/frog/#usage-instructions-making-frog-leap","text":"To let Frog leap, simply invoking frog without arguments will produce a list of available commandline options. Some main options are: frog -t [file] will run all modules on the text in [file] frog --testdir=[dir] will let Frog process all files in the directory [dir] . frog -S [port] starts up a Frog server listening on port number [port] . With --skip=[mptnc] you can tell Frog to skip tokenization ( t ), base phrase chunking ( c ), named-entity recognition ( n ), multi-word unit chunking for the parser ( m ), or parsing ( p ). Frog can be used from Python through the python-frog binding, which has to be obtained separately unless you are using LaMachine . A python-frog example is shown below: import frog frog = frog.Frog(frog.FrogOptions(parser=False)) output = frog.process_raw(\"Dit is een test\") print(\"RAW OUTPUT=\",output) output = frog.process(\"Dit is nog een test.\") print(\"PARSED OUTPUT=\",output) If you want to connect to the Frog server using Python, then you can use the Frog client included in PyNLPl (also included as part of LaMachine ). from pynlpl.clients.frogclient import FrogClient port = 8020 frogclient = FrogClient('localhost',port) for data in frogclient.process(\"Een voorbeeldbericht om te froggen\"): word, lemma, morph, pos = data[:4] #TODO: further processing Wouter van Atteveldt has developed a Frog client for R, frogr . This package contains functions for connecting to a Frog server from R and creating a document-term matrix from the resulting tokens. Since this yields a standard term-document matrix, it can be used with other R packages e.g. for corpus analysis or text classification using RTextTools . Machiel Molenaar developed a Frog client for Go, aptly named gorf . Notice : we are in the process of writing a reference guide for Frog that explains all options in detail.","title":"Usage instructions: Making Frog leap"},{"location":"tools/frog/#mentions","text":"","title":"Mentions"},{"location":"tools/frog/#publications","text":"If you use Frog for your own work, please cite the following paper: * Van den Bosch, A., Busser, G.J., Daelemans, W., and Canisius, S. (2007). An efficient memory-based morphosyntactic tagger and parser for Dutch, In F. van Eynde, P. Dirix, I. Schuurman, and V. Vandeghinste (Eds.), Selected Papers of the 17th Computational Linguistics in the Netherlands Meeting, Leuven, Belgium, pp. 99-114 Frog uses the CGN part-of-speech tagset. Full documentation can be found as: * Van Eynde, F. (2004). Part of speech tagging en lemmatisering van het Corpus Gesproken Nederlands. KU Leuven.","title":"Publications"},{"location":"tools/frog/#credits-and-contact-information","text":"Frog, formerly known as Tadpole and before that as MB-TALPA, was coded by Bertjan Busser, Ko van der Sloot, Maarten van Gompel, and Peter Berck, subsuming code by Sander Canisius (constraint satisfaction inference-based dependency parser), Antal van den Bosch (MBMA, MBLEM, tagger-lemmatizer integration), Jakub Zavrel (MBT), and Maarten van Gompel (Ucto). In the context of the CLARIN-NL infrastructure project TTNWW, Frederik Vaassen (CLiPS, Antwerp) created the base phrase chunking module, and Bart Desmet (LT3, Ghent) provided the data for the named-entity module. Maarten van Gompel designed the FoLiA XML output format that Frog produces, and also wrote a Frog client in Python. Wouter van Atteveldt wrote a Frog client in R. The development of Frog relies on earlier work and ideas from Ko van der Sloot (lead programmer of MBT and TiMBL and the TiMBL API), Walter Daelemans, Jakub Zavrel, Peter Berck, Gert Durieux, and Ton Weijters. The development and improvement of Frog also relies on your bug reports, suggestions, and comments. Use the github issue tracker or mail lamasoftware (at) science.ru.nl.","title":"Credits and Contact Information"},{"location":"tools/frog/#webpages","text":"Frog mainpage Frog as a webservice Frog documentation Frog GitHub page","title":"Webpages"},{"location":"tools/gecco/","text":"Gecco Gecco is a modular, scalable, and context-aware spelling correction framework. It's designed to build a complete context-aware spelling correction system given your own data set. Overview Gecco (Generic Environment for Context-Aware Correction of Orthography) is a generic modular and distributed framework for spelling correction. Its primary goal is to build a complete context-aware spelling correction system using your own dataset. Most of its modules are language-independent, and they can be trained using a source corpus. Training is explicitly included in the framework. The framework is designed to be easily extendable, allowing you to write modules in Python 3 to enhance its functionality. Additionally, Gecco is scalable and can be distributed across multiple servers to handle larger tasks efficiently. Given an input text, Gecco will add various suggestions for correction. Learn The system can be invoked from the command-line, as a Python binding, as a RESTful webservice, or through the web application (two interfaces). To install Gecco, we strongly recommend you to use our LaMachine distribution, which can be obtained from [https://github.com/proycon/lamachine]. LaMachine includes Gecco and can be run in multiple ways: as a virtual machine, as a docker app, or as a compilation script setting up a Python virtual environment. Gecco uses memory-based technologies, and depending on the models you train, may take up considerable memory. Therefore we recommend at least 16GB RAM, training may require even more. For various modules, model size may be reduced by increasing frequency thresholds, but this will come at the cost of reduced accuracy. Gecco will only run on POSIX-complaint operating systems (i.e. Linux, BSD, Mac OS X), not on Windows. Please refer to (the GitHub)[https://github.com/proycon/gecco] for more information on the requirements and set-up.","title":"Gecco"},{"location":"tools/gecco/#gecco","text":"Gecco is a modular, scalable, and context-aware spelling correction framework. It's designed to build a complete context-aware spelling correction system given your own data set.","title":"Gecco"},{"location":"tools/gecco/#overview","text":"Gecco (Generic Environment for Context-Aware Correction of Orthography) is a generic modular and distributed framework for spelling correction. Its primary goal is to build a complete context-aware spelling correction system using your own dataset. Most of its modules are language-independent, and they can be trained using a source corpus. Training is explicitly included in the framework. The framework is designed to be easily extendable, allowing you to write modules in Python 3 to enhance its functionality. Additionally, Gecco is scalable and can be distributed across multiple servers to handle larger tasks efficiently. Given an input text, Gecco will add various suggestions for correction.","title":"Overview"},{"location":"tools/gecco/#learn","text":"The system can be invoked from the command-line, as a Python binding, as a RESTful webservice, or through the web application (two interfaces). To install Gecco, we strongly recommend you to use our LaMachine distribution, which can be obtained from [https://github.com/proycon/lamachine]. LaMachine includes Gecco and can be run in multiple ways: as a virtual machine, as a docker app, or as a compilation script setting up a Python virtual environment. Gecco uses memory-based technologies, and depending on the models you train, may take up considerable memory. Therefore we recommend at least 16GB RAM, training may require even more. For various modules, model size may be reduced by increasing frequency thresholds, but this will come at the cost of reduced accuracy. Gecco will only run on POSIX-complaint operating systems (i.e. Linux, BSD, Mac OS X), not on Windows. Please refer to (the GitHub)[https://github.com/proycon/gecco] for more information on the requirements and set-up.","title":"Learn"},{"location":"tools/gretel/","text":"GrETEL GrETEL is a user-friendly query engine in which linguists can use an utterance as a starting point for searching a treebank for a linguistic phenomenon. Instead of a formal, technical query instruction, it takes a natural language example as input. Overview GrETEL stands for Greedy Extraction of Trees for Empirical Linguistics. GrETEL is a user-friendly query engine in which linguists can use an utterance as a starting point for searching a treebank for a linguistic phenomenon. Instead of a formal, technical query instruction, it takes a natural language example as input. This provides a convenient way for novice and non-technical users to use treebanks with a limited knowledge of the underlying syntax and formal query languages. By allowing linguists to search for constructions similar to the example they provide, GrETEL aims to bridge the gap between descriptive-theoretical and computational linguistics. The example-based query procedure consists of multiple steps: The user enters an example of the construction they are interested in. The example is parsed with Alpino , and is returned in the form of a matrix, in which the user specifies which aspects of this example are essential for the construction under investigation. This matrix is automatically converted in an XPath query, which can be manually edited or even written by the user. The user chooses the corpus on which the query must be executed. The query is executed on the selected corpus, and the matching constructions are presented to the user as a list of sentences, which can be downloaded. The user can also click on the sentences in order to visualize the results as syntax trees, in which the matching part of the tree is highlighted. The results of the query can then be efficiently analysed through the use of a pivot table. GrETEL is usable online on its webpage: https://gretel.hum.uu.nl/. Because GrETEL relies on Alpino for parsing the example utterance, it only works for Dutch corpora and example utterances. However, GrETEL is in itself not bound to any particular language, as shown by the GrETEL variant for Afrikaans , and Poly-GrETEL , which enabled simultaneous querying in multiple languages in a parallel treebank. Data GrETEL ships with (treebanks of) the LASSY-SMALL, the CGN and the Schlichting/Van Kampen corpora for querying. GrETEL also allows users to upload their own corpus. Uploaded corpora are automatically parsed with the Alpino parser , preparing it for treebank querying. More about the LASSY-SMALL, the CGN and the Schlichting/Van Kampen corpora: LASSY-SMALL CGN (Corpus Gesproken Nederlands) Schlichting/Van Kampen corpus Learn Instruction and support Most recently, a complete tutorial on GrETEL was given on Oct 7, 2022 by Jan Odijk (in Dutch). This tutorial was taped and included slides and exercises, which can be found here . The GrETEL webpage offers a wide range of information, including tutorials, documentation and FAQ. * Slides * Exercises * Manuals and documentation * FAQ * Contact the developers Local installation While local installation is not necessary, given its web version , it is possible to install GrETEL locally. GrETEL\u2019s GitHub repository has detailed information on how to install GrETEL locally, as well as notes for users and developers. User support The current version of GrETEL is developed by the Digital Humanities Lab at Utrecht University. If you have any suggestions, questions, or general feedback you are welcome to give us a ring, or send us an email. You can find contact information on Digital Humanities Lab's website or in the footer of the GrETEL webpage . Jan Odijk's valedictory speech In his valedictory speech called \"Taaltechnologie voor taalkundig onderzoek [Language technology for linguistic research]\", professor Jan Odijk (UU) discussed two of his more recent research projects in detail; one of which is GrETEL, the other being SASTA , which he considers a spin-off of GrETEL. The recording of this ceremony can be found here ; the transcript of his speech can be found here ( both in Dutch ). Mentions Key publications Jan Odijk, Martijn van der Klis and Sheean Spoel (2018). \u201cExtensions to the GrETEL treebank query application.\u201d In: Proceedings of the 16th International Workshop on Treebanks and Linguistic Theories. Prague, Czech Republic. pp. 46-55. Liesbeth Augustinus, Vincent Vandeghinste and Frank Van Eynde (2012). \u201cExample-based treebank querying.\u201d In: Proceedings of the 8th Conference on Language Resources and Evaluation (LREC 2012). Istanbul, Turkey. pp. 3161-3167. Other publications Jan Odijk (2020). De verleidingen en gevaren van GrETEL. Nederlandse taalkunde, 25(1), 7-37. Jan Odijk (2023, 30 Jan.). Taaltechnologie voor taalkundig onderzoek . Valedictory speech, Utrecht University. https://surfdrive.surf.nl/files/index.php/s/pzNHSgd6t8L0Wnk Webpages Source code: https://github.com/UUDigitalHumanitieslab/gretel Previous versions: * GrETEL 3: http://gretel.ccl.kuleuven.be/gretel3/ * GrETEL 2: http://gretel.ccl.kuleuven.be/gretel-2.0/ebs/input.php More publications and talks: http://gretel.ccl.kuleuven.be/project/publications.php Poly-GrETEL: http://gretel.ccl.kuleuven.be/poly-gretel/ GrETEL for Afrikaans: http://gretel.ccl.kuleuven.be/afribooms/ Video\u2019s Jan Odijk (2022). \u201cGrETEL Tutorial.\u201d Online.","title":"GrETEL"},{"location":"tools/gretel/#gretel","text":"GrETEL is a user-friendly query engine in which linguists can use an utterance as a starting point for searching a treebank for a linguistic phenomenon. Instead of a formal, technical query instruction, it takes a natural language example as input.","title":"GrETEL"},{"location":"tools/gretel/#overview","text":"GrETEL stands for Greedy Extraction of Trees for Empirical Linguistics. GrETEL is a user-friendly query engine in which linguists can use an utterance as a starting point for searching a treebank for a linguistic phenomenon. Instead of a formal, technical query instruction, it takes a natural language example as input. This provides a convenient way for novice and non-technical users to use treebanks with a limited knowledge of the underlying syntax and formal query languages. By allowing linguists to search for constructions similar to the example they provide, GrETEL aims to bridge the gap between descriptive-theoretical and computational linguistics. The example-based query procedure consists of multiple steps: The user enters an example of the construction they are interested in. The example is parsed with Alpino , and is returned in the form of a matrix, in which the user specifies which aspects of this example are essential for the construction under investigation. This matrix is automatically converted in an XPath query, which can be manually edited or even written by the user. The user chooses the corpus on which the query must be executed. The query is executed on the selected corpus, and the matching constructions are presented to the user as a list of sentences, which can be downloaded. The user can also click on the sentences in order to visualize the results as syntax trees, in which the matching part of the tree is highlighted. The results of the query can then be efficiently analysed through the use of a pivot table. GrETEL is usable online on its webpage: https://gretel.hum.uu.nl/. Because GrETEL relies on Alpino for parsing the example utterance, it only works for Dutch corpora and example utterances. However, GrETEL is in itself not bound to any particular language, as shown by the GrETEL variant for Afrikaans , and Poly-GrETEL , which enabled simultaneous querying in multiple languages in a parallel treebank.","title":"Overview"},{"location":"tools/gretel/#data","text":"GrETEL ships with (treebanks of) the LASSY-SMALL, the CGN and the Schlichting/Van Kampen corpora for querying. GrETEL also allows users to upload their own corpus. Uploaded corpora are automatically parsed with the Alpino parser , preparing it for treebank querying. More about the LASSY-SMALL, the CGN and the Schlichting/Van Kampen corpora: LASSY-SMALL CGN (Corpus Gesproken Nederlands) Schlichting/Van Kampen corpus","title":"Data"},{"location":"tools/gretel/#learn","text":"","title":"Learn"},{"location":"tools/gretel/#instruction-and-support","text":"Most recently, a complete tutorial on GrETEL was given on Oct 7, 2022 by Jan Odijk (in Dutch). This tutorial was taped and included slides and exercises, which can be found here . The GrETEL webpage offers a wide range of information, including tutorials, documentation and FAQ. * Slides * Exercises * Manuals and documentation * FAQ * Contact the developers","title":"Instruction and support"},{"location":"tools/gretel/#local-installation","text":"While local installation is not necessary, given its web version , it is possible to install GrETEL locally. GrETEL\u2019s GitHub repository has detailed information on how to install GrETEL locally, as well as notes for users and developers.","title":"Local installation"},{"location":"tools/gretel/#user-support","text":"The current version of GrETEL is developed by the Digital Humanities Lab at Utrecht University. If you have any suggestions, questions, or general feedback you are welcome to give us a ring, or send us an email. You can find contact information on Digital Humanities Lab's website or in the footer of the GrETEL webpage .","title":"User support"},{"location":"tools/gretel/#jan-odijks-valedictory-speech","text":"In his valedictory speech called \"Taaltechnologie voor taalkundig onderzoek [Language technology for linguistic research]\", professor Jan Odijk (UU) discussed two of his more recent research projects in detail; one of which is GrETEL, the other being SASTA , which he considers a spin-off of GrETEL. The recording of this ceremony can be found here ; the transcript of his speech can be found here ( both in Dutch ).","title":"Jan Odijk's valedictory speech"},{"location":"tools/gretel/#mentions","text":"","title":"Mentions"},{"location":"tools/gretel/#key-publications","text":"Jan Odijk, Martijn van der Klis and Sheean Spoel (2018). \u201cExtensions to the GrETEL treebank query application.\u201d In: Proceedings of the 16th International Workshop on Treebanks and Linguistic Theories. Prague, Czech Republic. pp. 46-55. Liesbeth Augustinus, Vincent Vandeghinste and Frank Van Eynde (2012). \u201cExample-based treebank querying.\u201d In: Proceedings of the 8th Conference on Language Resources and Evaluation (LREC 2012). Istanbul, Turkey. pp. 3161-3167.","title":"Key publications"},{"location":"tools/gretel/#other-publications","text":"Jan Odijk (2020). De verleidingen en gevaren van GrETEL. Nederlandse taalkunde, 25(1), 7-37. Jan Odijk (2023, 30 Jan.). Taaltechnologie voor taalkundig onderzoek . Valedictory speech, Utrecht University. https://surfdrive.surf.nl/files/index.php/s/pzNHSgd6t8L0Wnk","title":"Other publications"},{"location":"tools/gretel/#webpages","text":"Source code: https://github.com/UUDigitalHumanitieslab/gretel Previous versions: * GrETEL 3: http://gretel.ccl.kuleuven.be/gretel3/ * GrETEL 2: http://gretel.ccl.kuleuven.be/gretel-2.0/ebs/input.php More publications and talks: http://gretel.ccl.kuleuven.be/project/publications.php Poly-GrETEL: http://gretel.ccl.kuleuven.be/poly-gretel/ GrETEL for Afrikaans: http://gretel.ccl.kuleuven.be/afribooms/","title":"Webpages"},{"location":"tools/gretel/#videos","text":"Jan Odijk (2022). \u201cGrETEL Tutorial.\u201d Online.","title":"Video\u2019s"},{"location":"tools/grlc/","text":"grlc grlc makes all your Linked Data accessible to the Web by automatically converting your SPARQL queries into RESTful APIs. Overview grlc is a lightweight server that takes SPARQL queries (stored in a GitHub repository, in your local filesystem, or listed in a URL), and translates them to Linked Data Web APIs. This enables universal access to Linked Data. Users are not required to know SPARQL to query their data, but instead can access a web API. grlc assumes that you have a collection of SPARQL queries as .rq files. grlc will create one API operation for each SPARQL query/.rq file in the collection. Your queries can add API parameters to each operation by using the parameter mapping syntax. This allows your query to define query variables which will be mapped to API parameters for your API operation (see here for an example). Your queries can include special decorators to add extra functionality to your API. Learn Instruction webpages The Quick Tutorial is a quick walkthrough for deploying your own Linked Data API using grlc. Mentions Articles (incl. conference papers, presentations and demo\u2019s) Albert Mero\u00f1o-Pe\u00f1uela, Rinke Hoekstra. \u201cgrlc Makes GitHub Taste Like Linked Data APIs\u201d. The Semantic Web \u2013 ESWC 2016 Satellite Events, Heraklion, Crete, Greece, May 29 \u2013 June 2, 2016, Revised Selected Papers. LNCS 9989, pp. 342-353 (2016). (PDF) Albert Mero\u00f1o-Pe\u00f1uela, Rinke Hoekstra. \u201cSPARQL2Git: Transparent SPARQL and Linked Data API Curation via Git\u201d. In: Proceedings of the 14th Extended Semantic Web Conference (ESWC 2017), Poster and Demo Track. Portoroz, Slovenia, May 28th \u2013 June 1st, 2017 (2017). (PDF) Albert Mero\u00f1o-Pe\u00f1uela, Rinke Hoekstra. \u201cAutomatic Query-centric API for Routine Access to Linked Data\u201d. In: The Semantic Web \u2013 ISWC 2017, 16th International Semantic Web Conference. Lecture Notes in Computer Science, vol 10587, pp. 334-339 (2017). (PDF) Pasquale Lisena, Albert Mero\u00f1o-Pe\u00f1uela, Tobias Kuhn, Rapha\u00ebl Troncy. \u201cEasy Web API Development with SPARQL Transformer\u201d. In: The Semantic Web \u2013 ISWC 2019, 18th International Semantic Web Conference. Lecture Notes in Computer Science, vol 11779, pp. 454-470 (2019). (PDF) Projects Teaching and Instruction","title":"grlc"},{"location":"tools/grlc/#grlc","text":"grlc makes all your Linked Data accessible to the Web by automatically converting your SPARQL queries into RESTful APIs.","title":"grlc"},{"location":"tools/grlc/#overview","text":"grlc is a lightweight server that takes SPARQL queries (stored in a GitHub repository, in your local filesystem, or listed in a URL), and translates them to Linked Data Web APIs. This enables universal access to Linked Data. Users are not required to know SPARQL to query their data, but instead can access a web API. grlc assumes that you have a collection of SPARQL queries as .rq files. grlc will create one API operation for each SPARQL query/.rq file in the collection. Your queries can add API parameters to each operation by using the parameter mapping syntax. This allows your query to define query variables which will be mapped to API parameters for your API operation (see here for an example). Your queries can include special decorators to add extra functionality to your API.","title":"Overview"},{"location":"tools/grlc/#learn","text":"","title":"Learn"},{"location":"tools/grlc/#instruction-webpages","text":"The Quick Tutorial is a quick walkthrough for deploying your own Linked Data API using grlc.","title":"Instruction webpages"},{"location":"tools/grlc/#mentions","text":"","title":"Mentions"},{"location":"tools/grlc/#articles-incl-conference-papers-presentations-and-demos","text":"Albert Mero\u00f1o-Pe\u00f1uela, Rinke Hoekstra. \u201cgrlc Makes GitHub Taste Like Linked Data APIs\u201d. The Semantic Web \u2013 ESWC 2016 Satellite Events, Heraklion, Crete, Greece, May 29 \u2013 June 2, 2016, Revised Selected Papers. LNCS 9989, pp. 342-353 (2016). (PDF) Albert Mero\u00f1o-Pe\u00f1uela, Rinke Hoekstra. \u201cSPARQL2Git: Transparent SPARQL and Linked Data API Curation via Git\u201d. In: Proceedings of the 14th Extended Semantic Web Conference (ESWC 2017), Poster and Demo Track. Portoroz, Slovenia, May 28th \u2013 June 1st, 2017 (2017). (PDF) Albert Mero\u00f1o-Pe\u00f1uela, Rinke Hoekstra. \u201cAutomatic Query-centric API for Routine Access to Linked Data\u201d. In: The Semantic Web \u2013 ISWC 2017, 16th International Semantic Web Conference. Lecture Notes in Computer Science, vol 10587, pp. 334-339 (2017). (PDF) Pasquale Lisena, Albert Mero\u00f1o-Pe\u00f1uela, Tobias Kuhn, Rapha\u00ebl Troncy. \u201cEasy Web API Development with SPARQL Transformer\u201d. In: The Semantic Web \u2013 ISWC 2019, 18th International Semantic Web Conference. Lecture Notes in Computer Science, vol 11779, pp. 454-470 (2019). (PDF)","title":"Articles (incl. conference papers, presentations and demo\u2019s)"},{"location":"tools/grlc/#projects","text":"","title":"Projects"},{"location":"tools/grlc/#teaching-and-instruction","text":"","title":"Teaching and Instruction"},{"location":"tools/hypodisc/","text":"HypoDisc This tool aims to discover hypotheses in RDF knowledge graphs by: 1) Training a multi-hop link prediction model on a graph subset, encouraging the clustering of similar context entities. 2) Using cluster centroids as representatives, ranking paths within clusters: top-k paths become the hypotheses. Overview Hypodisc supports multimodal information of various types, which, in accordance to the RDF data model, are stored as string literals with an accompanying datatype or language tag. At present, Hypodisc understands numerical and temporal datatypes, as well as natural language and other strings. Literals with language tags are also treated as strings. To discover meaningful patterns in these data, a cluster-based approach is applied to all elements of the same datatype with a certain context. See the documentation for more info. Learn Multimodal learning This pipeline includes experimental support for multimodal features, stored as literal nodes. These are disabled by default, but can be enabled by specifying one or more of the following modalities: numerical, temporal, textual, spatial, and visual. These modalities map to their corresponding XSD datatypes, or b64Image for images encoded as string literals. Note that, for literals to be considered a member of a modality they should correctly be annotated with a datatype or language tag. Find out more about how to create and run a HDF5 file (which in includes text and images) on the github . Generating Clusters t-SNE can be used to visualize the clusters in the entity embedding space. You can enable this feature, by installing t-SNE as a submodule of this repository. Detailed information on how to use the tool can be found on the github . The README.md on the tool's github contains more information.","title":"HypoDisc"},{"location":"tools/hypodisc/#hypodisc","text":"This tool aims to discover hypotheses in RDF knowledge graphs by: 1) Training a multi-hop link prediction model on a graph subset, encouraging the clustering of similar context entities. 2) Using cluster centroids as representatives, ranking paths within clusters: top-k paths become the hypotheses.","title":"HypoDisc"},{"location":"tools/hypodisc/#overview","text":"Hypodisc supports multimodal information of various types, which, in accordance to the RDF data model, are stored as string literals with an accompanying datatype or language tag. At present, Hypodisc understands numerical and temporal datatypes, as well as natural language and other strings. Literals with language tags are also treated as strings. To discover meaningful patterns in these data, a cluster-based approach is applied to all elements of the same datatype with a certain context. See the documentation for more info.","title":"Overview"},{"location":"tools/hypodisc/#learn","text":"","title":"Learn"},{"location":"tools/hypodisc/#multimodal-learning","text":"This pipeline includes experimental support for multimodal features, stored as literal nodes. These are disabled by default, but can be enabled by specifying one or more of the following modalities: numerical, temporal, textual, spatial, and visual. These modalities map to their corresponding XSD datatypes, or b64Image for images encoded as string literals. Note that, for literals to be considered a member of a modality they should correctly be annotated with a datatype or language tag. Find out more about how to create and run a HDF5 file (which in includes text and images) on the github .","title":"Multimodal learning"},{"location":"tools/hypodisc/#generating-clusters","text":"t-SNE can be used to visualize the clusters in the entity embedding space. You can enable this feature, by installing t-SNE as a submodule of this repository. Detailed information on how to use the tool can be found on the github . The README.md on the tool's github contains more information.","title":"Generating Clusters"},{"location":"tools/mediasuite/","text":"Media Suite The CLARIAH Media Suite is a research environment of the Dutch infrastructure for digital humanities and social science. It facilitates scholarly research with large Dutch media collections by providing advanced search and analysis tools. Overview The CLARIAH Media Suite is an application for doing research with data collections by scholars and students at universities and in higher education (e.g., film, television, and other media scholars, oral historians, and political historians). It consists of three building blocks: data, tools to work with the data, and a workspace to store your work with the data. The Media Suite is an innovative digital research environment, an experimental environment (LAB) , in which we are experimenting with new ways of working with multimedia data collections. It caters to various levels of expertise and research interests: from providing access to many audio-visual collections for exploratory research to close reading; and from more complex modes of data analysis to distant reading strategies. The transparent search and analysis tools that the Media Suite offers, combined with its APIs that can be used with Jupyter notebooks, allow for many new possibilities for research and represents the middle ground between full algorithmic literacy and being a data novice. Users from Dutch universities and research institutes can log into the Media Suite using their university credentials. Read more about access to the Media Suite . Data Via the CLARIAH infrastructure, the Media Suite provides access to data collections in Dutch archives (among others: The Netherlands Institute for Sound and Vision, EYE Film Museum collections, DANS oral history interview collections, collections from the Open Images Project). Typically, data collections are registered in a registry that allows the infastructure to either access collections directly or use some form of data harvesting to enables access. More about our data: What collections/data are available via the Media Suite? > How does the Media Suite make the data available? > Can I play/view all the sources that I find via the Media Suite? > Tools As a research environment, the CLARIAH Media Suite aims to support scholars in all the steps of their research process. At a general level, it provides tools for exploring the data and collections, creating personal selections (or corpora), adding annotations (such as tags, comments, links, and other metadata), and the possibility to export them. The Media Suite also facilitates working with data directly by using its APIs in combination with Jupyter Notebooks. Workspace Workspace The CLARIAH Media Suite offers a \u201cvirtual work space\u201d to its users. It allows researchers to store bookmarks, annotations, saved queries, personal collections, or automatic enrichments. The workspace thus provides researchers with novel ways for making transparent and managing their research process. Read more about the Media Suite > Learn Instruction and support The Media Suite Learn pages offer a wide set instruction and support material: Quick start guide Written, video and hybrid tutorials for different levels of user expertise, beginner\u2019s to advanced levels, based on subject or tool: Subject tutorials , Tool tutorials Frequently Asked Questions Glossary of terms Overview of example projects that have used Media Suite collections and tools in either teaching or research. Media Suite\u2019s Group Library in Zotero. Advice and user support Media Suite Learn team aims to support a wide array of approaches and areas of teaching and research, and are happy to offer advice on how to use the Media Suite productively in your own project. You can contact the team via mediastudies@clariah.nl . The team regularly contributes to organising research events that introduce the Media Suite to new users, and that reflect on digital methods and videographic approaches more broadly. Events include online webinars and public research seminars focussing on state-of-the-art digital scholarship, with contributions from scholars from the Netherlands and abroad. Public forum The Media Suite Public Forum uses Gitter , an open source instant messaging and chat room system. To start chatting, you would need to have either a Twitter or Github account. Media Suite related questions & answers Bug reports Discussions on new features and future directions Mentions Publications Aasman, S., Melgar Estrada, L., Slootweg, T. & Wegter, R., (2019). Tales of a Tool Encounter: Exploring Video Annotation for Doing Media History, VIEW Journal of European Television History and Culture, Special Issue on Audiovisual Data in Digital Humanities, eds. Pelle Snickars, Mark Williams and Andreas Fickers, Spring 2019. Open access, online multi-media article. Ashkpour, A., Merono-Penuela, A., & Mandemakers, K. (2015). The Aggregate Dutch Historical Censuses: Harmonization and RDF. Historical Methods: A Journal of Quantitative and Interdisciplinary History, 48(4), 230-245. (PDF) Bilgin, A., Sang, E.T.K., Smeenk, K., Hollink, L., van Ossenbruggen, J., Harbers, F. and Broersma, M. (2018). Utilizing a Transparency-driven Environment toward Trusted Automatic Genre Classification: A Case Study in Journalism History. Proceedings 14th International Conference on e-Science (e-Science) (pp. 486-496). IEEE. Bloothooft, G., Oosterlaken, R., Reynaert, M., Depuydt, K., Schoonheim, T. (2018). NAMES: Towards gold standards for personal names. DHBenelux conference 2018. Broersma, M., & Harbers, F. Eds. (2019). Dossier CLARIAH Media projects. Tijdschrift voor Mediageschiedenis/Journal for Media History. See all publications Presentations Erp, M. van. A philosophy of change, Institute for Social Work, Utrecht, 12 december 2018 Dijk, J. van. Towards Semantic enrichment of Newspapers: A Historical Ecology use case, Amsterdam, 28 december 2017 See all presentations Webpages \u2018Big Heritage Data\u2019 in Media Suite, Beeld en geluid Media Suite makes \u2018Big Heritage Data\u2019 accessible for research, DANS Video\u2019s Talkshow Mediasuite // Data Stories, Dutch Media Week","title":"Media Suite"},{"location":"tools/mediasuite/#media-suite","text":"The CLARIAH Media Suite is a research environment of the Dutch infrastructure for digital humanities and social science. It facilitates scholarly research with large Dutch media collections by providing advanced search and analysis tools.","title":"Media Suite"},{"location":"tools/mediasuite/#overview","text":"The CLARIAH Media Suite is an application for doing research with data collections by scholars and students at universities and in higher education (e.g., film, television, and other media scholars, oral historians, and political historians). It consists of three building blocks: data, tools to work with the data, and a workspace to store your work with the data. The Media Suite is an innovative digital research environment, an experimental environment (LAB) , in which we are experimenting with new ways of working with multimedia data collections. It caters to various levels of expertise and research interests: from providing access to many audio-visual collections for exploratory research to close reading; and from more complex modes of data analysis to distant reading strategies. The transparent search and analysis tools that the Media Suite offers, combined with its APIs that can be used with Jupyter notebooks, allow for many new possibilities for research and represents the middle ground between full algorithmic literacy and being a data novice. Users from Dutch universities and research institutes can log into the Media Suite using their university credentials. Read more about access to the Media Suite .","title":"Overview"},{"location":"tools/mediasuite/#data","text":"Via the CLARIAH infrastructure, the Media Suite provides access to data collections in Dutch archives (among others: The Netherlands Institute for Sound and Vision, EYE Film Museum collections, DANS oral history interview collections, collections from the Open Images Project). Typically, data collections are registered in a registry that allows the infastructure to either access collections directly or use some form of data harvesting to enables access. More about our data: What collections/data are available via the Media Suite? > How does the Media Suite make the data available? > Can I play/view all the sources that I find via the Media Suite? >","title":"Data"},{"location":"tools/mediasuite/#tools","text":"As a research environment, the CLARIAH Media Suite aims to support scholars in all the steps of their research process. At a general level, it provides tools for exploring the data and collections, creating personal selections (or corpora), adding annotations (such as tags, comments, links, and other metadata), and the possibility to export them. The Media Suite also facilitates working with data directly by using its APIs in combination with Jupyter Notebooks. Workspace","title":"Tools"},{"location":"tools/mediasuite/#workspace","text":"The CLARIAH Media Suite offers a \u201cvirtual work space\u201d to its users. It allows researchers to store bookmarks, annotations, saved queries, personal collections, or automatic enrichments. The workspace thus provides researchers with novel ways for making transparent and managing their research process. Read more about the Media Suite >","title":"Workspace"},{"location":"tools/mediasuite/#learn","text":"","title":"Learn"},{"location":"tools/mediasuite/#instruction-and-support","text":"The Media Suite Learn pages offer a wide set instruction and support material: Quick start guide Written, video and hybrid tutorials for different levels of user expertise, beginner\u2019s to advanced levels, based on subject or tool: Subject tutorials , Tool tutorials Frequently Asked Questions Glossary of terms Overview of example projects that have used Media Suite collections and tools in either teaching or research. Media Suite\u2019s Group Library in Zotero.","title":"Instruction and support"},{"location":"tools/mediasuite/#advice-and-user-support","text":"Media Suite Learn team aims to support a wide array of approaches and areas of teaching and research, and are happy to offer advice on how to use the Media Suite productively in your own project. You can contact the team via mediastudies@clariah.nl . The team regularly contributes to organising research events that introduce the Media Suite to new users, and that reflect on digital methods and videographic approaches more broadly. Events include online webinars and public research seminars focussing on state-of-the-art digital scholarship, with contributions from scholars from the Netherlands and abroad.","title":"Advice and user support"},{"location":"tools/mediasuite/#public-forum","text":"The Media Suite Public Forum uses Gitter , an open source instant messaging and chat room system. To start chatting, you would need to have either a Twitter or Github account. Media Suite related questions & answers Bug reports Discussions on new features and future directions","title":"Public forum"},{"location":"tools/mediasuite/#mentions","text":"","title":"Mentions"},{"location":"tools/mediasuite/#publications","text":"Aasman, S., Melgar Estrada, L., Slootweg, T. & Wegter, R., (2019). Tales of a Tool Encounter: Exploring Video Annotation for Doing Media History, VIEW Journal of European Television History and Culture, Special Issue on Audiovisual Data in Digital Humanities, eds. Pelle Snickars, Mark Williams and Andreas Fickers, Spring 2019. Open access, online multi-media article. Ashkpour, A., Merono-Penuela, A., & Mandemakers, K. (2015). The Aggregate Dutch Historical Censuses: Harmonization and RDF. Historical Methods: A Journal of Quantitative and Interdisciplinary History, 48(4), 230-245. (PDF) Bilgin, A., Sang, E.T.K., Smeenk, K., Hollink, L., van Ossenbruggen, J., Harbers, F. and Broersma, M. (2018). Utilizing a Transparency-driven Environment toward Trusted Automatic Genre Classification: A Case Study in Journalism History. Proceedings 14th International Conference on e-Science (e-Science) (pp. 486-496). IEEE. Bloothooft, G., Oosterlaken, R., Reynaert, M., Depuydt, K., Schoonheim, T. (2018). NAMES: Towards gold standards for personal names. DHBenelux conference 2018. Broersma, M., & Harbers, F. Eds. (2019). Dossier CLARIAH Media projects. Tijdschrift voor Mediageschiedenis/Journal for Media History. See all publications","title":"Publications"},{"location":"tools/mediasuite/#presentations","text":"Erp, M. van. A philosophy of change, Institute for Social Work, Utrecht, 12 december 2018 Dijk, J. van. Towards Semantic enrichment of Newspapers: A Historical Ecology use case, Amsterdam, 28 december 2017 See all presentations","title":"Presentations"},{"location":"tools/mediasuite/#webpages","text":"\u2018Big Heritage Data\u2019 in Media Suite, Beeld en geluid Media Suite makes \u2018Big Heritage Data\u2019 accessible for research, DANS","title":"Webpages"},{"location":"tools/mediasuite/#videos","text":"Talkshow Mediasuite // Data Stories, Dutch Media Week","title":"Video\u2019s"},{"location":"tools/opensonar/","text":"OpenSoNaR OpenSoNaR is an easy-to-use online corpus retrieval system that allows for analyzing and searching the SoNaR and CGN corpora. Overview OpenSoNaR is an online application for exploration of and searching in the SoNaR and CGN corpora. In the Exploration (Dutch: verken) interface one can investigate corpus distributions, request statistics from sub-corpora, retrieve n-grams from sub-corpora and search for specific documents. In the Search (Dutch: zoek) interface one can use four different search strategies: simple (simpel), extended (uitgebreid), advanced (geavanceerd) or expert. OpenSoNaR can be accessed on (https://opensonar.ivdnt.org/). To use OpenSoNaR, an account is required. Employees of universities or research institutes from the Netherlands can log in with the user ID and password of their own organization. If you do not have an account at an academic institute, please apply for an account at clarin.eu . The SoNaR corpus contains more than 500 million words of text from various domains and genres. All texts were tokenised, POS tagged and lemmatised. The named entities were also labelled. All annotations of SoNaR were produced automatically. The Corpus of Spoken Dutch (Corpus Gesproken Nederlands, CGN) is a collection of 900 hours (almost 9 million words) of contemporary Dutch speech, originating from Flemish and Dutch speakers. The speech fragments (spontaneous and prepared) are aligned with various transcriptions (including orthographic, phonetic) and annotations (lemma, POS tags). All annotations have been verified manually, except for the phonetic transcription: only 11,3% was verified. Due to the size of the corpora the number of hits shown in OpenSoNaR is limited to 8 million hits. If the results of your query exceeds this limit only the first 8,000,000 hits will be shown. Learn Instructional webpages and manuals OpenSoNaR has a built-in page guide with four steps, found on the top right of the home page of the tool. A more detailed application manual can be found here . Note: access to this manual requires the user to log in to the application. A resource webpage including a manual to the SoNaR corpus can be found here . A resource webpage including a manual to the CGN corpus can be found here . Workshop and tutorial The Week van het Nederlands hosted a tutorial on using OpenSoNaR on October 9, 2020. The videos of this tutorial can be found here . The slides and exercises accompanying said tutorial can be found here . Mentions The application is a web-based frontend for the BlackLab search engine for corpora with token-based annotation. The current frontend is a further development of the corpus-frontend application developed by INT and its design is inspired by the first version of the OpenSoNaR user interface by Tilburg and Radboud University . Publications Oostdijk, N., Reynaert, M., Hoste, V., & Schuurman, I. (2013). The construction of a 500-million-word reference corpus of contemporary written Dutch. Essential speech and language technology for Dutch: Results by the STEVIN programme, 219-247. Corpus Gesproken Nederlands - CGN (Version 2.0.3) (2014) [Data set]. Available at the Dutch Language Institute: (http://hdl.handle.net/10032/tm-a2-k6) Reynaert, M., Camp, M. V. D., & Zaanen, M. V. (2014). OpenSoNaR: user-driven development of the SoNaR corpus interfaces. Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: System Demonstrations, 124\u2013128. (https://aclanthology.org/C14-2027.pdf) Webpages (https://opensonar.ivdnt.org/)","title":"OpenSoNaR"},{"location":"tools/opensonar/#opensonar","text":"OpenSoNaR is an easy-to-use online corpus retrieval system that allows for analyzing and searching the SoNaR and CGN corpora.","title":"OpenSoNaR"},{"location":"tools/opensonar/#overview","text":"OpenSoNaR is an online application for exploration of and searching in the SoNaR and CGN corpora. In the Exploration (Dutch: verken) interface one can investigate corpus distributions, request statistics from sub-corpora, retrieve n-grams from sub-corpora and search for specific documents. In the Search (Dutch: zoek) interface one can use four different search strategies: simple (simpel), extended (uitgebreid), advanced (geavanceerd) or expert. OpenSoNaR can be accessed on (https://opensonar.ivdnt.org/). To use OpenSoNaR, an account is required. Employees of universities or research institutes from the Netherlands can log in with the user ID and password of their own organization. If you do not have an account at an academic institute, please apply for an account at clarin.eu . The SoNaR corpus contains more than 500 million words of text from various domains and genres. All texts were tokenised, POS tagged and lemmatised. The named entities were also labelled. All annotations of SoNaR were produced automatically. The Corpus of Spoken Dutch (Corpus Gesproken Nederlands, CGN) is a collection of 900 hours (almost 9 million words) of contemporary Dutch speech, originating from Flemish and Dutch speakers. The speech fragments (spontaneous and prepared) are aligned with various transcriptions (including orthographic, phonetic) and annotations (lemma, POS tags). All annotations have been verified manually, except for the phonetic transcription: only 11,3% was verified. Due to the size of the corpora the number of hits shown in OpenSoNaR is limited to 8 million hits. If the results of your query exceeds this limit only the first 8,000,000 hits will be shown.","title":"Overview"},{"location":"tools/opensonar/#learn","text":"","title":"Learn"},{"location":"tools/opensonar/#instructional-webpages-and-manuals","text":"OpenSoNaR has a built-in page guide with four steps, found on the top right of the home page of the tool. A more detailed application manual can be found here . Note: access to this manual requires the user to log in to the application. A resource webpage including a manual to the SoNaR corpus can be found here . A resource webpage including a manual to the CGN corpus can be found here .","title":"Instructional webpages and manuals"},{"location":"tools/opensonar/#workshop-and-tutorial","text":"The Week van het Nederlands hosted a tutorial on using OpenSoNaR on October 9, 2020. The videos of this tutorial can be found here . The slides and exercises accompanying said tutorial can be found here .","title":"Workshop and tutorial"},{"location":"tools/opensonar/#mentions","text":"The application is a web-based frontend for the BlackLab search engine for corpora with token-based annotation. The current frontend is a further development of the corpus-frontend application developed by INT and its design is inspired by the first version of the OpenSoNaR user interface by Tilburg and Radboud University .","title":"Mentions"},{"location":"tools/opensonar/#publications","text":"Oostdijk, N., Reynaert, M., Hoste, V., & Schuurman, I. (2013). The construction of a 500-million-word reference corpus of contemporary written Dutch. Essential speech and language technology for Dutch: Results by the STEVIN programme, 219-247. Corpus Gesproken Nederlands - CGN (Version 2.0.3) (2014) [Data set]. Available at the Dutch Language Institute: (http://hdl.handle.net/10032/tm-a2-k6) Reynaert, M., Camp, M. V. D., & Zaanen, M. V. (2014). OpenSoNaR: user-driven development of the SoNaR corpus interfaces. Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: System Demonstrations, 124\u2013128. (https://aclanthology.org/C14-2027.pdf)","title":"Publications"},{"location":"tools/opensonar/#webpages","text":"(https://opensonar.ivdnt.org/)","title":"Webpages"},{"location":"tools/paqu/","text":"PaQu \u2013 Parse and Query The PaQu web service makes it possible to search in syntactically annotated corpora in Dutch. You can parse your own Dutch text corpus or use one of two corpora provided by the developers. Overview PaQu uses the Alpino parser to make treebanks of your own text corpus, and to search in these treebanks using an interface based on the LASSY Word Relations Search interface . Uploading one's own corpus requires a log-in. Two treebanks are already available in the application: Lassy Klein (1M words, manually checked syntactic analysis) and Lassy Groot (700M words, syntactic analysis automatically assigned by Alpino). PaQu offers two ways to search through the syntactically annotated texts. The first option is to use the search bar to look for word pairs, optionally complemented by their syntactic relationship. The second search option is to use the query language XPath. Learn Preliminaries \u2013 Alpino annotations Using PaQu requires one to be familiar with the Alpino annotation guidelines. PaQu's own information page gives a good brief overview of all syntactic relations. For a detailed description of the syntactic annotations used by Alpino, one should check the document: Lassy Syntactische Annotatie . For the annotation of parts-of-speech and lemmas, one should check the document: Part of speech tagging en lemmatisering van het D-coi corpus . These documents are, however, only available in Dutch. The following document (in English) may also be useful: Manual for syntactic annotators . Querying with PaQu PaQu offers two ways to search through the syntactically annotated texts. The first option is to use the search bar to look for word pairs, optionally complemented by their syntactic relationship. For instance, in the image below, PaQu is told to look for the adjective (adj) taalkundig 'linguistic' that is a modifier (mod) to the noun (n) onderzoek 'research'. As said above, to use this effectively, the user must be familiar with the Alpino annotation guidelines, for which see the links provided under Preliminaries. Also check out PaQu's own information page for more tips and tricks on search for word pairs (in Dutch). The second search option is to use the query language XPath. In the image below, the XPath query tells PaQu to look for any instance of the verb zoeken 'search' that takes a PP-complement headed by the preposition in . Using the more advanced XPath querying method allows the user to search for more complex structures, however requires them to be familiar with the Alpino annotations, as well as XPath. A 'cookbook' for writing XPath queries for Alpino-annotated treebanks can be found here (in English). Also check out PaQu's own information page for more tips and tricks on using XPath in PaQu (in Dutch). Local Installation PaQu can be installed locally. For this, we refer to the tool's GitHub page (in Dutch). Contact Project leader: Gertjan van Noord Developer: Peter Kleiweg Mentions Publications Odijk, Jan, van Noord, Gertjan, Kleiweg, Peter & Tjong Kim Sang, Erik. (2017). The Parse and Query (PaQu) Application . In: Odijk, Jan & van Hessen, Arjan (eds.) CLARIN in the Low Countries (pp. 281-297). Ubiquity Press, London. https://doi.org/10.5334/bbi.23 Webpages PaQu webservice GitHub page Related tools and resources Alpino is a parser for Dutch, used by PaQu. GrETEL is a tool to query-by-example corpora and treebanks that were parsed by Alpino. The Lassy corpus was parsed with Alpino. The Lassy Klein subcorpus was manually corrected. Van Noord, Gertjan, Bouma, Gosse, Van Eynde, Frank, De Kok, Dani\u00ebl, Van der Linde, Jelmer, Schuurman, Ineke, Tjong Kim Sang, Erik, & Vandeghinste, Vincent (2013). Large scale syntactic annotation of written Dutch: Lassy. In Peter Spyns, & Jan Odijk (Eds.), Essential speech and language technology for Dutch: Results by the STEVIN programme (pp. 147-164). Springer Berlin, Heidelberg. https://doi.org/10.1007/978-3-642-30910-6","title":"PaQu \u2013 Parse and Query"},{"location":"tools/paqu/#paqu-parse-and-query","text":"The PaQu web service makes it possible to search in syntactically annotated corpora in Dutch. You can parse your own Dutch text corpus or use one of two corpora provided by the developers.","title":"PaQu \u2013 Parse and Query"},{"location":"tools/paqu/#overview","text":"PaQu uses the Alpino parser to make treebanks of your own text corpus, and to search in these treebanks using an interface based on the LASSY Word Relations Search interface . Uploading one's own corpus requires a log-in. Two treebanks are already available in the application: Lassy Klein (1M words, manually checked syntactic analysis) and Lassy Groot (700M words, syntactic analysis automatically assigned by Alpino). PaQu offers two ways to search through the syntactically annotated texts. The first option is to use the search bar to look for word pairs, optionally complemented by their syntactic relationship. The second search option is to use the query language XPath.","title":"Overview"},{"location":"tools/paqu/#learn","text":"","title":"Learn"},{"location":"tools/paqu/#preliminaries-alpino-annotations","text":"Using PaQu requires one to be familiar with the Alpino annotation guidelines. PaQu's own information page gives a good brief overview of all syntactic relations. For a detailed description of the syntactic annotations used by Alpino, one should check the document: Lassy Syntactische Annotatie . For the annotation of parts-of-speech and lemmas, one should check the document: Part of speech tagging en lemmatisering van het D-coi corpus . These documents are, however, only available in Dutch. The following document (in English) may also be useful: Manual for syntactic annotators .","title":"Preliminaries \u2013 Alpino annotations"},{"location":"tools/paqu/#querying-with-paqu","text":"PaQu offers two ways to search through the syntactically annotated texts. The first option is to use the search bar to look for word pairs, optionally complemented by their syntactic relationship. For instance, in the image below, PaQu is told to look for the adjective (adj) taalkundig 'linguistic' that is a modifier (mod) to the noun (n) onderzoek 'research'. As said above, to use this effectively, the user must be familiar with the Alpino annotation guidelines, for which see the links provided under Preliminaries. Also check out PaQu's own information page for more tips and tricks on search for word pairs (in Dutch). The second search option is to use the query language XPath. In the image below, the XPath query tells PaQu to look for any instance of the verb zoeken 'search' that takes a PP-complement headed by the preposition in . Using the more advanced XPath querying method allows the user to search for more complex structures, however requires them to be familiar with the Alpino annotations, as well as XPath. A 'cookbook' for writing XPath queries for Alpino-annotated treebanks can be found here (in English). Also check out PaQu's own information page for more tips and tricks on using XPath in PaQu (in Dutch).","title":"Querying with PaQu"},{"location":"tools/paqu/#local-installation","text":"PaQu can be installed locally. For this, we refer to the tool's GitHub page (in Dutch).","title":"Local Installation"},{"location":"tools/paqu/#contact","text":"Project leader: Gertjan van Noord Developer: Peter Kleiweg","title":"Contact"},{"location":"tools/paqu/#mentions","text":"","title":"Mentions"},{"location":"tools/paqu/#publications","text":"Odijk, Jan, van Noord, Gertjan, Kleiweg, Peter & Tjong Kim Sang, Erik. (2017). The Parse and Query (PaQu) Application . In: Odijk, Jan & van Hessen, Arjan (eds.) CLARIN in the Low Countries (pp. 281-297). Ubiquity Press, London. https://doi.org/10.5334/bbi.23","title":"Publications"},{"location":"tools/paqu/#webpages","text":"PaQu webservice GitHub page","title":"Webpages"},{"location":"tools/paqu/#related-tools-and-resources","text":"Alpino is a parser for Dutch, used by PaQu. GrETEL is a tool to query-by-example corpora and treebanks that were parsed by Alpino. The Lassy corpus was parsed with Alpino. The Lassy Klein subcorpus was manually corrected. Van Noord, Gertjan, Bouma, Gosse, Van Eynde, Frank, De Kok, Dani\u00ebl, Van der Linde, Jelmer, Schuurman, Ineke, Tjong Kim Sang, Erik, & Vandeghinste, Vincent (2013). Large scale syntactic annotation of written Dutch: Lassy. In Peter Spyns, & Jan Odijk (Eds.), Essential speech and language technology for Dutch: Results by the STEVIN programme (pp. 147-164). Springer Berlin, Heidelberg. https://doi.org/10.1007/978-3-642-30910-6","title":"Related tools and resources"},{"location":"tools/sasta/","text":"SASTA SASTA is a tool for the analysis of spontaneous language transcripts, to aid clinical linguists and research into language development and language disorders. SASTA analyzes a transcript grammatically using Alpino , an automatic utterance parser for Dutch, and can recognize a significant number of forms of deviant language use and analyze them correctly, following multiple assessment methods available for Dutch ( TARSP , STAP and ASTA ). Overview SASTA can analyze transcripts following multiple assessment methods available for Dutch: TARSP (Schlichting 2005, 2017) for young children (1\u20134 years), inspired by LARSP for English (Crystal et al. 1989); STAP (Verbeek et al. 2007, van Ierland et al. 2008) for older children (4\u20138 years); ASTA (Boxum et al. 2013) for adults suffering from aphasia. SASTA generates as output a method-specific form and an annotated transcript. The generated transcript can be corrected by a linguist, if needed, and re-uploaded into SASTA, after which SASTA generates an adapted method-specific form. Overall, SASTA achieves an accuracy between 88 and 95% on training data for TARSP and STAP. SASTA accepts as input transcripts in MS Word or plain text (given some SASTA-specific requirements), as well as CHAT (MacWhinney 2000), and uses AuCHAnn to generate valid CHAT files for transcripts accompanied by an interpretation, which significantly improves results. SASTA analyzes a transcript grammatically using Alpino . It then uses specially constructed (XPath) queries for all measures defined within the assessment method to count the frequencies of linguistic phenomena in the spontaneous language sample. As such, SASTA may be considered a spin-off of GrETEL , that can be used to investigate syntactic phenomena using query-by-example. Further development of SASTA is ongoing, in close collaboration with researchers in language development and with linguists in clinics. Using SASTA requires a login. An account can be created using the Sign up button on the SASTA homepage . Documentation Documentation on how to use SASTA can be found here ( Dutch only ). SASTA's code is itself available on Github , where it is accompanied by more context and technical documentation. sastadev SASTA relies on a Python package called sastadev in the backend. This package is freely available on Github , with documentation available on Read the Docs . Learn Documentation Documentation on how to use SASTA can be found here ( Dutch only ). SASTA's code is itself available on Github , where it is accompanied by more context and documentation. Jan Odijk's valedictory speech In his valedictory speech called \"Taaltechnologie voor taalkundig onderzoek [Language technology for linguistic research]\", professor Jan Odijk (UU), SASTA's project lead, discussed two of his more recent research projects in detail; one of which is SASTA, the other being GrETEL , of which he considers SASTA a spin-off. The recording of this ceremony can be found here ; the transcript of his speech can be found here ( both in Dutch ). Lectures on sastadev Professor Jan Odijk recently gave three lectures on sastadev ( in Dutch ). These lectures were recorded and made fully available on SURFdrive including additional materials: * Lecture 1 : The syntactic structures in Alpino * Lecture 2 : Implementation of the methods (TARSP, STAP, ASTA) * Lecture 3 : Analysis of deviant language Mentions Pipeline In order to move toward implementation of SASTA in clinical practice and beyond, improving and extending the application is ongoing. In collaboration with the Koninklijke Auris Groep and commercial partner ITSLanguage, a pipeline is actively being developed , consisting of the following tools: * TT (ITSLanguage): TT supports the linguist in transcribing an uploaded audio or video fragment containing spontaneous language in CHAT format using automatic speech recognition. * SASTA (Utrecht University): SASTA analyzes a transcript grammatically and can recognize a significant number of forms of deviant language use and analyze them correctly, following multiple assessment methods available for Dutch (TARSP, STAP and ASTA). * Stamper (Anouk Scheffer, PhD candidate at Utrecht University/Auris): Stamper analyzes SASTA's output, by grouping various structures and making easy-to-use overviews. Publications on SASTA Odijk, J. (2021). Towards Semi-Automatic Analysis of Spontaneous Language for Dutch. In Selected papers from the CLARIN Annual Conference 2020 (Vol. 180, pp. 165-175). (Link\u00f6ping Electronic Conference Proceedings). Link\u00f6ping University Press. https://doi.org/10.3384/ecp18018 Renckens, E., & Odijk, J. (2021). Online tool SASTA analyseert taal. eData & Research , 15 (2), 7-7. https://edata.nl/2021/02/10/online-tool-sasta-analyseert-taal/ Other relevant publications Boxum, E., van der Scheer, F. and Zwaga, M. (2013). ASTA: Analyse voor Spontane Taal bij Afasie (4th ed.). Vereniging voor Klinische Lingu\u00efstiek. Crystal, D., Fletcher, P. and Garman, M. (1989). Grammatical Analysis of Language Disability (2nd ed.). London: Cole and Whurr. https://hdl.handle.net/10092/17651 van Ierland, M., Verbeek, J. and van den Dungen, L. (2008). Spontane Taal Analyse Procedure: Handleiding van het STAP-instrument . Universiteit van Amsterdam. MacWhinney, B. (2000). The CHILDES project: Tools for analyzing talk: Transcription format and programs (3rd ed.). Lawrence Erlbaum Associates Publishers. Odijk, J. (2023, 30 Jan.). Taaltechnologie voor taalkundig onderzoek . Valedictory speech, Utrecht University. https://surfdrive.surf.nl/files/index.php/s/pzNHSgd6t8L0Wnk Schlichting, L. (2005). TARSP: Taal Analyse Remedi\u00ebring en Screening Procedure: Taalontwikkelingsschaal van Nederlandse kinderen van 1\u20134 jaar (7th ed.). Amsterdam: Pearson. ISBN 978 90 265 1355 8. Schlichting, L. (2017). TARSP: Taal analyse remedie\u0308ring en screening procedure: Taalontwikkelingsschaal Van Nederlandse Kinderen van 1\u20134 Jaar met Aanvullende Structuren tot 6 jaar (8th ed.). Amsterdam: Pearson. ISBN 978 90 430 3561 3. Verbeek, J., van Ierland, M. and van den Dungen, L. (2007). Spontane Taal Analyse Procedure: Verantwoording van het STAP-instrument . Universiteit van Amsterdam. Credits and Contact Information SASTA is being actively developed by Utrecht University, and maintained by its Centre for Digital Humanities \u2013 Research Software Lab . SASTA is the result of close collaboration of several institutes, involving UU and the Dutch Association of Clinical Linguists (VKL), but also Hogeschool Utrecht, CLARIAH, Auris and Stichting Taaltechnologie, firmly embedding it among interested clinical linguists. For questions and feedback, please contact: Centre for Digital Humanities \u2013 Research Software Lab . Webpages SASTA main page SASTA documentation SASTA GitHub page sastadev PyPI page sastadev Github page sastadev documentation Other AuCHAnn Alpino GrETEL","title":"SASTA"},{"location":"tools/sasta/#sasta","text":"SASTA is a tool for the analysis of spontaneous language transcripts, to aid clinical linguists and research into language development and language disorders. SASTA analyzes a transcript grammatically using Alpino , an automatic utterance parser for Dutch, and can recognize a significant number of forms of deviant language use and analyze them correctly, following multiple assessment methods available for Dutch ( TARSP , STAP and ASTA ).","title":"SASTA"},{"location":"tools/sasta/#overview","text":"SASTA can analyze transcripts following multiple assessment methods available for Dutch: TARSP (Schlichting 2005, 2017) for young children (1\u20134 years), inspired by LARSP for English (Crystal et al. 1989); STAP (Verbeek et al. 2007, van Ierland et al. 2008) for older children (4\u20138 years); ASTA (Boxum et al. 2013) for adults suffering from aphasia. SASTA generates as output a method-specific form and an annotated transcript. The generated transcript can be corrected by a linguist, if needed, and re-uploaded into SASTA, after which SASTA generates an adapted method-specific form. Overall, SASTA achieves an accuracy between 88 and 95% on training data for TARSP and STAP. SASTA accepts as input transcripts in MS Word or plain text (given some SASTA-specific requirements), as well as CHAT (MacWhinney 2000), and uses AuCHAnn to generate valid CHAT files for transcripts accompanied by an interpretation, which significantly improves results. SASTA analyzes a transcript grammatically using Alpino . It then uses specially constructed (XPath) queries for all measures defined within the assessment method to count the frequencies of linguistic phenomena in the spontaneous language sample. As such, SASTA may be considered a spin-off of GrETEL , that can be used to investigate syntactic phenomena using query-by-example. Further development of SASTA is ongoing, in close collaboration with researchers in language development and with linguists in clinics. Using SASTA requires a login. An account can be created using the Sign up button on the SASTA homepage .","title":"Overview"},{"location":"tools/sasta/#documentation","text":"Documentation on how to use SASTA can be found here ( Dutch only ). SASTA's code is itself available on Github , where it is accompanied by more context and technical documentation.","title":"Documentation"},{"location":"tools/sasta/#sastadev","text":"SASTA relies on a Python package called sastadev in the backend. This package is freely available on Github , with documentation available on Read the Docs .","title":"sastadev"},{"location":"tools/sasta/#learn","text":"","title":"Learn"},{"location":"tools/sasta/#documentation_1","text":"Documentation on how to use SASTA can be found here ( Dutch only ). SASTA's code is itself available on Github , where it is accompanied by more context and documentation.","title":"Documentation"},{"location":"tools/sasta/#jan-odijks-valedictory-speech","text":"In his valedictory speech called \"Taaltechnologie voor taalkundig onderzoek [Language technology for linguistic research]\", professor Jan Odijk (UU), SASTA's project lead, discussed two of his more recent research projects in detail; one of which is SASTA, the other being GrETEL , of which he considers SASTA a spin-off. The recording of this ceremony can be found here ; the transcript of his speech can be found here ( both in Dutch ).","title":"Jan Odijk's valedictory speech"},{"location":"tools/sasta/#lectures-on-sastadev","text":"Professor Jan Odijk recently gave three lectures on sastadev ( in Dutch ). These lectures were recorded and made fully available on SURFdrive including additional materials: * Lecture 1 : The syntactic structures in Alpino * Lecture 2 : Implementation of the methods (TARSP, STAP, ASTA) * Lecture 3 : Analysis of deviant language","title":"Lectures on sastadev"},{"location":"tools/sasta/#mentions","text":"","title":"Mentions"},{"location":"tools/sasta/#pipeline","text":"In order to move toward implementation of SASTA in clinical practice and beyond, improving and extending the application is ongoing. In collaboration with the Koninklijke Auris Groep and commercial partner ITSLanguage, a pipeline is actively being developed , consisting of the following tools: * TT (ITSLanguage): TT supports the linguist in transcribing an uploaded audio or video fragment containing spontaneous language in CHAT format using automatic speech recognition. * SASTA (Utrecht University): SASTA analyzes a transcript grammatically and can recognize a significant number of forms of deviant language use and analyze them correctly, following multiple assessment methods available for Dutch (TARSP, STAP and ASTA). * Stamper (Anouk Scheffer, PhD candidate at Utrecht University/Auris): Stamper analyzes SASTA's output, by grouping various structures and making easy-to-use overviews.","title":"Pipeline"},{"location":"tools/sasta/#publications-on-sasta","text":"Odijk, J. (2021). Towards Semi-Automatic Analysis of Spontaneous Language for Dutch. In Selected papers from the CLARIN Annual Conference 2020 (Vol. 180, pp. 165-175). (Link\u00f6ping Electronic Conference Proceedings). Link\u00f6ping University Press. https://doi.org/10.3384/ecp18018 Renckens, E., & Odijk, J. (2021). Online tool SASTA analyseert taal. eData & Research , 15 (2), 7-7. https://edata.nl/2021/02/10/online-tool-sasta-analyseert-taal/","title":"Publications on SASTA"},{"location":"tools/sasta/#other-relevant-publications","text":"Boxum, E., van der Scheer, F. and Zwaga, M. (2013). ASTA: Analyse voor Spontane Taal bij Afasie (4th ed.). Vereniging voor Klinische Lingu\u00efstiek. Crystal, D., Fletcher, P. and Garman, M. (1989). Grammatical Analysis of Language Disability (2nd ed.). London: Cole and Whurr. https://hdl.handle.net/10092/17651 van Ierland, M., Verbeek, J. and van den Dungen, L. (2008). Spontane Taal Analyse Procedure: Handleiding van het STAP-instrument . Universiteit van Amsterdam. MacWhinney, B. (2000). The CHILDES project: Tools for analyzing talk: Transcription format and programs (3rd ed.). Lawrence Erlbaum Associates Publishers. Odijk, J. (2023, 30 Jan.). Taaltechnologie voor taalkundig onderzoek . Valedictory speech, Utrecht University. https://surfdrive.surf.nl/files/index.php/s/pzNHSgd6t8L0Wnk Schlichting, L. (2005). TARSP: Taal Analyse Remedi\u00ebring en Screening Procedure: Taalontwikkelingsschaal van Nederlandse kinderen van 1\u20134 jaar (7th ed.). Amsterdam: Pearson. ISBN 978 90 265 1355 8. Schlichting, L. (2017). TARSP: Taal analyse remedie\u0308ring en screening procedure: Taalontwikkelingsschaal Van Nederlandse Kinderen van 1\u20134 Jaar met Aanvullende Structuren tot 6 jaar (8th ed.). Amsterdam: Pearson. ISBN 978 90 430 3561 3. Verbeek, J., van Ierland, M. and van den Dungen, L. (2007). Spontane Taal Analyse Procedure: Verantwoording van het STAP-instrument . Universiteit van Amsterdam.","title":"Other relevant publications"},{"location":"tools/sasta/#credits-and-contact-information","text":"SASTA is being actively developed by Utrecht University, and maintained by its Centre for Digital Humanities \u2013 Research Software Lab . SASTA is the result of close collaboration of several institutes, involving UU and the Dutch Association of Clinical Linguists (VKL), but also Hogeschool Utrecht, CLARIAH, Auris and Stichting Taaltechnologie, firmly embedding it among interested clinical linguists. For questions and feedback, please contact: Centre for Digital Humanities \u2013 Research Software Lab .","title":"Credits and Contact Information"},{"location":"tools/sasta/#webpages","text":"SASTA main page SASTA documentation SASTA GitHub page sastadev PyPI page sastadev Github page sastadev documentation","title":"Webpages"},{"location":"tools/sasta/#other","text":"AuCHAnn Alpino GrETEL","title":"Other"},{"location":"tools/stam/","text":"STAM Stand-off Text Annotation Model (STAM) is a data model for stand-off-text annotation where any information on a text is represented as an annotation. Overview STAM is a minimalist data model for stand-off text annotation. Any information on a text is represented an annotation, which can be any kind of remarks, classifications, or tags on specific portions of the text or the entire resource. Annotations can also point to other annotations (higher-order annotations). STAM does not define specific vocabularies and accepts plain text as its base resource. It is independent of complex data models like RDF, W3C Web Annotations, TEI, or FoLiA. STAM aims to be a functional and practical solution, allowing users to use use vocabularies that are formalised elsewhere. STAM is primarily intended as a model for data representation, and less so as a format for data interchange. It is designed in such as way that an efficient implementation (both speed & memory) is feasible. Goals/characteristics of STAM are: Simplicity - the data model must be easy to understand for a user/developer to use and only contain what is needed, not more. We provide a minimal foundation upon which other projects can build more complex solutions. These are deliberately kept out of STAM itself. The notion that everything is an annotation is at the core of STAM and one of the things that keeps it simple. Separation from semantics - The data model does not commit to any vocabulary or annotation paradigm. It must be flexible enough to express whatever annotation paradigm a researcher wants to use, yet provide the facilities to be specific enough for practical purposes. The model basically allows for any kind of directed or undirected graph. Standalone - No dependency on other data models (e.g. RDF) aside from Unicode and JSON for serialisation, no dependency on any software services. Practical - Rather than provide a theoretical framework, we primarily aim to provide a practical specification and actual low-level tooling you can get to work with right away. Performant - The data model is set up in such a way that it allows for efficient/performant implementations, with regard to processing requirements but especially memory consumption. The model should be suitable for big data (millions of annotations). We sit at a point where we deem to have an optimal trade-off between simplicity and performance. Import & Export - Reads/writes a simple JSON format. But also designed with export to more complex formats in mind (such as W3C Web Annotations / RDF) and imports from common formats such as TSV and CONLL. Note that although STAM puts no constraints on annotation paradigms and vocabularies, higher data models may. The name STAM, an acronym for \" Stand-off Text Annotation Model \", is Dutch, Swedish, Afrikaans and Frisian for \" trunk \" (as in the trunk of a tree), the name itself depicts a solid foundation upon which more elaborate solutions can be built. If you want to learn more, please have a look at the specification on project's github page and at the implementations mentioned below: Implementations There are currently two implementations for STAM: stam-rust - A STAM library written in Rust, aims to be a full STAM implementation with high performance and memory-based storage model. stam-python - A STAM library for Python. This is not an independent implementation but it is a Python binding to the above Rust library. Furthermore, there is also the following implementation that builds upon the primary STAM library: stam-tools - A set of command-line tools to work with STAM Learn STAM Specification The STAM specification lays out the data model of STAM in formal terms, and is a complete source to understand what STAM is all about: STAM Specification Python Tutorial: Standoff Text Annotation for Pythonistas To get hands-on experience with STAM from Python, please consult this tutorial, which comes in the form of a Jupyter Notebook you can run interactively: STAM Tutorial: Standoff Text Annotation for Pythonistas The full Python API is documented here: API Reference Rust library The core library for STAM is implemented in Rust. It is also used by the Python binding. Advanced programmers may also use it directly to build efficient applications that deal with stand-off annotation on text: stam-rust: STAM library for Rust API Reference Extensions STAM is kept simple and only the bare minimum is defined. Other functionality is included in extensions. Extensions do one or more of the following: they extend the data model, specify new serialisations, specify mappings/crosswalks to other paradigms/formats, specify additional functionality. The following are currently defined: STAM-Vocab - Allows expressing and validating against user-defined vocabularies. STAM-Webannotations - Models W3C Web Annotations using STAM and vice versa. STAM-Textvalidation - Adds an extra redundancy layer that helps protecting data integrity and aids readability of serialisations STAM-CSV - Defines an alternative serialisation format using CSV. STAM-Baseoffset - allows splitting large monolithic text resources into multiple smaller text resources, whilst still retaining the ability the reference offsets as if they refer to the original/monolithic resource. Implementations SHOULD explicitly state which extensions they support. For more information, have a look at the README .","title":"STAM"},{"location":"tools/stam/#stam","text":"Stand-off Text Annotation Model (STAM) is a data model for stand-off-text annotation where any information on a text is represented as an annotation.","title":"STAM"},{"location":"tools/stam/#overview","text":"STAM is a minimalist data model for stand-off text annotation. Any information on a text is represented an annotation, which can be any kind of remarks, classifications, or tags on specific portions of the text or the entire resource. Annotations can also point to other annotations (higher-order annotations). STAM does not define specific vocabularies and accepts plain text as its base resource. It is independent of complex data models like RDF, W3C Web Annotations, TEI, or FoLiA. STAM aims to be a functional and practical solution, allowing users to use use vocabularies that are formalised elsewhere. STAM is primarily intended as a model for data representation, and less so as a format for data interchange. It is designed in such as way that an efficient implementation (both speed & memory) is feasible.","title":"Overview"},{"location":"tools/stam/#goalscharacteristics-of-stam-are","text":"Simplicity - the data model must be easy to understand for a user/developer to use and only contain what is needed, not more. We provide a minimal foundation upon which other projects can build more complex solutions. These are deliberately kept out of STAM itself. The notion that everything is an annotation is at the core of STAM and one of the things that keeps it simple. Separation from semantics - The data model does not commit to any vocabulary or annotation paradigm. It must be flexible enough to express whatever annotation paradigm a researcher wants to use, yet provide the facilities to be specific enough for practical purposes. The model basically allows for any kind of directed or undirected graph. Standalone - No dependency on other data models (e.g. RDF) aside from Unicode and JSON for serialisation, no dependency on any software services. Practical - Rather than provide a theoretical framework, we primarily aim to provide a practical specification and actual low-level tooling you can get to work with right away. Performant - The data model is set up in such a way that it allows for efficient/performant implementations, with regard to processing requirements but especially memory consumption. The model should be suitable for big data (millions of annotations). We sit at a point where we deem to have an optimal trade-off between simplicity and performance. Import & Export - Reads/writes a simple JSON format. But also designed with export to more complex formats in mind (such as W3C Web Annotations / RDF) and imports from common formats such as TSV and CONLL. Note that although STAM puts no constraints on annotation paradigms and vocabularies, higher data models may. The name STAM, an acronym for \" Stand-off Text Annotation Model \", is Dutch, Swedish, Afrikaans and Frisian for \" trunk \" (as in the trunk of a tree), the name itself depicts a solid foundation upon which more elaborate solutions can be built. If you want to learn more, please have a look at the specification on project's github page and at the implementations mentioned below:","title":"Goals/characteristics of STAM are:"},{"location":"tools/stam/#implementations","text":"There are currently two implementations for STAM: stam-rust - A STAM library written in Rust, aims to be a full STAM implementation with high performance and memory-based storage model. stam-python - A STAM library for Python. This is not an independent implementation but it is a Python binding to the above Rust library. Furthermore, there is also the following implementation that builds upon the primary STAM library: stam-tools - A set of command-line tools to work with STAM","title":"Implementations"},{"location":"tools/stam/#learn","text":"","title":"Learn"},{"location":"tools/stam/#stam-specification","text":"The STAM specification lays out the data model of STAM in formal terms, and is a complete source to understand what STAM is all about: STAM Specification","title":"STAM Specification"},{"location":"tools/stam/#python-tutorial-standoff-text-annotation-for-pythonistas","text":"To get hands-on experience with STAM from Python, please consult this tutorial, which comes in the form of a Jupyter Notebook you can run interactively: STAM Tutorial: Standoff Text Annotation for Pythonistas The full Python API is documented here: API Reference","title":"Python Tutorial: Standoff Text Annotation for Pythonistas"},{"location":"tools/stam/#rust-library","text":"The core library for STAM is implemented in Rust. It is also used by the Python binding. Advanced programmers may also use it directly to build efficient applications that deal with stand-off annotation on text: stam-rust: STAM library for Rust API Reference","title":"Rust library"},{"location":"tools/stam/#extensions","text":"STAM is kept simple and only the bare minimum is defined. Other functionality is included in extensions. Extensions do one or more of the following: they extend the data model, specify new serialisations, specify mappings/crosswalks to other paradigms/formats, specify additional functionality. The following are currently defined: STAM-Vocab - Allows expressing and validating against user-defined vocabularies. STAM-Webannotations - Models W3C Web Annotations using STAM and vice versa. STAM-Textvalidation - Adds an extra redundancy layer that helps protecting data integrity and aids readability of serialisations STAM-CSV - Defines an alternative serialisation format using CSV. STAM-Baseoffset - allows splitting large monolithic text resources into multiple smaller text resources, whilst still retaining the ability the reference offsets as if they refer to the original/monolithic resource. Implementations SHOULD explicitly state which extensions they support. For more information, have a look at the README .","title":"Extensions"},{"location":"tools/textfabric/","text":"Text-Fabric Text-Fabric processes corpora as annotated graphs, treating them as structured data without losing text richness like embedding. It works after markup removal, preserving complete logical structure. Overview A corpus of ancient texts and (linguistic) annotations represents a large body of knowledge. Text-Fabric (TF) makes that knowledge accessible to programmers and non-programmers. TF is machinery for processing such corpora as annotated graphs. It treats corpora and annotations as data, much like big tables, but without losing the rich structure of text, such as embedding and multiple representations. It deals with text in a state where all markup is gone, but where the complete logical structure still sits in the data. Whether a corpus comes from plain texts, OCR output, databases, XML, TEI: TF has support to convert it to single column files, where each file corresponds with a feature of the text. The Python library tf can be used to collect a bunch of features and display it as an annotated text. What ties the features together are natural numbers, that serve to anchor the elementary positions in the text as well as the relevant structures within the text. When TF loads a dataset of features, you can instruct it to get the features from anywhere. That means it supports workflows where annotations are produced by third parties and can be used against the original corpus, without additional work. It also facilitates mappings between ongoing versions of the corpus, so that annotations made on older versions can be ported to newer versions without redoing the annotation creation. Learn The Text-Fabric webpages offer a wide set instruction and support material, including: * Installation instructions * How to use it * FAQ * File formats * Dataset manipulation * Search usage A tutorial Have a look on the Text-Fabric webpages for a complete overview. Instructions Prerequisites : you need to have Python installed on your machine.Make sure you have at least Python 3.7.0. If you do not have Python : * The fastest way to set up everything you need to use TF is by installing the JupyterLab Desktop application. This installs both JupyterLab and Python in one go, as a desktop application running under MacOS, Linux or Windows. The Jupyter Desktop App can be downloaded from JupyterLab-desktop, choose the one that fits your system. You can access the tutorial on the Text-Fabric website , providing step-by-step guide for the installation process. There is also a tutorial available. Mentions Articles Johnson, C., de Ridder, A., Kokken, M., & Roorda, D. (2020-06-26). OldAssyrian. DOI: 10.5281/zenodo.3909515[https://doi.org/10.5281/zenodo.3909515]. Johnson, C., de Ridder, A., Kokken, M., & Roorda, D. (2020-06-26). OldBabylonian. DOI: (10.5281/zenodo.3909507)[https://doi.org/10.5281/zenodo.3909507]. Johnson, C., & Roorda, D. (2022-02-02). NinMed. DOI: (10.5281/zenodo.5984171)[https://doi.org/10.5281/zenodo.5984171]. Johnson, C., & Roorda, D. (2018-03-07). Uruk. DOI: (10.5281/zenodo.1482791.)[https://doi.org/10.5281/zenodo.1482791] *Kingham, c. & Roorda, D. Northeastern Neo-Aramaic Text-Fabric Corpus. DOI: (10.5281/zenodo.3250720)[https://doi.org/10.5281/zenodo.3250720]. Naaijer, M., & Roorda, D. (2020-12-01). Dead Sea Scrolls. DOI: (10.5281/zenodo.3944788)[https://doi.org/10.5281/zenodo.3944788]. Roorda, D. (2018). Coding the Hebrew Bible. In: Research Data Journal for the Humanities and Social Sciences, 27-41. DOI: (10.1163/24523666-01000011)[https://doi.org/10.1163/24523666-01000011] Roorda, D. (2019). Text-fabric: handling Biblical data with IKEA logistics. In: HIPHIL Novum, 126-135[https://www.hiphil.org/index.php/hiphil/article/view/50]. Roorda, D., Kalkman, G., Naaijer, M., & van Cranenburgh, A. (2014). LAF-Fabric: a data analysis tool for Linguistic Annotation Framework with an application to the Hebrew Bible. In: Computational Linguistics in the Netherlands Journal, 105-120[https://www.clinjournal.org/clinj/article/view/44]. Roorda, D. (2015). The Hebrew Bible as Data: Laboratory \u2013 Sharing \u2013 Experiences. In: CLARIN in the Low Countries, 211-224[https://arxiv.org/abs/1501.01866]. Scherer, B., Mataar, Y., van Peursen, W., & Roorda, D. (2021-12-24). Dhammapada-Latine. DOI: (10.5281/zenodo.5803464)[https://doi.org/10.5281/zenodo.5803464]. Sikkel, C., Roorda, D., & van Peursen, W. (2019-01-31). BHSA. DOI: (10.5281/zenodo.1007624)[https://doi.org/10.5281/zenodo.1007624]. van Lit, C., & Roorda, D. (2019-01-15). Quran. DOI: (10.5281/zenodo.2532177)[https://doi.org/10.5281/zenodo.2532177].","title":"Text-Fabric"},{"location":"tools/textfabric/#text-fabric","text":"Text-Fabric processes corpora as annotated graphs, treating them as structured data without losing text richness like embedding. It works after markup removal, preserving complete logical structure.","title":"Text-Fabric"},{"location":"tools/textfabric/#overview","text":"A corpus of ancient texts and (linguistic) annotations represents a large body of knowledge. Text-Fabric (TF) makes that knowledge accessible to programmers and non-programmers. TF is machinery for processing such corpora as annotated graphs. It treats corpora and annotations as data, much like big tables, but without losing the rich structure of text, such as embedding and multiple representations. It deals with text in a state where all markup is gone, but where the complete logical structure still sits in the data. Whether a corpus comes from plain texts, OCR output, databases, XML, TEI: TF has support to convert it to single column files, where each file corresponds with a feature of the text. The Python library tf can be used to collect a bunch of features and display it as an annotated text. What ties the features together are natural numbers, that serve to anchor the elementary positions in the text as well as the relevant structures within the text. When TF loads a dataset of features, you can instruct it to get the features from anywhere. That means it supports workflows where annotations are produced by third parties and can be used against the original corpus, without additional work. It also facilitates mappings between ongoing versions of the corpus, so that annotations made on older versions can be ported to newer versions without redoing the annotation creation.","title":"Overview"},{"location":"tools/textfabric/#learn","text":"The Text-Fabric webpages offer a wide set instruction and support material, including: * Installation instructions * How to use it * FAQ * File formats * Dataset manipulation * Search usage A tutorial Have a look on the Text-Fabric webpages for a complete overview.","title":"Learn"},{"location":"tools/textfabric/#instructions","text":"Prerequisites : you need to have Python installed on your machine.Make sure you have at least Python 3.7.0. If you do not have Python : * The fastest way to set up everything you need to use TF is by installing the JupyterLab Desktop application. This installs both JupyterLab and Python in one go, as a desktop application running under MacOS, Linux or Windows. The Jupyter Desktop App can be downloaded from JupyterLab-desktop, choose the one that fits your system. You can access the tutorial on the Text-Fabric website , providing step-by-step guide for the installation process. There is also a tutorial available.","title":"Instructions"},{"location":"tools/textfabric/#mentions","text":"","title":"Mentions"},{"location":"tools/textfabric/#articles","text":"Johnson, C., de Ridder, A., Kokken, M., & Roorda, D. (2020-06-26). OldAssyrian. DOI: 10.5281/zenodo.3909515[https://doi.org/10.5281/zenodo.3909515]. Johnson, C., de Ridder, A., Kokken, M., & Roorda, D. (2020-06-26). OldBabylonian. DOI: (10.5281/zenodo.3909507)[https://doi.org/10.5281/zenodo.3909507]. Johnson, C., & Roorda, D. (2022-02-02). NinMed. DOI: (10.5281/zenodo.5984171)[https://doi.org/10.5281/zenodo.5984171]. Johnson, C., & Roorda, D. (2018-03-07). Uruk. DOI: (10.5281/zenodo.1482791.)[https://doi.org/10.5281/zenodo.1482791] *Kingham, c. & Roorda, D. Northeastern Neo-Aramaic Text-Fabric Corpus. DOI: (10.5281/zenodo.3250720)[https://doi.org/10.5281/zenodo.3250720]. Naaijer, M., & Roorda, D. (2020-12-01). Dead Sea Scrolls. DOI: (10.5281/zenodo.3944788)[https://doi.org/10.5281/zenodo.3944788]. Roorda, D. (2018). Coding the Hebrew Bible. In: Research Data Journal for the Humanities and Social Sciences, 27-41. DOI: (10.1163/24523666-01000011)[https://doi.org/10.1163/24523666-01000011] Roorda, D. (2019). Text-fabric: handling Biblical data with IKEA logistics. In: HIPHIL Novum, 126-135[https://www.hiphil.org/index.php/hiphil/article/view/50]. Roorda, D., Kalkman, G., Naaijer, M., & van Cranenburgh, A. (2014). LAF-Fabric: a data analysis tool for Linguistic Annotation Framework with an application to the Hebrew Bible. In: Computational Linguistics in the Netherlands Journal, 105-120[https://www.clinjournal.org/clinj/article/view/44]. Roorda, D. (2015). The Hebrew Bible as Data: Laboratory \u2013 Sharing \u2013 Experiences. In: CLARIN in the Low Countries, 211-224[https://arxiv.org/abs/1501.01866]. Scherer, B., Mataar, Y., van Peursen, W., & Roorda, D. (2021-12-24). Dhammapada-Latine. DOI: (10.5281/zenodo.5803464)[https://doi.org/10.5281/zenodo.5803464]. Sikkel, C., Roorda, D., & van Peursen, W. (2019-01-31). BHSA. DOI: (10.5281/zenodo.1007624)[https://doi.org/10.5281/zenodo.1007624]. van Lit, C., & Roorda, D. (2019-01-15). Quran. DOI: (10.5281/zenodo.2532177)[https://doi.org/10.5281/zenodo.2532177].","title":"Articles"},{"location":"tools/ucto/","text":"Ucto Ucto tokenizes text files: it separates words from punctuation, and splits sentences. It offers several other basic preprocessing steps such as changing case that you can all use to make your text suited for further processing such as indexing, part-of-speech tagging, or machine translation. Ucto comes with tokenisation rules for several languages and can be easily extended to suit other languages. It has been incorporated for tokenizing Dutch text in Frog, our Dutch morpho-syntactic processor. Overview Comes with tokenization rules for English, Dutch, French, Italian, and Swedish; easily extendible to other languages. Recognizes dates, times, units, currencies, abbreviations. Recognizes paired quote spans, sentences, and paragraphs. Produces UTF8 encoding and NFC output normalization, optionally accepts other encodings as input. Optional conversion to all lowercase or uppercase. Supports FoLiA XML . Ucto is also available as a webservice . Learn Webservice Ucto is available as a webservice . Download & Installation Ucto is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation . To download and install Ucto: First check if there are up-to-date packages included in your distribution's package manager. There are packages for Arch Linux, Debian, FreeBSD and Ubuntu. If not, it is strongly recommended to use the LaMachine software distribution, which includes Ucto and all necessary dependencies, and runs on Linux, BSD and Mac OS X. It can also run as a virtual machine under any host OS. Alternatively, you can always download, compile and install Ucto manually, as described below. Manual installation Source code Stable releases To compile these manually consult the included INSTALL documents, you will need current versions of the following dependencies of our software: ticcutils - A shared utility library; libfolia - A library for the FoLiA format. As well as the following 3rd party dependencies: icu - A C++ library for Unicode and Globalization support. On Debian/Ubuntu systems, install the package libicu-dev . libxml2 - An XML library. On Debian/Ubuntu systems install the package libxml2-dev . A sane build environment with a C++ compiler (e.g. gcc or clang), autotools, libtool, pkg-config. Documentation The Ucto documentation can be found here . Python binding Ucto can be used from Python through the python-ucto binding, which has to be obtained separately unless you are using LaMachine . Support The development and improvement of Ucto also relies on your bug reports, suggestions, and comments. Use the github issue tracker or mail lamasoftware (at) science.ru.nl. Mentions Publications Van Gompel, M., van der Sloot, K., & van den Bosch, A. (2012). Ucto: Unicode Tokeniser . Version 0.5, 3 , 12-05. Webpages Ucto home page Ucto as a webservice Ucto is included in LaMachine , a unified software distribution for Natural Language Processing, also including (among others): Alpino Frog FoLiA Colibri Core CLAM FLAT Credits, Contact Information and License Ucto was written by Maarten van Gompel and Ko van der Sloot ( Radboud University ). Work on Ucto was funded by NWO, the Netherlands Organisation for Scientific Research, under the Implicit Linguistics project and the CLARIN-NL program. The development and improvement of Ucto also relies on your bug reports, suggestions, and comments. Use the github issue tracker or mail lamasoftware (at) science.ru.nl. Ucto is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation .","title":"Ucto"},{"location":"tools/ucto/#ucto","text":"Ucto tokenizes text files: it separates words from punctuation, and splits sentences. It offers several other basic preprocessing steps such as changing case that you can all use to make your text suited for further processing such as indexing, part-of-speech tagging, or machine translation. Ucto comes with tokenisation rules for several languages and can be easily extended to suit other languages. It has been incorporated for tokenizing Dutch text in Frog, our Dutch morpho-syntactic processor.","title":"Ucto"},{"location":"tools/ucto/#overview","text":"Comes with tokenization rules for English, Dutch, French, Italian, and Swedish; easily extendible to other languages. Recognizes dates, times, units, currencies, abbreviations. Recognizes paired quote spans, sentences, and paragraphs. Produces UTF8 encoding and NFC output normalization, optionally accepts other encodings as input. Optional conversion to all lowercase or uppercase. Supports FoLiA XML . Ucto is also available as a webservice .","title":"Overview"},{"location":"tools/ucto/#learn","text":"","title":"Learn"},{"location":"tools/ucto/#webservice","text":"Ucto is available as a webservice .","title":"Webservice"},{"location":"tools/ucto/#download-installation","text":"Ucto is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation . To download and install Ucto: First check if there are up-to-date packages included in your distribution's package manager. There are packages for Arch Linux, Debian, FreeBSD and Ubuntu. If not, it is strongly recommended to use the LaMachine software distribution, which includes Ucto and all necessary dependencies, and runs on Linux, BSD and Mac OS X. It can also run as a virtual machine under any host OS. Alternatively, you can always download, compile and install Ucto manually, as described below.","title":"Download &amp; Installation"},{"location":"tools/ucto/#manual-installation","text":"Source code Stable releases To compile these manually consult the included INSTALL documents, you will need current versions of the following dependencies of our software: ticcutils - A shared utility library; libfolia - A library for the FoLiA format. As well as the following 3rd party dependencies: icu - A C++ library for Unicode and Globalization support. On Debian/Ubuntu systems, install the package libicu-dev . libxml2 - An XML library. On Debian/Ubuntu systems install the package libxml2-dev . A sane build environment with a C++ compiler (e.g. gcc or clang), autotools, libtool, pkg-config.","title":"Manual installation"},{"location":"tools/ucto/#documentation","text":"The Ucto documentation can be found here .","title":"Documentation"},{"location":"tools/ucto/#python-binding","text":"Ucto can be used from Python through the python-ucto binding, which has to be obtained separately unless you are using LaMachine .","title":"Python binding"},{"location":"tools/ucto/#support","text":"The development and improvement of Ucto also relies on your bug reports, suggestions, and comments. Use the github issue tracker or mail lamasoftware (at) science.ru.nl.","title":"Support"},{"location":"tools/ucto/#mentions","text":"","title":"Mentions"},{"location":"tools/ucto/#publications","text":"Van Gompel, M., van der Sloot, K., & van den Bosch, A. (2012). Ucto: Unicode Tokeniser . Version 0.5, 3 , 12-05.","title":"Publications"},{"location":"tools/ucto/#webpages","text":"Ucto home page Ucto as a webservice Ucto is included in LaMachine , a unified software distribution for Natural Language Processing, also including (among others): Alpino Frog FoLiA Colibri Core CLAM FLAT","title":"Webpages"},{"location":"tools/ucto/#credits-contact-information-and-license","text":"Ucto was written by Maarten van Gompel and Ko van der Sloot ( Radboud University ). Work on Ucto was funded by NWO, the Netherlands Organisation for Scientific Research, under the Implicit Linguistics project and the CLARIN-NL program. The development and improvement of Ucto also relies on your bug reports, suggestions, and comments. Use the github issue tracker or mail lamasoftware (at) science.ru.nl. Ucto is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation .","title":"Credits, Contact Information and License"},{"location":"tools/udpipe_frysk/","text":"UDPipe Frysk UDPipe Frysk is a webservice for lemmatizing, part-of-speech tagging and dependency parsing of (West) Frisian texts using UDPipe (Straka and Strakov\u00e1, 2017). The tool allows for multiple ways of processing a text (the web service facilitates texts, files and web addresses). UDPipe parses texts following the Universal Dependencies programme, which aims at cross-linguistically consistent tagging and annotation of dependency trees. Overview UDPipe Frysk is a webservice for lemmatizing, PoS tagging and dependency parsing of Frisian texts, developed by the Fryske Akademy and the Univerity of Groningen. Researchers can use the webservice for research concerning, for example, language change, syntactic relationships, author recognition, sentiment analysis or for developing automatic question-answer systems. The user can type in a Frisian text themselves, upload the text as a file or provide a URL to a Frisian website. The output can then be downloaded in various formats (txt, Excel, CoNLL-U) for further analysis. UDPipe parses texts following the Universal Dependencies annotation guidelines, which aim at cross-linguistically consistent tagging and annotation of dependency trees. UDPipe Frysk also provides online parsing of Dutch and English texts using Dutch and English models. Learn Using UDPipe Frysk UDPipe Frysk is freely available. The user can type in a Frisian text themselves, upload the text as a file or provide a URL to a Frisian website. The output can then be downloaded in various formats (txt, Excel, CoNLL-U) for further analysis. The UDPipe Frysk webservice is built on the R package udpipe . More technical information can be found on the UDPipe Frysk page . UDPipe Frysk runs best on a computer with a monitor with a minimum resolution of 1370 x 870 (width x height). The use of (a recent version of) Chrome, Chromium, Edge, Firefox or Opera as a web browser is to be preferred. Universal Dependencies Universal Dependencies (UD) is a programme that aims at cross-linguistically consistent tagging and dependency parsing. UD is an open community effort with over 500 contributors producing over 200 treebanks in over 100 languages. If you\u2019re new to UD, you should start by reading the first part of the Short Introduction and then browsing the annotation guidelines on the UD website. UDPipe Developed at Charles University, \u00daFAL by Straka and Strakov\u00e1 , UDPipe is a pipeline for morphosyntactic tagging and parsing following the UD programme. A wide array of models for multiple languages is available for UDPipe on the LINDAT/CLARIN repository. However, the model used by UDPipe Frysk is not included in the LINDAT/CLARIN repository, and can instead be found on the Fryske Akademy's Bitbucket page . UDPipe can be installed locally, for instance as Python or R package, allowing the user to freely choose the preferred model, or even train their own. For the local installation of UDPipe, we refer to \u00daFAL's GitHub page and relevant package pages. UDPipe Frysk can then be used locally, too, by installing UDPipe, and using the Frisian model. \u00daFAL also hosts a webservice of UDPipe (not including Frisian), which can be found here . UDPipe output is formatted in CoNLL-U, introduced by UD. The format is explained here . Mentions Key publications Wilbert Heeringa, Gosse Bouma, Martha Hofman, Eduard Drenth, Jan Wijffels & Hans Van de Velde (2021). POS tagging, lemmatization and dependency parsing of West Frisian, arXiv preprint arXiv:2107.07974. https://arxiv.org/pdf/2107.07974.pdf Renckens, E. (2020, May 29). Tool voor Taalkundig Onderzoek Fries. E-Data &Research. https://edata.nl/2020/05/20/tool-voor-taalkundig-onderzoek-fries/ (in Dutch) Key webpages UDPipe Frysk UDPipe Frysk Bitbucket page Relevant literature Milan Straka and Jana Strakov\u00e1. 2017. Tokenizing, POS Tagging, Lemmatizing and Parsing UD 2.0 with UDPipe. In Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies, pages 88\u201399, Vancouver, Canada. Association for Computational Linguistics. Relevant webpages Universal Dependencies homepage UDPipe homepage UDPipe GitHub page UDPipe models UDPipe webservice Python UDPipe package R UDPipe package CoNLL-U format documentation Acknowledgments The development of this software was made possible by a CLARIAH-Plus project financed by the Dutch Research Council (Grant 184.034.023).","title":"UDPipe Frysk"},{"location":"tools/udpipe_frysk/#udpipe-frysk","text":"UDPipe Frysk is a webservice for lemmatizing, part-of-speech tagging and dependency parsing of (West) Frisian texts using UDPipe (Straka and Strakov\u00e1, 2017). The tool allows for multiple ways of processing a text (the web service facilitates texts, files and web addresses). UDPipe parses texts following the Universal Dependencies programme, which aims at cross-linguistically consistent tagging and annotation of dependency trees.","title":"UDPipe Frysk"},{"location":"tools/udpipe_frysk/#overview","text":"UDPipe Frysk is a webservice for lemmatizing, PoS tagging and dependency parsing of Frisian texts, developed by the Fryske Akademy and the Univerity of Groningen. Researchers can use the webservice for research concerning, for example, language change, syntactic relationships, author recognition, sentiment analysis or for developing automatic question-answer systems. The user can type in a Frisian text themselves, upload the text as a file or provide a URL to a Frisian website. The output can then be downloaded in various formats (txt, Excel, CoNLL-U) for further analysis. UDPipe parses texts following the Universal Dependencies annotation guidelines, which aim at cross-linguistically consistent tagging and annotation of dependency trees. UDPipe Frysk also provides online parsing of Dutch and English texts using Dutch and English models.","title":"Overview"},{"location":"tools/udpipe_frysk/#learn","text":"","title":"Learn"},{"location":"tools/udpipe_frysk/#using-udpipe-frysk","text":"UDPipe Frysk is freely available. The user can type in a Frisian text themselves, upload the text as a file or provide a URL to a Frisian website. The output can then be downloaded in various formats (txt, Excel, CoNLL-U) for further analysis. The UDPipe Frysk webservice is built on the R package udpipe . More technical information can be found on the UDPipe Frysk page . UDPipe Frysk runs best on a computer with a monitor with a minimum resolution of 1370 x 870 (width x height). The use of (a recent version of) Chrome, Chromium, Edge, Firefox or Opera as a web browser is to be preferred.","title":"Using UDPipe Frysk"},{"location":"tools/udpipe_frysk/#universal-dependencies","text":"Universal Dependencies (UD) is a programme that aims at cross-linguistically consistent tagging and dependency parsing. UD is an open community effort with over 500 contributors producing over 200 treebanks in over 100 languages. If you\u2019re new to UD, you should start by reading the first part of the Short Introduction and then browsing the annotation guidelines on the UD website.","title":"Universal Dependencies"},{"location":"tools/udpipe_frysk/#udpipe","text":"Developed at Charles University, \u00daFAL by Straka and Strakov\u00e1 , UDPipe is a pipeline for morphosyntactic tagging and parsing following the UD programme. A wide array of models for multiple languages is available for UDPipe on the LINDAT/CLARIN repository. However, the model used by UDPipe Frysk is not included in the LINDAT/CLARIN repository, and can instead be found on the Fryske Akademy's Bitbucket page . UDPipe can be installed locally, for instance as Python or R package, allowing the user to freely choose the preferred model, or even train their own. For the local installation of UDPipe, we refer to \u00daFAL's GitHub page and relevant package pages. UDPipe Frysk can then be used locally, too, by installing UDPipe, and using the Frisian model. \u00daFAL also hosts a webservice of UDPipe (not including Frisian), which can be found here . UDPipe output is formatted in CoNLL-U, introduced by UD. The format is explained here .","title":"UDPipe"},{"location":"tools/udpipe_frysk/#mentions","text":"","title":"Mentions"},{"location":"tools/udpipe_frysk/#key-publications","text":"Wilbert Heeringa, Gosse Bouma, Martha Hofman, Eduard Drenth, Jan Wijffels & Hans Van de Velde (2021). POS tagging, lemmatization and dependency parsing of West Frisian, arXiv preprint arXiv:2107.07974. https://arxiv.org/pdf/2107.07974.pdf Renckens, E. (2020, May 29). Tool voor Taalkundig Onderzoek Fries. E-Data &Research. https://edata.nl/2020/05/20/tool-voor-taalkundig-onderzoek-fries/ (in Dutch)","title":"Key publications"},{"location":"tools/udpipe_frysk/#key-webpages","text":"UDPipe Frysk UDPipe Frysk Bitbucket page","title":"Key webpages"},{"location":"tools/udpipe_frysk/#relevant-literature","text":"Milan Straka and Jana Strakov\u00e1. 2017. Tokenizing, POS Tagging, Lemmatizing and Parsing UD 2.0 with UDPipe. In Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies, pages 88\u201399, Vancouver, Canada. Association for Computational Linguistics.","title":"Relevant literature"},{"location":"tools/udpipe_frysk/#relevant-webpages","text":"Universal Dependencies homepage UDPipe homepage UDPipe GitHub page UDPipe models UDPipe webservice Python UDPipe package R UDPipe package CoNLL-U format documentation","title":"Relevant webpages"},{"location":"tools/udpipe_frysk/#acknowledgments","text":"The development of this software was made possible by a CLARIAH-Plus project financed by the Dutch Research Council (Grant 184.034.023).","title":"Acknowledgments"}]}